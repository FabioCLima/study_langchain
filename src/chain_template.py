'''Template de Fluxo para Chains no LangChain

Este template demonstra o fluxo padrÃ£o para criar chains no LangChain
seguindo as melhores prÃ¡ticas e organizaÃ§Ã£o de cÃ³digo.

Fluxo de Etapas:
1. Imports e Setup
2. ConfiguraÃ§Ã£o do Modelo
3. DefiniÃ§Ã£o do Prompt
4. ConfiguraÃ§Ã£o do Output Parser
5. CriaÃ§Ã£o da Chain
6. ExecuÃ§Ã£o e Tratamento de Erros
7. ValidaÃ§Ã£o e Testes
'''

from dotenv import load_dotenv, find_dotenv
from langchain_openai import ChatOpenAI  # type: ignore
from langchain_core.prompts import ChatPromptTemplate  # type: ignore
from langchain_core.output_parsers import StrOutputParser  # type: ignore
from langchain_core.runnables import RunnablePassthrough  # type: ignore
import os
from typing import Dict, Any, Optional

# =============================================================================
# ETAPA 1: IMPORTS E SETUP
# =============================================================================

def setup_environment() -> None:
    """Configura o ambiente e valida as dependÃªncias."""
    # Carrega variÃ¡veis de ambiente
    _ = load_dotenv(find_dotenv())
    
    # Valida API key
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("OPENAI_API_KEY nÃ£o encontrada no arquivo .env")
    
    print("âœ… Ambiente configurado com sucesso")

# =============================================================================
# ETAPA 2: CONFIGURAÃ‡ÃƒO DO MODELO
# =============================================================================

def create_model(
    model_name: str = "gpt-3.5-turbo",
    temperature: float = 0.5,
    max_tokens: Optional[int] = None
) -> ChatOpenAI:
    """
    Cria e configura o modelo LLM.
    
    Args:
        model_name: Nome do modelo a ser usado
        temperature: Controla criatividade (0.0 = determinÃ­stico, 1.0 = muito criativo)
        max_tokens: NÃºmero mÃ¡ximo de tokens na resposta
    
    Returns:
        ChatOpenAI: Modelo configurado
    """
    model = ChatOpenAI(
        model=model_name,
        temperature=temperature,
        max_tokens=max_tokens
    )  # type: ignore
    
    print(f"âœ… Modelo {model_name} configurado (temperature={temperature})")
    return model

# =============================================================================
# ETAPA 3: DEFINIÃ‡ÃƒO DO PROMPT
# =============================================================================

def create_prompt() -> ChatPromptTemplate:
    """
    Cria o template do prompt.
    
    Returns:
        ChatPromptTemplate: Template do prompt configurado
    """
    prompt = ChatPromptTemplate.from_template(
        """VocÃª Ã© um assistente especializado em {domain}.
        
        Contexto: {context}
        Pergunta: {question}
        
        InstruÃ§Ãµes especÃ­ficas:
        - Responda de forma clara e concisa
        - Use exemplos quando apropriado
        - Mantenha o foco no tÃ³pico solicitado
        
        Resposta:"""
    )
    
    print("âœ… Template de prompt criado")
    return prompt

# =============================================================================
# ETAPA 4: CONFIGURAÃ‡ÃƒO DO OUTPUT PARSER
# =============================================================================

def create_output_parser() -> StrOutputParser:
    """
    Cria o parser de saÃ­da.
    
    Returns:
        StrOutputParser: Parser configurado
    """
    output_parser = StrOutputParser()
    print("âœ… Output parser configurado")
    return output_parser

# =============================================================================
# ETAPA 5: CRIAÃ‡ÃƒO DA CHAIN
# =============================================================================

def create_chain(
    model: ChatOpenAI,
    prompt: ChatPromptTemplate,
    output_parser: StrOutputParser
) -> Any:
    """
    Cria a chain usando LCEL (LangChain Expression Language).
    
    Args:
        model: Modelo LLM configurado
        prompt: Template do prompt
        output_parser: Parser de saÃ­da
    
    Returns:
        Chain configurada
    """
    # Chain bÃ¡sica: prompt -> model -> output_parser
    chain = prompt | model | output_parser
    
    print("âœ… Chain criada com sucesso")
    return chain

# =============================================================================
# ETAPA 6: EXECUÃ‡ÃƒO E TRATAMENTO DE ERROS
# =============================================================================

def execute_chain(
    chain: Any,
    input_data: Dict[str, Any],
    retry_count: int = 3
) -> str:
    """
    Executa a chain com tratamento de erros.
    
    Args:
        chain: Chain configurada
        input_data: Dados de entrada
        retry_count: NÃºmero de tentativas em caso de erro
    
    Returns:
        str: Resposta da chain
    """
    try:
        response = chain.invoke(input_data)
        print("âœ… Chain executada com sucesso")
        return response
    except Exception as e:
        print(f"âŒ Erro na execuÃ§Ã£o da chain: {e}")
        if retry_count > 0:
            print(f"ğŸ”„ Tentando novamente... ({retry_count} tentativas restantes)")
            return execute_chain(chain, input_data, retry_count - 1)
        raise

# =============================================================================
# ETAPA 7: VALIDAÃ‡ÃƒO E TESTES
# =============================================================================

def validate_response(response: str) -> bool:
    """
    Valida se a resposta estÃ¡ no formato esperado.
    
    Args:
        response: Resposta da chain
    
    Returns:
        bool: True se vÃ¡lida, False caso contrÃ¡rio
    """
    if not response or len(response.strip()) < 10:
        print("âš ï¸ Resposta muito curta ou vazia")
        return False
    
    print("âœ… Resposta validada")
    return True

# =============================================================================
# FUNÃ‡ÃƒO PRINCIPAL - FLUXO COMPLETO
# =============================================================================

def main() -> None:
    """Executa o fluxo completo de criaÃ§Ã£o e execuÃ§Ã£o de uma chain."""
    
    print("ğŸš€ Iniciando criaÃ§Ã£o da chain...")
    
    # Etapa 1: Setup
    setup_environment()
    
    # Etapa 2: Modelo
    model = create_model(
        model_name="gpt-3.5-turbo",
        temperature=0.7
    )
    
    # Etapa 3: Prompt
    prompt = create_prompt()
    
    # Etapa 4: Output Parser
    output_parser = create_output_parser()
    
    # Etapa 5: Chain
    chain = create_chain(model, prompt, output_parser)
    
    # Etapa 6: ExecuÃ§Ã£o
    input_data = {
        "domain": "tecnologia",
        "context": "DiscussÃ£o sobre inteligÃªncia artificial",
        "question": "O que Ã© machine learning?"
    }
    
    response = execute_chain(chain, input_data)
    
    # Etapa 7: ValidaÃ§Ã£o
    if validate_response(response):
        print("\nğŸ“ Resposta final:")
        print("=" * 50)
        print(response)
        print("=" * 50)
    else:
        print("âŒ Falha na validaÃ§Ã£o da resposta")

if __name__ == "__main__":
    main()