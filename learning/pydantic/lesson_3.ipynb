{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff18aec7",
   "metadata": {},
   "source": [
    "# Lesson 3: Prompting for structure and setting up a retry method \n",
    "\n",
    "In this notebook, you'll learn how to combine Pydantic with retry strategies to reliably extract structured output from an LLM.\n",
    "\n",
    "By the end, you'll be able to:\n",
    "- Define structured data models for LLM responses\n",
    "- Build robust retry mechanisms for validation errors\n",
    "- Create reusable functions for LLM interactions\n",
    "\n",
    "---\n",
    "\n",
    "### Import packages and initialize the OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b663f423",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import json\n",
    "from datetime import date\n",
    "from typing import Literal, Type\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, EmailStr, Field, ValidationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831dfbb9",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "# Load environment variables for API access\n",
    "load_dotenv()\n",
    "# Initialize OpenAI client for API calls\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a014bd26",
   "metadata": {},
   "source": [
    "### Define some sample input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e8446f",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "# Define a JSON string representing user input\n",
    "user_input_json = \"\"\"\n",
    "{\n",
    "    \"name\": \"Joe User\",\n",
    "    \"email\": \"joe.user@example.com\",\n",
    "    \"query\": \"I forgot my password.\",\n",
    "    \"order_number\": null,\n",
    "    \"purchase_date\": null\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf1220",
   "metadata": {},
   "source": [
    "### Define your UserInput data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee3d7bf",
   "metadata": {
    "height": 217
   },
   "outputs": [],
   "source": [
    "# Define UserInput model\n",
    "class UserInput(BaseModel):\n",
    "    \"\"\"Pydantic model for validating user input with optional fields.\n",
    "\n",
    "    Data model represents comprehensive user input containing name, email, query,\n",
    "    order ID, and purchase date.\n",
    "\n",
    "    Attributes:\n",
    "        name (str): Full name of the user.\n",
    "        email (EmailStr): Email address of the user.\n",
    "        query (str): User's query or issue description.\n",
    "        order_id (int | None): 5-digit order number (cannot start with 0).\n",
    "        purchase_date (date | None): Date of purchase.\n",
    "\n",
    "    Raises:\n",
    "        ValidationError: If any validation constraint is violated:\n",
    "            - Invalid email format\n",
    "            - order_id outside range 10000-99999\n",
    "            - Invalid date format for purchase_date\n",
    "\n",
    "    Note:\n",
    "        This class leverages Pydantic's Field for advanced validation.\n",
    "        Optional fields can be omitted and will default to None.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    name: str\n",
    "    email: EmailStr\n",
    "    query: str\n",
    "    order_id: int | None = Field(\n",
    "        None,\n",
    "        description=(\"5-digit order number (cannot start with 0)\"),\n",
    "        ge=10000,\n",
    "        le=99999,\n",
    "    )\n",
    "    purchase_date: date | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1df5466",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Create UserInput instance from JSON data\n",
    "user_input = UserInput.model_validate_json(user_input_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8337578",
   "metadata": {},
   "source": [
    "### Create a new data model called CustomerQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7354431",
   "metadata": {
    "height": 217
   },
   "outputs": [],
   "source": [
    "# Define the CustomerQuery model that inherits from UserInput\n",
    "class CustomerQuery(UserInput):\n",
    "    \"\"\"Represents a structured customer query with classification metadata.\n",
    "\n",
    "    This model extends `UserInput` by adding fields necessary for automated\n",
    "    processing, routing, and prioritization within a customer support system.\n",
    "    It enriches the raw user input with structured data like priority,\n",
    "    category, and tags.\n",
    "\n",
    "    Attributes:\n",
    "        priority: The urgency of the query. Must be one of 'low', 'medium',\n",
    "            or 'high'.\n",
    "        category: The general topic of the query. Must be one of\n",
    "            'refund_request', 'information_request', or 'other'.\n",
    "        is_complaint: A boolean flag indicating if the query should be\n",
    "            treated as a formal complaint.\n",
    "        tags: A list of keywords that provide additional context for filtering\n",
    "            and analysis.\n",
    "\n",
    "    Example:\n",
    "        A typical use case involves parsing a dictionary of data, perhaps from\n",
    "        an API request or a form submission, into a validated model instance.\n",
    "\n",
    "        >>> query_data = {\n",
    "        ...     \"name\": \"John Doe\",\n",
    "        ...     \"email\": \"john.doe@example.com\",\n",
    "        ...     \"query\": \"My package hasn't arrived yet.\",\n",
    "        ...     \"order_id\": 54321,\n",
    "        ...     \"priority\": \"high\",\n",
    "        ...     \"category\": \"other\",\n",
    "        ...     \"is_complaint\": True,\n",
    "        ...     \"tags\": [\"shipping\", \"delay\", \"package_tracking\"]\n",
    "        ... }\n",
    "        >>> customer_query = CustomerQuery(**query_data)\n",
    "        >>> print(customer_query.priority)\n",
    "        high\n",
    "        >>> print(customer_query.is_complaint)\n",
    "        True\n",
    "        >>> print(customer_query.name)  # Inherited from UserInput\n",
    "        John Doe\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    priority: str = Field(..., description=\"Priority level: low, medium, high\")\n",
    "    category: Literal[\"refund_request\", \"information_request\", \"other\"] = Field(\n",
    "        ..., description=\"Query category\"\n",
    "    )\n",
    "    is_complaint: bool = Field(..., description=\"Whether this is a complaint\")\n",
    "    tags: list[str] = Field(..., description=\"Relevant keyword tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a42dae9",
   "metadata": {},
   "source": [
    "### Construct a prompt with example output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5fbbb8",
   "metadata": {
    "height": 232
   },
   "outputs": [],
   "source": [
    "# Create a prompt with generic example data to guide LLM.\n",
    "example_response_structure = \"\"\"{\n",
    "    \"name\": \"Example User\",\n",
    "    \"email\": \"user@example.com\",\n",
    "    \"query\": \"I ordered a new computer monitor and it arrived with the screen cracked.\n",
    "    I need to exchange it for a new one.\",\n",
    "    \"order_id\": 12345,\n",
    "    \"purchase_date\": \"2025-12-31\",\n",
    "    \"priority\": \"medium\",\n",
    "    \"category\": \"refund_request\",\n",
    "    \"is_complaint\": true,\n",
    "    \"tags\": [\"monitor\", \"support\", \"exchange\"]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12cb08",
   "metadata": {
    "height": 234
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"Example User\",\n",
      "    \"email\": \"user@example.com\",\n",
      "    \"query\": \"I ordered a new computer monitor and it arrived with the screen cracked. I need to exchange it for a new one.\",\n",
      "    \"order_id\": 12345,\n",
      "    \"purchase_date\": \"2025-12-31\",\n",
      "    \"priority\": \"medium\",\n",
      "    \"category\": \"refund_request\",\n",
      "    \"is_complaint\": true,\n",
      "    \"tags\": [\n",
      "        \"monitor\",\n",
      "        \"support\",\n",
      "        \"exchange\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example response structure to guide the LLM.\n",
    "# Using a dictionary instead of a formatted string makes it cleaner and easier to maintain.\n",
    "example_response_structure = {\n",
    "    \"name\": \"Example User\",\n",
    "    \"email\": \"user@example.com\",\n",
    "    \"query\": (\n",
    "        \"I ordered a new computer monitor and it arrived with the screen cracked. \"\n",
    "        \"I need to exchange it for a new one.\"\n",
    "    ),\n",
    "    \"order_id\": 12345,\n",
    "    \"purchase_date\": \"2025-12-31\",\n",
    "    \"priority\": \"medium\",\n",
    "    \"category\": \"refund_request\",\n",
    "    \"is_complaint\": True,\n",
    "    \"tags\": [\"monitor\", \"support\", \"exchange\"],\n",
    "}\n",
    "\n",
    "# (Optional) Pretty print the structure for quick inspection\n",
    "\n",
    "print(json.dumps(example_response_structure, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c3d239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please analyze this user query\n",
      " {\n",
      "  \"name\": \"Joe User\",\n",
      "  \"email\": \"joe.user@example.com\",\n",
      "  \"query\": \"I forgot my password.\",\n",
      "  \"order_id\": null,\n",
      "  \"purchase_date\": null\n",
      "}:\n",
      "\n",
      "Return your analysis as a JSON object matching this exact structure \n",
      "and data types:\n",
      "{'name': 'Example User', 'email': 'user@example.com', 'query': 'I ordered a new computer monitor and it arrived with the screen cracked. I need to exchange it for a new one.', 'order_id': 12345, 'purchase_date': '2025-12-31', 'priority': 'medium', 'category': 'refund_request', 'is_complaint': True, 'tags': ['monitor', 'support', 'exchange']}\n",
      "\n",
      "Respond ONLY with valid JSON. Do not include any explanations or \n",
      "other text or formatting before or after the JSON object.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create prompt with user data and expected JSON structure\n",
    "prompt = f\"\"\"\n",
    "Please analyze this user query\\n {user_input.model_dump_json(indent=2)}:\n",
    "\n",
    "Return your analysis as a JSON object matching this exact structure \n",
    "and data types:\n",
    "{example_response_structure}\n",
    "\n",
    "Respond ONLY with valid JSON. Do not include any explanations or \n",
    "other text or formatting before or after the JSON object.\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da900118",
   "metadata": {},
   "source": [
    "### Define a function to call an LLM and try it with your prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0693378",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# Define a function to call the LLM\n",
    "def call_llm(prompt: str, model: str = \"gpt-4o\"):  # noqa: ANN201\n",
    "    \"\"\"Call a Large Language Model (LLM) with a given prompt and return the response.\n",
    "\n",
    "    This function sends the provided prompt to the configured LLM client using the\n",
    "    chat completion API. It extracts the first message content from the response.\n",
    "    If the response object does not match the expected structure, a ``ValueError``\n",
    "    is raised.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input text or instruction to send to the LLM.\n",
    "        model (str, optional): The model identifier to use for the request.\n",
    "            Defaults to ``\"gpt-4o\"``.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text content from the model's first response choice.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the response object does not contain the expected structure.\n",
    "\n",
    "    Example:\n",
    "        >>> call_llm(\"Write a haiku about autumn leaves.\")\n",
    "        'Golden leaves drifting...'\n",
    "\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model, messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    if not response or not response.choices or not response.choices[0].message.content:\n",
    "        error_msg: str = \"Response object does not contain expected structure.\"\n",
    "        raise ValueError(error_msg)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31aebc",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"name\": \"Joe User\",\n",
      "  \"email\": \"joe.user@example.com\",\n",
      "  \"query\": \"I forgot my password.\",\n",
      "  \"order_id\": null,\n",
      "  \"purchase_date\": null,\n",
      "  \"priority\": \"high\",\n",
      "  \"category\": \"account_help\",\n",
      "  \"is_complaint\": false,\n",
      "  \"tags\": [\"password\", \"support\", \"login\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Get response from LLM\n",
    "response_content = call_llm(prompt)\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b2ed3",
   "metadata": {},
   "source": [
    "### Validate the LLM output using your CustomerQuery model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df9eded",
   "metadata": {},
   "source": [
    "### Note: the following cell will produce a validation error. This is expected. You can simply proceed with the rest of the notebook as cells below do not depend on this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a28c96",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for CustomerQuery\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"name\": \"J...port\", \"login\"]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Attempt to parse the response into CustomerQuery model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m valid_data = \u001b[43mCustomerQuery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_content\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain/study_langchain/.venv/lib/python3.13/site-packages/pydantic/main.py:746\u001b[39m, in \u001b[36mBaseModel.model_validate_json\u001b[39m\u001b[34m(cls, json_data, strict, context, by_alias, by_name)\u001b[39m\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    741\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    742\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    743\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    744\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for CustomerQuery\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"name\": \"J...port\", \"login\"]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid"
     ]
    }
   ],
   "source": [
    "# Attempt to parse the response into CustomerQuery model\n",
    "valid_data = CustomerQuery.model_validate_json(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc028767",
   "metadata": {},
   "source": [
    "### Define a function for error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61877cdd",
   "metadata": {},
   "source": [
    "## Limpeza de respostas JSON\n",
    "\n",
    "Problema: LLMs frequentemente retornam JSON envolto em blocos de código markdown. Para resolver isso vamos adicionar uma função clean_json_response() que remove formatação extra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8278c690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation failed: 1 validation error for CustomerQuery\n",
      "category\n",
      "  Input should be 'refund_request', 'information_request' or 'other' [type=literal_error, input_value='account_help', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/literal_error\n",
      "Cleaned response: {\n",
      "  \"name\": \"Joe User\",\n",
      "  \"email\": \"joe.user@example.com\",\n",
      "  \"query\": \"I forgot my password.\",\n",
      "  \"order_id\": null,\n",
      "  \"purchase_date\": null,\n",
      "  \"priority\": \"high\",\n",
      "  \"category\": \"account_help\",\n",
      "  \"is_complaint\": false,\n",
      "  \"tags\": [\"password\", \"support\", \"login\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# Function to clean JSON response by removing markdown formatting and fixing quotes\n",
    "def clean_json_response(response: str) -> str:\n",
    "    \"\"\"Clean JSON response by removing markdown code blocks and extra formatting.\n",
    "\n",
    "    Args:\n",
    "        response (str): The raw response string from the LLM.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned JSON string with markdown formatting removed and double quotes enforced.\n",
    "\n",
    "    \"\"\"\n",
    "    # Remove markdown code blocks\n",
    "    if response.strip().startswith(\"```\"):\n",
    "        lines = response.strip().split(\"\\n\")\n",
    "        # Remove first line if it starts with ```\n",
    "        if lines[0].startswith(\"```\"):\n",
    "            lines = lines[1:]\n",
    "        # Remove last line if it starts with ```\n",
    "        if lines and lines[-1].startswith(\"```\"):\n",
    "            lines = lines[:-1]\n",
    "        response = \"\\n\".join(lines)\n",
    "\n",
    "    # Replace single quotes with double quotes only outside of values\n",
    "    # This regex replaces only the quotes around keys and values, not inside values\n",
    "    # If you want a more robust solution, use json.loads after fixing the format\n",
    "    # Here, we use a simple regex for demonstration\n",
    "    response = re.sub(r\"(?<!\\\\)'\", '\"', response)\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "# Try to parse the response with cleaning\n",
    "try:\n",
    "    cleaned_response = clean_json_response(response_content)\n",
    "    valid_data = CustomerQuery.model_validate_json(cleaned_response)\n",
    "    print(\"Validation successful!\")\n",
    "    print(valid_data.model_dump_json(indent=2))\n",
    "except ValidationError as e:\n",
    "    print(f\"Validation failed: {e}\")\n",
    "    print(f\"Cleaned response: {cleaned_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4eac4c",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "# Define a function to validate an LLM response\n",
    "def validate_with_model(\n",
    "    data_model: type[BaseModel], llm_response: str\n",
    ") -> tuple[BaseModel | None, str | None]:\n",
    "    \"\"\"Valida a resposta de um LLM contra um modelo Pydantic, limpando-a primeiro.\n",
    "\n",
    "    Esta função recebe a resposta em string de um LLM, tenta limpá-la de\n",
    "    artefatos comuns (como blocos de código Markdown ```json), e então valida\n",
    "    o JSON resultante contra o modelo Pydantic fornecido. Erros de parsing\n",
    "    ou validação são capturados e retornados de forma controlada.\n",
    "\n",
    "    Args:\n",
    "        data_model (Type[BaseModel]): A classe do modelo Pydantic a ser usada\n",
    "            para a validação.\n",
    "        llm_response (str): A resposta em string do LLM, que pode conter o JSON\n",
    "            puro ou estar envolvida em blocos de código Markdown.\n",
    "\n",
    "    Returns:\n",
    "        tuple[BaseModel | None, str | None]: Uma tupla contendo:\n",
    "            - O objeto Pydantic validado em caso de sucesso, ou `None` em caso de falha.\n",
    "            - `None` em caso de sucesso, ou uma string com a mensagem de erro\n",
    "              detalhada em caso de falha de parsing ou validação.\n",
    "\n",
    "    Example:\n",
    "        >>> from pydantic import BaseModel\n",
    "        >>> class User(BaseModel):\n",
    "        ...     name: str\n",
    "        ...     age: int\n",
    "        ...\n",
    "        >>> # Exemplo com JSON \"sujo\" (em um bloco de Markdown)\n",
    "        >>> dirty_json = '```json\\\\n{\"name\": \"Bob\", \"age\": 25}\\\\n```'\n",
    "        >>> validated, error = validate_with_model(User, dirty_json)\n",
    "        >>> print(validated.name)\n",
    "        Bob\n",
    "        >>> print(error)\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Passo 1: Limpar a string de resposta do LLM para remover ```json\n",
    "        if \"```json\" in llm_response:\n",
    "            clean_response = llm_response.split(\"```json\\n\")[1].split(\"\\n```\")[0]\n",
    "        else:\n",
    "            # Caso o LLM retorne o JSON sem o bloco de markdown\n",
    "            clean_response = llm_response.strip().replace(\"```\", \"\")\n",
    "\n",
    "        # Passo 2: Validar o JSON limpo com o modelo Pydantic\n",
    "        validated_data = data_model.model_validate_json(clean_response)\n",
    "\n",
    "        # Se a validação for bem-sucedida\n",
    "        print(\"✅ Validação bem-sucedida!\")\n",
    "        print(validated_data.model_dump_json(indent=2))\n",
    "        return validated_data, None\n",
    "\n",
    "    # Passo 3: Capturar os erros esperados (JSON inválido ou dados incorretos)\n",
    "    except (ValidationError, json.JSONDecodeError, IndexError) as err:\n",
    "        error_message = f\"A resposta gerou um erro de validação ou parsing: {err}\"\n",
    "        print(f\"❌ {error_message}\")\n",
    "        return None, error_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6386ae2",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ A resposta gerou um erro de validação ou parsing: 1 validation error for CustomerQuery\n",
      "category\n",
      "  Input should be 'refund_request', 'information_request' or 'other' [type=literal_error, input_value='account_help', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/literal_error\n"
     ]
    }
   ],
   "source": [
    "# Test your validation function with the LLM response\n",
    "validated_data, validation_error = validate_with_model(CustomerQuery, response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a45068",
   "metadata": {},
   "source": [
    "### Define a function to create a retry prompt including error details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf4c3ba",
   "metadata": {
    "height": 506
   },
   "outputs": [],
   "source": [
    "# Define a function to create a retry prompt with error feedback\n",
    "def create_retry_prompt(\n",
    "    original_prompt: str, original_response: str, error_message: str\n",
    ") -> str:\n",
    "    r\"\"\"Create a retry prompt that incorporates error feedback for the LLM.\n",
    "\n",
    "    This function generates a new prompt intended to help the LLM correct errors\n",
    "    in its previous response. It embeds the original prompt, the original LLM\n",
    "    response, and the associated error message into a structured format. The\n",
    "    instruction at the end ensures that the model outputs only valid JSON without\n",
    "    additional text or explanations.\n",
    "\n",
    "    Args:\n",
    "        original_prompt (str): The original user request sent to the LLM.\n",
    "        original_response (str): The raw LLM response that contained an error.\n",
    "        error_message (str): A description of the validation error or issue with\n",
    "            the original response.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted retry prompt string that guides the LLM to produce a\n",
    "        corrected response in valid JSON format.\n",
    "\n",
    "    Example:\n",
    "        >>> original_prompt = \"Generate user details in JSON format.\"\n",
    "        >>> original_response = \"{'name': 'Alice', 'age': 'twenty'}\"\n",
    "        >>> error_message = \"Field 'age' must be an integer.\"\n",
    "        >>> retry = create_retry_prompt(original_prompt, original_response,\n",
    "    error_message)\n",
    "        >>> print(retry[:100])  # preview the start of the retry prompt\n",
    "        \"\\\\nThis is a request to fix an error in the structure of an llm_response...\"\n",
    "\n",
    "    \"\"\"\n",
    "    retry_prompt: str = (\n",
    "        \"This is a request to fix an error in the structure of an llm_response.\\n\"\n",
    "        \"Here is the original request:\\n\"\n",
    "        \"<original_prompt>\\n\"\n",
    "        f\"{original_prompt}\\n\"\n",
    "        \"</original_prompt>\\n\"\n",
    "        \"\\n\"\n",
    "        \"Here is the original llm_response:\\n\"\n",
    "        \"<llm_response>\\n\"\n",
    "        f\"{original_response}\\n\"\n",
    "        \"</llm_response>\\n\"\n",
    "        \"\\n\"\n",
    "        \"This response generated an error:\\n\"\n",
    "        \"<error_message>\\n\"\n",
    "        f\"{error_message}\\n\"\n",
    "        \"</error_message>\\n\"\n",
    "        \"\\n\"\n",
    "        \"Compare the error message and the llm_response and identify what needs to be\\\n",
    "        fixed or removed in the llm_response to resolve this error.\\n\"\n",
    "        \"Respond ONLY with valid JSON. Do not include any explanations or other text or\\\n",
    "        formatting before or after the JSON string.\"\n",
    "    )\n",
    "    return retry_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9d6a8",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a request to fix an error in the structure of an llm_response.\n",
      "Here is the original request:\n",
      "<original_prompt>\n",
      "\n",
      "Please analyze this user query\n",
      " {\n",
      "  \"name\": \"Joe User\",\n",
      "  \"email\": \"joe.user@example.com\",\n",
      "  \"query\": \"I forgot my password.\",\n",
      "  \"order_id\": null,\n",
      "  \"purchase_date\": null\n",
      "}:\n",
      "\n",
      "Return your analysis as a JSON object matching this exact structure \n",
      "and data types:\n",
      "{'name': 'Example User', 'email': 'user@example.com', 'query': 'I ordered a new computer monitor and it arrived with the screen cracked. I need to exchange it for a new one.', 'order_id': 12345, 'purchase_date': '2025-12-31', 'priority': 'medium', 'category': 'refund_request', 'is_complaint': True, 'tags': ['monitor', 'support', 'exchange']}\n",
      "\n",
      "Respond ONLY with valid JSON. Do not include any explanations or \n",
      "other text or formatting before or after the JSON object.\n",
      "\n",
      "</original_prompt>\n",
      "\n",
      "Here is the original llm_response:\n",
      "<llm_response>\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Joe User\",\n",
      "  \"email\": \"joe.user@example.com\",\n",
      "  \"query\": \"I forgot my password.\",\n",
      "  \"order_id\": null,\n",
      "  \"purchase_date\": null,\n",
      "  \"priority\": \"high\",\n",
      "  \"category\": \"account_help\",\n",
      "  \"is_complaint\": false,\n",
      "  \"tags\": [\"password\", \"support\", \"login\"]\n",
      "}\n",
      "```\n",
      "</llm_response>\n",
      "\n",
      "This response generated an error:\n",
      "<error_message>\n",
      "A resposta gerou um erro de validação ou parsing: 1 validation error for CustomerQuery\n",
      "category\n",
      "  Input should be 'refund_request', 'information_request' or 'other' [type=literal_error, input_value='account_help', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/literal_error\n",
      "</error_message>\n",
      "\n",
      "Compare the error message and the llm_response and identify what needs to be        fixed or removed in the llm_response to resolve this error.\n",
      "Respond ONLY with valid JSON. Do not include any explanations or other text or        formatting before or after the JSON string.\n"
     ]
    }
   ],
   "source": [
    "# Create a retry prompt for validation errors\n",
    "validation_retry_prompt = create_retry_prompt(\n",
    "    original_prompt=prompt,\n",
    "    original_response=response_content,\n",
    "    error_message=validation_error,\n",
    ")\n",
    "\n",
    "print(validation_retry_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4136f8d5",
   "metadata": {},
   "source": [
    "### Call the LLM with your retry prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaed200",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"name\": \"Joe User\",\n",
      "  \"email\": \"joe.user@example.com\",\n",
      "  \"query\": \"I forgot my password.\",\n",
      "  \"order_id\": null,\n",
      "  \"purchase_date\": null,\n",
      "  \"priority\": \"high\",\n",
      "  \"category\": \"other\",\n",
      "  \"is_complaint\": false,\n",
      "  \"tags\": [\"password\", \"support\", \"login\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Call the LLM with the validation retry prompt\n",
    "validation_retry_response = call_llm(validation_retry_prompt)\n",
    "print(validation_retry_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c6bf94",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validação bem-sucedida!\n",
      "{\n",
      "  \"name\": \"Joe User\",\n",
      "  \"email\": \"joe.user@example.com\",\n",
      "  \"query\": \"I forgot my password.\",\n",
      "  \"order_id\": null,\n",
      "  \"purchase_date\": null,\n",
      "  \"priority\": \"high\",\n",
      "  \"category\": \"other\",\n",
      "  \"is_complaint\": false,\n",
      "  \"tags\": [\n",
      "    \"password\",\n",
      "    \"support\",\n",
      "    \"login\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Attempt to validate retry response from LLM\n",
    "validated_data, validation_error = validate_with_model(\n",
    "    CustomerQuery, validation_retry_response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe7564",
   "metadata": {},
   "source": [
    "### Create a second retry prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ac9f0",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a request to fix an error in the structure of an llm_response.\n",
      "Here is the original request:\n",
      "<original_prompt>\n",
      "This is a request to fix an error in the structure of an llm_response.\n",
      "Here is the original request:\n",
      "<original_prompt>\n",
      "\n",
      "Please analyze this user query\n",
      " {\n",
      "  \"name\": \"Joe User\",\n",
      "  \"email\": \"joe.user@example.com\",\n",
      "  \"query\": \"I forgot my password.\",\n",
      "  \"order_id\": null,\n",
      "  \"purchase_date\": null\n",
      "}:\n",
      "\n",
      "Return your analysis as a JSON object matching this exact structure \n",
      "and data types:\n",
      "{'name': 'Example User', 'email': 'user@example.com', 'query': 'I ordered a new computer monitor and it arrived with the screen cracked. I need to exchange it for a new one.', 'order_id': 12345, 'purchase_date': '2025-12-31', 'priority': 'medium', 'category': 'refund_request', 'is_complaint': True, 'tags': ['monitor', 'support', 'exchange']}\n",
      "\n",
      "Respond ONLY with valid JSON. Do not include any explanations or \n",
      "other text or formatting before or after the JSON object.\n",
      "\n",
      "</original_prompt>\n",
      "\n",
      "Here is the original llm_response:\n",
      "<llm_response>\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Joe User\",\n",
      "  \"email\": \"joe.user@example.com\",\n",
      "  \"query\": \"I forgot my password.\",\n",
      "  \"order_id\": null,\n",
      "  \"purchase_date\": null,\n",
      "  \"priority\": \"high\",\n",
      "  \"category\": \"account_help\",\n",
      "  \"is_complaint\": false,\n",
      "  \"tags\": [\"password\", \"support\", \"login\"]\n",
      "}\n",
      "```\n",
      "</llm_response>\n",
      "\n",
      "This response generated an error:\n",
      "<error_message>\n",
      "A resposta gerou um erro de validação ou parsing: 1 validation error for CustomerQuery\n",
      "category\n",
      "  Input should be 'refund_request', 'information_request' or 'other' [type=literal_error, input_value='account_help', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/literal_error\n",
      "</error_message>\n",
      "\n",
      "Compare the error message and the llm_response and identify what needs to be        fixed or removed in the llm_response to resolve this error.\n",
      "Respond ONLY with valid JSON. Do not include any explanations or other text or        formatting before or after the JSON string.\n",
      "</original_prompt>\n",
      "\n",
      "Here is the original llm_response:\n",
      "<llm_response>\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Joe User\",\n",
      "  \"email\": \"joe.user@example.com\",\n",
      "  \"query\": \"I forgot my password.\",\n",
      "  \"order_id\": null,\n",
      "  \"purchase_date\": null,\n",
      "  \"priority\": \"high\",\n",
      "  \"category\": \"other\",\n",
      "  \"is_complaint\": false,\n",
      "  \"tags\": [\"password\", \"support\", \"login\"]\n",
      "}\n",
      "```\n",
      "</llm_response>\n",
      "\n",
      "This response generated an error:\n",
      "<error_message>\n",
      "None\n",
      "</error_message>\n",
      "\n",
      "Compare the error message and the llm_response and identify what needs to be        fixed or removed in the llm_response to resolve this error.\n",
      "Respond ONLY with valid JSON. Do not include any explanations or other text or        formatting before or after the JSON string.\n"
     ]
    }
   ],
   "source": [
    "# Create a second retry prompt for validation errors\n",
    "second_validation_retry_prompt = create_retry_prompt(\n",
    "    original_prompt=validation_retry_prompt,\n",
    "    original_response=validation_retry_response,\n",
    "    error_message=validation_error,\n",
    ")\n",
    "\n",
    "print(second_validation_retry_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165f17d4",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"name\": \"Joe User\",\n",
      "  \"email\": \"joe.user@example.com\",\n",
      "  \"query\": \"I forgot my password.\",\n",
      "  \"order_id\": null,\n",
      "  \"purchase_date\": null,\n",
      "  \"priority\": \"high\",\n",
      "  \"category\": \"information_request\",\n",
      "  \"is_complaint\": false,\n",
      "  \"tags\": [\"password\", \"support\", \"login\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Call the LLM with the second validation retry prompt\n",
    "second_validation_retry_response = call_llm(second_validation_retry_prompt)\n",
    "print(second_validation_retry_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c414cdf2",
   "metadata": {},
   "source": [
    "### Define a function to handle multiple retries in a feedback loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5ffccd",
   "metadata": {
    "height": 659
   },
   "outputs": [],
   "source": [
    "# Define a function to automatically retry an LLM call multiple times\n",
    "def validate_llm_response(prompt, data_model, n_retry=5, model=\"gpt-4o\"):\n",
    "    # Initial LLM call\n",
    "    response_content = call_llm(prompt, model=model)\n",
    "    current_prompt = prompt\n",
    "\n",
    "    # Try to validate with the model\n",
    "    # attempt: 0=initial, 1=first retry, ...\n",
    "    for attempt in range(n_retry + 1):\n",
    "        validated_data, validation_error = validate_with_model(\n",
    "            data_model, response_content\n",
    "        )\n",
    "\n",
    "        if validation_error:\n",
    "            if attempt < n_retry:\n",
    "                print(f\"retry {attempt} of {n_retry} failed, trying again...\")\n",
    "            else:\n",
    "                print(f\"Max retries reached. Last error: {validation_error}\")\n",
    "                return None, (f\"Max retries reached. Last error: {validation_error}\")\n",
    "\n",
    "            validation_retry_prompt = create_retry_prompt(\n",
    "                original_prompt=current_prompt,\n",
    "                original_response=response_content,\n",
    "                error_message=validation_error,\n",
    "            )\n",
    "            response_content = call_llm(validation_retry_prompt, model=model)\n",
    "            current_prompt = validation_retry_prompt\n",
    "            continue\n",
    "\n",
    "        # If you get here, both parsing and validation succeeded\n",
    "        return validated_data, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab75c35",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ A resposta gerou um erro de validação ou parsing: 1 validation error for CustomerQuery\n",
      "category\n",
      "  Input should be 'refund_request', 'information_request' or 'other' [type=literal_error, input_value='account_issue', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/literal_error\n",
      "retry 0 of 5 failed, trying again...\n",
      "✅ Validação bem-sucedida!\n",
      "{\n",
      "  \"name\": \"Joe User\",\n",
      "  \"email\": \"joe.user@example.com\",\n",
      "  \"query\": \"I forgot my password.\",\n",
      "  \"order_id\": null,\n",
      "  \"purchase_date\": null,\n",
      "  \"priority\": \"high\",\n",
      "  \"category\": \"information_request\",\n",
      "  \"is_complaint\": false,\n",
      "  \"tags\": [\n",
      "    \"password\",\n",
      "    \"account\",\n",
      "    \"support\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test your complete solution with the original prompt\n",
    "validated_data, error = validate_llm_response(prompt, CustomerQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872bff52",
   "metadata": {},
   "source": [
    "### Have a look at the JSON schema of your CustomerQuery data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274df4e2",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"description\": \"Represents a structured customer query with classification metadata.\\n\\nThis model extends `UserInput` by adding fields necessary for automated\\nprocessing, routing, and prioritization within a customer support system.\\nIt enriches the raw user input with structured data like priority,\\ncategory, and tags.\\n\\nAttributes:\\n    priority: The urgency of the query. Must be one of 'low', 'medium',\\n        or 'high'.\\n    category: The general topic of the query. Must be one of\\n        'refund_request', 'information_request', or 'other'.\\n    is_complaint: A boolean flag indicating if the query should be\\n        treated as a formal complaint.\\n    tags: A list of keywords that provide additional context for filtering\\n        and analysis.\\n\\nExample:\\n    A typical use case involves parsing a dictionary of data, perhaps from\\n    an API request or a form submission, into a validated model instance.\\n\\n    >>> query_data = {\\n    ...     \\\"name\\\": \\\"John Doe\\\",\\n    ...     \\\"email\\\": \\\"john.doe@example.com\\\",\\n    ...     \\\"query\\\": \\\"My package hasn't arrived yet.\\\",\\n    ...     \\\"order_id\\\": 54321,\\n    ...     \\\"priority\\\": \\\"high\\\",\\n    ...     \\\"category\\\": \\\"other\\\",\\n    ...     \\\"is_complaint\\\": True,\\n    ...     \\\"tags\\\": [\\\"shipping\\\", \\\"delay\\\", \\\"package_tracking\\\"]\\n    ... }\\n    >>> customer_query = CustomerQuery(**query_data)\\n    >>> print(customer_query.priority)\\n    high\\n    >>> print(customer_query.is_complaint)\\n    True\\n    >>> print(customer_query.name)  # Inherited from UserInput\\n    John Doe\",\n",
      "  \"properties\": {\n",
      "    \"name\": {\n",
      "      \"title\": \"Name\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"email\": {\n",
      "      \"format\": \"email\",\n",
      "      \"title\": \"Email\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"query\": {\n",
      "      \"title\": \"Query\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"order_id\": {\n",
      "      \"anyOf\": [\n",
      "        {\n",
      "          \"maximum\": 99999,\n",
      "          \"minimum\": 10000,\n",
      "          \"type\": \"integer\"\n",
      "        },\n",
      "        {\n",
      "          \"type\": \"null\"\n",
      "        }\n",
      "      ],\n",
      "      \"default\": null,\n",
      "      \"description\": \"5-digit order number (cannot start with 0)\",\n",
      "      \"title\": \"Order Id\"\n",
      "    },\n",
      "    \"purchase_date\": {\n",
      "      \"anyOf\": [\n",
      "        {\n",
      "          \"format\": \"date\",\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        {\n",
      "          \"type\": \"null\"\n",
      "        }\n",
      "      ],\n",
      "      \"default\": null,\n",
      "      \"title\": \"Purchase Date\"\n",
      "    },\n",
      "    \"priority\": {\n",
      "      \"description\": \"Priority level: low, medium, high\",\n",
      "      \"title\": \"Priority\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"category\": {\n",
      "      \"description\": \"Query category\",\n",
      "      \"enum\": [\n",
      "        \"refund_request\",\n",
      "        \"information_request\",\n",
      "        \"other\"\n",
      "      ],\n",
      "      \"title\": \"Category\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"is_complaint\": {\n",
      "      \"description\": \"Whether this is a complaint\",\n",
      "      \"title\": \"Is Complaint\",\n",
      "      \"type\": \"boolean\"\n",
      "    },\n",
      "    \"tags\": {\n",
      "      \"description\": \"Relevant keyword tags\",\n",
      "      \"items\": {\n",
      "        \"type\": \"string\"\n",
      "      },\n",
      "      \"title\": \"Tags\",\n",
      "      \"type\": \"array\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"name\",\n",
      "    \"email\",\n",
      "    \"query\",\n",
      "    \"priority\",\n",
      "    \"category\",\n",
      "    \"is_complaint\",\n",
      "    \"tags\"\n",
      "  ],\n",
      "  \"title\": \"CustomerQuery\",\n",
      "  \"type\": \"object\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Investigate the model_json_schema for CustomerQuery\n",
    "data_model_schema = json.dumps(CustomerQuery.model_json_schema(), indent=2)\n",
    "print(data_model_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb71c0",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please analyze this user query\n",
      " {\n",
      "  \"name\": \"Joe User\",\n",
      "  \"email\": \"joe.user@example.com\",\n",
      "  \"query\": \"I forgot my password.\",\n",
      "  \"order_id\": null,\n",
      "  \"purchase_date\": null\n",
      "}:\n",
      "\n",
      "Return your analysis as a JSON object matching this exact structure \n",
      "and data types:\n",
      "{'name': 'Example User', 'email': 'user@example.com', 'query': 'I ordered a new computer monitor and it arrived with the screen cracked. I need to exchange it for a new one.', 'order_id': 12345, 'purchase_date': '2025-12-31', 'priority': 'medium', 'category': 'refund_request', 'is_complaint': True, 'tags': ['monitor', 'support', 'exchange']}\n",
      "\n",
      "Respond ONLY with valid JSON. Do not include any explanations or \n",
      "other text or formatting before or after the JSON object.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the original prompt from above\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975f95d8",
   "metadata": {},
   "source": [
    "### Construct a new prompt using the JSON schema of your data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1aa232",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "# Create new prompt with user input and model_json_schema\n",
    "prompt = f\"\"\"\n",
    "Please analyze this user query\\n {user_input.model_dump_json(indent=2)}:\n",
    "\n",
    "Return your analysis as a JSON object matching the following schema:\n",
    "{data_model_schema}\n",
    "\n",
    "Respond ONLY with valid JSON. Do not include any explanations or \n",
    "other text or formatting before or after the JSON object.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d21919a",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validação bem-sucedida!\n",
      "{\n",
      "  \"name\": \"Joe User\",\n",
      "  \"email\": \"joe.user@example.com\",\n",
      "  \"query\": \"I forgot my password.\",\n",
      "  \"order_id\": null,\n",
      "  \"purchase_date\": null,\n",
      "  \"priority\": \"medium\",\n",
      "  \"category\": \"information_request\",\n",
      "  \"is_complaint\": false,\n",
      "  \"tags\": [\n",
      "    \"password\",\n",
      "    \"account_access\",\n",
      "    \"support\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Run your validate_llm_response function with the new prompt\n",
    "final_analysis, error = validate_llm_response(prompt, CustomerQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30963c1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lesson, you explored how to combine Pydantic models with retry logic to reliably extract structured data from LLM outputs. You practiced building reusable validation functions and prompts, and saw how robust error handling can help you get consistent, usable results from language models. These techniques will help you confidently scale up your LLM-powered workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7674f81",
   "metadata": {},
   "source": [
    "### Foco Geral da Lição: Engenharia de Confiabilidade para LLMs\n",
    "\n",
    "O foco central desta lição não é apenas \"usar Pydantic com um LLM\". O foco é **resolver o problema fundamental da imprevisibilidade dos Modelos de Linguagem**.\n",
    "\n",
    "LLMs são modelos probabilísticos; eles não garantem que a saída será sempre perfeita ou no formato exato que você precisa. Esta lição ensina um padrão de design crucial que podemos chamar de **\"Circuito de Auto-correção para Extração de Dados\"**.\n",
    "\n",
    "A lição demonstra duas estratégias para alcançar essa confiabilidade:\n",
    "\n",
    "1.  **Correção Programática (Retry Loop)**: Aceitar que o LLM vai errar e criar um sistema que:\n",
    "    * **Valida** a saída rigorosamente (com Pydantic).\n",
    "    * **Diagnostica** o erro (capturando a `ValidationError`).\n",
    "    * **Fornece Feedback** ao LLM, pedindo para ele mesmo corrigir o erro.\n",
    "    * **Automatiza** esse ciclo até que o resultado seja satisfatório.\n",
    "\n",
    "2.  **Prevenção via Engenharia de Prompt (JSON Schema)**: A segunda parte da lição mostra que a melhor maneira de corrigir um erro é, na verdade, evitar que ele aconteça. Ao substituir um exemplo vago por um **JSON Schema** formal no prompt, você fornece instruções técnicas e inequívocas ao LLM, diminuindo drasticamente a probabilidade de um erro inicial.\n",
    "\n",
    "Em suma, a lição ensina a tratar a saída de um LLM não como um resultado final, mas como uma primeira tentativa que deve ser validada e, se necessário, refinada através de um loop de feedback inteligente.\n",
    "\n",
    "### Aplicação em LangChain\n",
    "\n",
    "O LangChain foi criado exatamente para abstrair e simplificar os padrões que você construiu manualmente nesta lição.\n",
    "\n",
    "* **Output Parsers**: A ideia de validar e formatar a saída de um LLM é tão central que o LangChain tem um componente inteiro dedicado a isso: os `OutputParsers`.\n",
    "    * O `PydanticOutputParser` faz exatamente o que a segunda parte da lição ensina: ele pega seu modelo Pydantic, gera as instruções de formatação (muitas vezes usando o JSON Schema) para o prompt, e depois valida a resposta do LLM, convertendo-a para uma instância do seu modelo.\n",
    "    * O `RetryWithErrorOutputParser` implementa a lógica de retentativa que você construiu na função `validate_llm_response`. Ele pode ser combinado com outro parser (como o Pydantic parser) e, se a validação inicial falhar, ele automaticamente cria um novo prompt com a mensagem de erro e chama o LLM novamente.\n",
    "\n",
    "Entender esta lição significa que você agora compreende profundamente *o que* esses componentes do LangChain estão fazendo por baixo dos panos, permitindo que você os utilize com muito mais eficácia.\n",
    "\n",
    "### Aplicação em LangGraph\n",
    "\n",
    "Em LangGraph, onde você constrói aplicações com múltiplos passos como um grafo, a confiabilidade de cada passo é ainda mais crítica.\n",
    "\n",
    "* **Nós (Nodes) Robustos**: A função `validate_llm_response` que você analisou é a receita perfeita para um **nó robusto** em um grafo. Você pode criar um \"Nó de Extração de Dados\" que recebe um texto do `Estado (State)` do grafo e tem a responsabilidade de retornar os dados estruturados. Todo o circuito de auto-correção (chamar, validar, tentar novamente) viveria dentro deste nó.\n",
    "* **Arestas Condicionais (Conditional Edges)**: A beleza do LangGraph está no controle explícito do fluxo. Após o seu \"Nó de Extração\" ser executado, uma aresta condicional pode verificar: \"A extração foi bem-sucedida?\".\n",
    "    * **Se sim**: O grafo segue para o próximo passo (ex: \"Tomar Decisão com Base nos Dados\").\n",
    "    * **Se não** (mesmo após as retentativas): O grafo pode ser roteado para um caminho de falha, como um nó de \"Escalonamento para Humano\" ou um que tenta uma abordagem completamente diferente.\n",
    "\n",
    "Esta lição fornece o padrão para construir os blocos de construção (nós) confiáveis que são essenciais para um grafo LangGraph funcional.\n",
    "\n",
    "### Como Usar a Ideia para Criar Agentes Melhores\n",
    "\n",
    "Agentes são a manifestação mais avançada dessas ideias. Um agente precisa constantemente interpretar o mundo, a saída de suas ferramentas e seu próprio \"pensamento\" para tomar decisões.\n",
    "\n",
    "1.  **Chamadas de Ferramentas (Tool Calling) Confiáveis**: Quando um agente decide usar uma ferramenta, a decisão de qual ferramenta chamar e com quais argumentos é uma saída do LLM. Usar o padrão desta lição garante que os argumentos gerados para a ferramenta sejam válidos *antes* da ferramenta ser executada. Se o agente \"alucinar\" um argumento inválido, um loop de retentativa pode forçá-lo a corrigir seu próprio raciocínio.\n",
    "\n",
    "2.  **Parsing da Saída de Ferramentas**: Quando uma ferramenta retorna uma informação (ex: o resultado de uma busca em API), essa informação pode ser complexa. O agente precisa \"entender\" essa saída para planejar seu próximo passo. Aplicar a lógica de extração estruturada e validação nesta saída garante que o agente baseie sua próxima decisão em dados limpos e corretos, e não em uma interpretação errada de uma string complexa.\n",
    "\n",
    "3.  **Agentes Auto-Corretivos**: O conceito de `create_retry_prompt` é a semente de um agente que pode refletir sobre seus próprios erros. Um agente avançado, ao encontrar um erro (seja de uma ferramenta ou de seu próprio plano), pode usar um \"meta-prompt\" similar ao de retentativa para analisar o erro, o contexto e o objetivo, e então formular um novo plano de ação.\n",
    "\n",
    "Em conclusão, a lição ensina o princípio fundamental que transforma um protótipo de IA em um sistema de produção: **nunca confie cegamente na saída do LLM; em vez disso, valide-a rigorosamente e construa mecanismos para que o sistema se recupere de falhas de forma autônoma.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
