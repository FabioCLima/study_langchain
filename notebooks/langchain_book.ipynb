{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee32b11",
   "metadata": {},
   "source": [
    "# Learning Langchain (The book)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6628b8b3",
   "metadata": {},
   "source": [
    "The main task of the software engineer working with LLMs is not to train an LLM, or even to fine-tune one (usually), but rather to take an existing LLM and work out how to get it to accomplish the task you need for your application. Adapting an existing LLM for your task is called `prompt engineering`\n",
    "\n",
    "`prompt engineering with LangChain` - how to use LangChain to get LLMs to do what do what you have in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e4e15",
   "metadata": {},
   "source": [
    "## Prompt technique\n",
    "___\n",
    "\n",
    "* Zero-Shot Prompting\n",
    "* Chain-of-Thought(CoT)\n",
    "* Retrieval-Augmented Generation - in real applications should be combined with CoT\n",
    "* Tool Calling - consist of prepending the prompt with a list of external functions the LLM can make use of, along with descriptions of what is good for nd instructions on how to use one (or more) of these functions.The developer of the application - should parse the output and call the appropriate functions.\n",
    "* Few-Shot Prompting\n",
    "\n",
    "Important things to keep in mind when prompting LLMs: each prompting technique is most useful when used in combination with (some of) the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364b44ed",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "**RAG** é uma técnica que combina **recuperação de informações** com **geração de texto**. Em vez de depender apenas do conhecimento pré-treinado do LLM, o RAG busca informações relevantes em uma base de dados externa e usa essas informações como contexto para gerar respostas mais precisas e atualizadas.\n",
    "\n",
    "### Funcionamento do RAG:\n",
    "1. **Indexação**: Documentos são vetorizados e armazenados\n",
    "2. **Recuperação**: Query do usuário busca documentos similares\n",
    "3. **Geração**: LLM usa documentos recuperados como contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f89c9",
   "metadata": {},
   "source": [
    "`LLM interface simply takes a string prompt as input, sends the input to the model provider, and then returns the model prediction as output.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7871f",
   "metadata": {},
   "source": [
    "## Capítulo 1\n",
    "___\n",
    "\n",
    "a - Chamada a um llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58811059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI  # type: ignore\n",
    "from langchain_core.prompts import ChatPromptTemplate  # type: ignore\n",
    "import os\n",
    "\n",
    "#* Carrega as variáveis de ambiente\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "#* Verifica se a API key está configurada\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY não encontrada no arquivo .env\")\n",
    "\n",
    "#* Configura o modelo com parâmetros específicos\n",
    "model: ChatOpenAI = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,  # Controla a criatividade das respostas\n",
    ")  # type: ignore\n",
    "\n",
    "response = model.invoke(\"The Sky is ?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d956fc3",
   "metadata": {},
   "source": [
    "b - Chatmodel\n",
    "\n",
    "`HumanMessage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bb8325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Brazil is Brasília.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = [HumanMessage(\"What is the capital of Brasil?\")]\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1725a9b",
   "metadata": {},
   "source": [
    "c - System\n",
    "\n",
    "`SystemMessage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57d2fcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris!!!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = ChatOpenAI()\n",
    "system_msg = \"\"\"You are a helpful assistant that responds to questions with three exclamations marks.\n",
    "\"\"\"\n",
    "human_msg = HumanMessage('What is the capital of France?')\n",
    "response = model.invoke([system_msg, human_msg])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e89fd3",
   "metadata": {},
   "source": [
    "Prompt instructions significantly influences the model's output. Prompts help the model understand context and generate relevant answers to queries.\n",
    "\n",
    "LangChain provides prompt template interfaces that make it easy to construct prompts with dynamic inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47b79d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Answer the questions based on the context below.\\nIf the question cannot be answered using the information provided, answer with \"I don\\'t know\".\\n\\nContext: The most recent advancements in NLP are being driven by Large Language Model (LLM). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face\\'s \\'transformers\\' library, or by utilizing OpenAI and cohere\\'s offerings through the `openai` and `cohere` libraries, respectively.\\n\\nQuestion: Which model providers offers LLMs?'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the questions based on the context below.\n",
    "If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\"\"\")\n",
    "\n",
    "response = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Model (LLM). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's 'transformers' library, or by utilizing OpenAI and cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offers LLMs?\"\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "246ae6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responda a questão baseada no contexto a seguir. Se a questão não puder ser respondida usando a informação, responda usando 'Não sei a resposta para essa pergunta!'\n",
      "\n",
      "Contexto: Quais os frameworks para a criação de IAs, chatbots, mais usados para construção de agentes de IA.\n",
      "\n",
      "Pergunta: Qual é o framework mais performático e mais simples de aprender?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "    \"\"\"Responda a questão baseada no contexto a seguir. Se a questão não puder ser respondida usando a informação, responda usando 'Não sei a resposta para essa pergunta!'\n",
    "\n",
    "Contexto: {contexto}\n",
    "\n",
    "Pergunta: {pergunta}\"\"\"\n",
    ")\n",
    "\n",
    "# 2. Invoke corrigido - fora do template e com aspas fechadas\n",
    "response = template.invoke({\n",
    "    \"contexto\": \"Quais os frameworks para a criação de IAs, chatbots, mais usados para construção de agentes de IA.\",\n",
    "    \"pergunta\": \"Qual é o framework mais performático e mais simples de aprender?\"\n",
    "})\n",
    "\n",
    "# 3. PromptValue não tem .content, usar .text ou apenas print(response)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bbe655",
   "metadata": {},
   "source": [
    "`Both template and model can be reused many times`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10f8f0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hugging Face, OpenAI, and Cohere offer Large Language Models (LLMs).' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 132, 'total_tokens': 150, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BzRyPTzlGfPMaGQdcaqegGsWcTvLo', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--e6d0d13a-ff28-4384-b09e-5d30e07e2e85-0' usage_metadata={'input_tokens': 132, 'output_tokens': 18, 'total_tokens': 150, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "# `prompt` and `completion` are the results of using template and model once\n",
    "\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0abc7470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Não sei a resposta para essa pergunta!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 85, 'total_tokens': 93, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_799e4ca3f1', 'id': 'chatcmpl-BzRyPYcRwFRjXEEnwe7DqZOQWMbi3', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--668e05b7-76bd-4e18-b404-b8b91833c0f4-0' usage_metadata={'input_tokens': 85, 'output_tokens': 8, 'total_tokens': 93, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "template = PromptTemplate.from_template(\n",
    "    \"\"\"Responda a questão baseada no contexto a seguir. Se a questão não puder ser respondida usando a informação, responda usando 'Não sei a resposta para essa pergunta!'\n",
    "\n",
    "Contexto: {contexto}\n",
    "\n",
    "Pergunta: {pergunta}\"\"\"\n",
    ")\n",
    "model = ChatOpenAI(model='gpt-4.1')\n",
    "\n",
    "# 2. Invoke corrigido - fora do template e com aspas fechadas\n",
    "prompt = template.invoke({\n",
    "    \"contexto\": \"Quais os frameworks para a criação de IAs, chatbots, mais usados para construção de agentes de IA.\",\n",
    "    \"pergunta\": \"Qual é o framework mais performático e mais simples de aprender?\"\n",
    "})\n",
    "\n",
    "# 3. PromptValue não tem .content, usar .text ou apenas print(response)\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f80bc9a",
   "metadata": {},
   "source": [
    "Building an AI chat application, the `ChatPromptTemplate` can be used instead to provide dynamic inputs based on the role of the chat message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7484cbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Context: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: Which model providers offer LLMs?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
    "        ),\n",
    "        (\"human\", \"Context: {context}\"),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aeb633d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='OpenAI and Cohere offer Large Language Models (LLMs).' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 137, 'total_tokens': 150, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BzRyQBtK9vgTLnOqDe0ONBm2dqgS9', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--e849bb9e-d8a1-4700-9f78-025ea1f40d93-0' usage_metadata={'input_tokens': 137, 'output_tokens': 13, 'total_tokens': 150, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# both `template` and `model` can be reused many times\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
    "        ),\n",
    "        (\"human\", \"Context: {context}\"),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# `prompt` and `completion` are the results of using template and model once\n",
    "\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a632c",
   "metadata": {},
   "source": [
    "1. Padrão de Prompts Dinâmicos - Sim, é um padrão estabelecido!\n",
    "O código que você está vendo segue um padrão muito comum e recomendado no LangChain:\n",
    "Por que este padrão é importante?\n",
    "```python\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Instrução do sistema...\"),\n",
    "    (\"human\", \"Entrada do usuário: {variavel}\"),\n",
    "])\n",
    "```\n",
    "Reutilização: O template pode ser usado múltiplas vezes com diferentes dados\n",
    "* Separação de responsabilidades: O prompt (template) é separado da execução (model)\n",
    "* Manutenibilidade: Fácil de modificar e testar prompts\n",
    "* Escalabilidade: Pode ser usado em pipelines complexos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a95a3",
   "metadata": {},
   "source": [
    "Quando usar:\n",
    "* Definir o papel(role)/comportamento(behave) do modelo.\n",
    "* Estabelecer regras e limitações\n",
    "* Configurar o contexto geral da conversa\n",
    "* Definir o tom e estilo de resposta.\n",
    "\n",
    "\n",
    "```python\n",
    "(\"system\", \"Você é um tutor de programação Python. Sempre explique conceitos de forma simples e forneça exemplos práticos.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b898b536",
   "metadata": {},
   "source": [
    "`HumanMessage` (Mensagem Humana)\n",
    "\n",
    "```python\n",
    "(\"human\", \"Pergunta: {question}\")\n",
    "```\n",
    "\n",
    "Quando Usar:\n",
    "\n",
    "* Entradas do usuário\n",
    "* Perguntas específicas\n",
    "* Dados que precisam ser processados\n",
    "* Contexto que o usuário fornece\n",
    "\n",
    "```python\n",
    "(\"human\", \"Explique o conceito de list comprehension em Python\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f531d8df",
   "metadata": {},
   "source": [
    "`AIMessage (Mensagem da IA)`\n",
    "```python\n",
    "(\"ai\", \"Resposta da IA...\")\n",
    "```\n",
    "\n",
    "Quando Usar:\n",
    "\n",
    "* Simular conversar anteriores\n",
    "* Fornecer exemplos de respostas\n",
    "* Criar contextos de few-shot learning\n",
    "* Manter histórico de conversas\n",
    "\n",
    "Exemplo prático:\n",
    "```python\n",
    "(\"ai\", \"List comprehension é uma forma concisa de criar listas em Python. Exemplo: [x*2 for x in range(5)]\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e2bd6",
   "metadata": {},
   "source": [
    "`Estrutura Recomendada para Prompts Dinâmicos`\n",
    "___\n",
    "```python\n",
    "# 1. Definir o template com tipos apropriados\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Defina o papel/contexto do modelo\"),\n",
    "    (\"human\", \"Forneça o contexto: {context}\"),\n",
    "    (\"human\", \"Faça a pergunta: {question}\"),\n",
    "])\n",
    "\n",
    "# 2. Criar o modelo\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# 3. Invocar com dados dinâmicos\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"seu contexto aqui\",\n",
    "    \"question\": \"sua pergunta aqui\"\n",
    "})\n",
    "\n",
    "# 4. Obter a resposta\n",
    "response = model.invoke(prompt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace97d30",
   "metadata": {},
   "source": [
    "`**Boas Práticas para Desenvolvedores de Agentes**`\n",
    "___\n",
    "\n",
    "1.Sempre use templates para prompts dinâmicos\n",
    "\n",
    "    -Evite strings hardcoded\n",
    "    -Facilita testes e modificações\n",
    "\n",
    "2.Separe claramente os tipos de mensagens\n",
    "\n",
    "    -System: configuração/contexto\n",
    "    -Human: entrada do usuário\n",
    "    -AI: respostas/exemplos\n",
    "\n",
    "3.Use variáveis para dados dinâmicos\n",
    "\n",
    "    -{context}, {question}, {user_input}\n",
    "    -Facilita a reutilização\n",
    "\n",
    "4.Mantenha prompts modulares\n",
    "\n",
    "    -Cada template com responsabilidade específica\n",
    "    -Combine templates para casos complexos\n",
    "\n",
    "**Versão básica**\n",
    "```python\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Responda baseado no contexto.\"),\n",
    "    (\"human\", \"Contexto: {context}\"),\n",
    "    (\"human\", \"Pergunta: {question}\"),\n",
    "])\n",
    "```\n",
    "**Versão avançada(para agentes)**\n",
    "```python\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um assistente especializado em análise de dados.\"),\n",
    "    (\"human\", \"Contexto: {context}\"),\n",
    "    (\"human\", \"Pergunta: {question}\"),\n",
    "    (\"ai\", \"Vou analisar o contexto e responder sua pergunta.\"),\n",
    "    (\"human\", \"Por favor, seja específico e forneça exemplos.\"),\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18157cbe",
   "metadata": {},
   "source": [
    "Para lembrar:\n",
    "\n",
    " - `SystemMessage` = \"Quem você é\"\n",
    " - `HumanMessage` = \"O que o usuário quer\"\n",
    " - `AIMessage` = \"Como você responde\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f0c28b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning é um campo da inteligência artificial que se concentra no desenvolvimento de algoritmos e modelos que permitem aos computadores aprender e melhorar a partir de dados. Esses modelos são treinados para fazer previsões, identificar padrões e tomar decisões sem serem explicitamente programados para cada tarefa.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Você é um assistente prestativo, especialista em Matemática.\"),\n",
    "    HumanMessage(content=\"Explique o que é Machine Learning em 50 palavras.\")\n",
    "]\n",
    "\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3c4aa4",
   "metadata": {},
   "source": [
    "`PromptTemplate` simples é só para texto, `ChatPromptTemplate` é para conversas com roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9a6d371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain é um framework de código aberto que facilita algoritmos e modelos de machine learning de linguagem natural. Ele fornece componentes pré-construídos e ferramentas para desenvolvedores criarem aplicações de processamento de linguagem.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"Você é um especialista em {tópico}\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"Explique {assunto} em 35 palavras\")\n",
    "])\n",
    "\n",
    "# Template\n",
    "formatted_prompt = prompt.format_messages(\n",
    "    tópico=\"framework\",\n",
    "    assunto=\"langchain\"\n",
    "    )\n",
    "\n",
    "response = chat.invoke(formatted_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af41a7e3",
   "metadata": {},
   "source": [
    "## Getting Specific Formats out of LLMs\n",
    "___\n",
    "\n",
    "The most common format to generate with LLMs is JSON, which can be sent over the to your frontend code or be saved to a databased.\n",
    "\n",
    "1. First task, define the output schema (from LLM). Then include that schema in the prompt , along with the text you want to use as the source.\n",
    "2. In order to define a `schema`, this is easiest to do with `Pydantic`(*library* used for validating data against schemas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "115f857d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='They weigh the same.' justification='A pound is a unit of weight. Therefore, a pound of bricks and a pound of feathers both weigh exactly one pound, regardless of the material. The difference is in their volume and density, not their weight.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    \"\"\"An answer to the user's question along with justification for the answer.\"\"\"\n",
    "    answer: str\n",
    "    \"\"\"The answer to the user's question\"\"\"\n",
    "    justification: str\n",
    "    \"\"\"Justification for the answer\"\"\"\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "structured_model = model.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "response = structured_model.invoke(\n",
    "    \"What weighs more, a pound of bricks or a pound of feathers\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec1ce2d",
   "metadata": {},
   "source": [
    "#### TUTORIAL: LangChain + Pandantic - Structured Output\n",
    "========================================================\n",
    "\n",
    "Este exemplo demonstra como usar Pydantic com LangChain para:\n",
    "1. Validar dados de entrada e saída\n",
    "2. Garantir formato estruturado das respostas\n",
    "3. Tornar o código mais robusto e type-safe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c5aa3276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1066a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerWithJustification(BaseModel):\n",
    "    \"\"\"\n",
    "    Modelo que representa uma resposta estruturada com justificativa.\n",
    "\n",
    "    💡 Por que usar Pydantic aqui?\n",
    "    - Validação automática de tipos\n",
    "    - Documentação clara dos campos\n",
    "    - Serialização JSON automática\n",
    "    - Integração nativa com LangChain\n",
    "    \"\"\"\n",
    "    answer: str = Field(\n",
    "        description=\"Resposta clara e concisa para a pergunta\",\n",
    "        min_length=1,\n",
    "        max_length=500\n",
    "    )\n",
    "\n",
    "    justification: str = Field(\n",
    "        description = \"Explicação detalhada do raciocínio\",\n",
    "        min_length=10,\n",
    "        max_length=100\n",
    "    )\n",
    "\n",
    "    confidence_level: Optional[float] = Field(\n",
    "        default=None,\n",
    "        description=\"Nível de confiança (0-1) na resposta\",\n",
    "        ge=0.0,\n",
    "        le=1.0\n",
    "    )\n",
    "\n",
    "    timestamp: Optional[str] = Field(\n",
    "        default_factory=lambda: datetime.now().isoformat(),\n",
    "        description=\"Timestamp da resposta\"\n",
    "    )\n",
    "\n",
    "    def display_response(self) ->str:\n",
    "        \"\"\"\n",
    "        Método para exibir a resposta de forma mais legível.\n",
    "\n",
    "        🔧 BENEFÍCIO: Evita o scroll infinito no output!\n",
    "        \"\"\"\n",
    "        output = f\"\"\"\n",
    "┌─ 💬 RESPOSTA ─────────────────────────────────────────┐\n",
    "│ {self.answer}\n",
    "├─ 🤔 JUSTIFICATIVA ───────────────────────────────────┤\n",
    "│ {self.justification}\n",
    "\"\"\"\n",
    "\n",
    "        if self.confidence_level:\n",
    "            confidence_bar = \"█\" * int(self.confidence_level * 10)\n",
    "            output += f\"\"\"├─ 📊 CONFIANÇA ───────────────────────────────────────┤\n",
    "│ {confidence_bar} {self.confidence_level:.1%}\n",
    "\"\"\"\n",
    "\n",
    "        output += f\"\"\"└─ 🕒 {self.timestamp} ──────────────────────────────────┘\"\"\"\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6881489d",
   "metadata": {},
   "source": [
    "#### 🚀 CLASSE PRINCIPAL PARA DEMONSTRAÇÃO\n",
    "\n",
    "`class LangChainPydanticDemo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c344c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangChainPydanticDemo:\n",
    "    \"\"\"\n",
    "    Conceitos Importantes:\n",
    "    1. Structured Output: LangChain força o LLM a retornar dados no formato exato do\n",
    "    modelo Pydantic.\n",
    "\n",
    "    2.Type safety: Pydantic valida automaticamente os tipos\n",
    "\n",
    "    3.Error Handling: Falhas na validação são capturadas\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name = \"gpt-4.1\"):\n",
    "        self.model = ChatOpenAI(\n",
    "                model = model_name,\n",
    "                temperature=0.1, #* baixa criatividade para respostas consistentes\n",
    "        )\n",
    "\n",
    "    #* 🔑 PONTO CHAVE: with_structured_output() força o formato\n",
    "        self.structured_model = self.model.with_structured_output(\n",
    "            AnswerWithJustification\n",
    "        )\n",
    "\n",
    "    def ask_question(self, question: str) -> AnswerWithJustification:\n",
    "        \"\"\"\n",
    "        Faz uma pergunta e retorna resposta estruturada.\n",
    "\n",
    "        🎯 BENEFÍCIOS do Structured Output:\n",
    "        - Resposta sempre no formato esperado\n",
    "        - Validação automática dos dados\n",
    "        - Facilita integração com APIs e bancos de dados\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.structured_model.invoke(question)\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            #* Em caso de erro, retorna resposta padrão\n",
    "            return AnswerWithJustification(\n",
    "                answer=\"Erro ao processar pergunta\",\n",
    "                justification=f\"Erro técnico: {str(e)}\",\n",
    "                confidence_level=0.0\n",
    "            )\n",
    "\n",
    "    def demo_multiple_questions(self):\n",
    "        \"\"\"\n",
    "        Demonstra o uso com múltiplas perguntas para mostrar consistência.\n",
    "        \"\"\"\n",
    "        questions = [\n",
    "            \"O que pesa mais: 1kg de chumbo ou 1kg de algodão?\",\n",
    "            \"Por que o céu é azul?\",\n",
    "            \"Qual a diferença entre Python e JavaScript?\"\n",
    "        ]\n",
    "\n",
    "        print(\"🎓 DEMONSTRAÇÃO: Múltiplas perguntas com formato consistente\\n\")\n",
    "\n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"❓ PERGUNTA {i}: {question}\")\n",
    "            response = self.ask_question(question)\n",
    "            print(response.display_response())\n",
    "            print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e19c7e6",
   "metadata": {},
   "source": [
    "#### 📖 EXEMPLOS PRÁTICOS DE USO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "76ae2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exemplo_basico():\n",
    "    \"\"\"Exemplo básico de uso do structured output.\"\"\"\n",
    "    print(\"🔵 EXEMPLO 1: Uso Básico\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    demo = LangChainPydanticDemo()\n",
    "\n",
    "    question = \"Explique o conceito de recursão em programação\"\n",
    "    response = demo.ask_question(question)\n",
    "\n",
    "    # ✨ SAÍDA LIMPA - sem scroll infinito!\n",
    "    print(response.display_response())\n",
    "\n",
    "    # 💾 BONUS: Fácil conversão para JSON\n",
    "    print(\"\\n🔧 BONUS - Dados em JSON:\")\n",
    "    print(json.dumps(response.model_dump(), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1fdb5c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 EXEMPLO 1: Uso Básico\n",
      "----------------------------------------\n",
      "\n",
      "┌─ 💬 RESPOSTA ─────────────────────────────────────────┐\n",
      "│ Recursão é um conceito em programação onde uma função chama a si mesma para resolver um problema, geralmente dividindo-o em subproblemas menores e semelhantes ao original.\n",
      "├─ 🤔 JUSTIFICATIVA ───────────────────────────────────┤\n",
      "│ A recursão é usada quando um problema pode ser decomposto em versões menores de si mesmo. Por meio d\n",
      "├─ 📊 CONFIANÇA ───────────────────────────────────────┤\n",
      "│ █████████ 98.0%\n",
      "└─ 🕒 2024-06-19T18:00:00Z ──────────────────────────────────┘\n",
      "\n",
      "🔧 BONUS - Dados em JSON:\n",
      "{\n",
      "  \"answer\": \"Recursão é um conceito em programação onde uma função chama a si mesma para resolver um problema, geralmente dividindo-o em subproblemas menores e semelhantes ao original.\",\n",
      "  \"justification\": \"A recursão é usada quando um problema pode ser decomposto em versões menores de si mesmo. Por meio d\",\n",
      "  \"confidence_level\": 0.98,\n",
      "  \"timestamp\": \"2024-06-19T18:00:00Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "exemplo_basico()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a92e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exemplo_avancado():\n",
    "    \"\"\"Exemplo avançado mostrando validação.\"\"\"\n",
    "    print(\"\\n🟢 EXEMPLO 2: Validação com Pydantic\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Tentativa de criar resposta inválida (para mostrar validação)\n",
    "    try:\n",
    "        invalid_response = AnswerWithJustification(\n",
    "            answer=\"\",  # ❌ Inválido: muito curto\n",
    "            justification=\"Curto\",  # ❌ Inválido: muito curto\n",
    "            confidence_level=1.5  # ❌ Inválido: > 1.0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ VALIDAÇÃO PYDANTIC FUNCIONOU: {e}\")\n",
    "\n",
    "    # ✅ Resposta válida\n",
    "    valid_response = AnswerWithJustification(\n",
    "        answer=\"Python é uma linguagem de programação\",\n",
    "        justification=\"Python é conhecida por sua sintaxe simples e legível\",\n",
    "        confidence_level=0.9\n",
    "    )\n",
    "\n",
    "    print(\"\\n✅ RESPOSTA VÁLIDA:\")\n",
    "    print(valid_response.display_response())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "96f501d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 EXEMPLO 2: Validação com Pydantic\n",
      "----------------------------------------\n",
      "❌ VALIDAÇÃO PYDANTIC FUNCIONOU: 3 validation errors for AnswerWithJustification\n",
      "answer\n",
      "  String should have at least 1 character [type=string_too_short, input_value='', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_too_short\n",
      "justification\n",
      "  String should have at least 10 characters [type=string_too_short, input_value='Curto', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_too_short\n",
      "confidence_level\n",
      "  Input should be less than or equal to 1 [type=less_than_equal, input_value=1.5, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/less_than_equal\n",
      "\n",
      "✅ RESPOSTA VÁLIDA:\n",
      "\n",
      "┌─ 💬 RESPOSTA ─────────────────────────────────────────┐\n",
      "│ Python é uma linguagem de programação\n",
      "├─ 🤔 JUSTIFICATIVA ───────────────────────────────────┤\n",
      "│ Python é conhecida por sua sintaxe simples e legível\n",
      "├─ 📊 CONFIANÇA ───────────────────────────────────────┤\n",
      "│ █████████ 90.0%\n",
      "└─ 🕒 2025-07-31T19:40:09.245567 ──────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "exemplo_avancado()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2224df6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎓 DEMONSTRAÇÃO COMPLETA:\n",
      "============================================================\n",
      "🎓 DEMONSTRAÇÃO: Múltiplas perguntas com formato consistente\n",
      "\n",
      "❓ PERGUNTA 1: O que pesa mais: 1kg de chumbo ou 1kg de algodão?\n",
      "\n",
      "┌─ 💬 RESPOSTA ─────────────────────────────────────────┐\n",
      "│ 1kg de chumbo e 1kg de algodão pesam exatamente o mesmo: 1kg cada.\n",
      "├─ 🤔 JUSTIFICATIVA ───────────────────────────────────┤\n",
      "│ A pergunta compara massas iguais de materiais diferentes. O peso é uma medida da força gravitacional\n",
      "├─ 📊 CONFIANÇA ───────────────────────────────────────┤\n",
      "│ ██████████ 100.0%\n",
      "└─ 🕒 2023-06-19T12:00:00Z ──────────────────────────────────┘\n",
      "\n",
      "============================================================\n",
      "\n",
      "❓ PERGUNTA 2: Por que o céu é azul?\n",
      "\n",
      "┌─ 💬 RESPOSTA ─────────────────────────────────────────┐\n",
      "│ O céu é azul porque a luz do Sol, ao passar pela atmosfera da Terra, sofre espalhamento pelas moléculas de ar, e a luz azul é espalhada em todas as direções com mais intensidade do que outras cores.\n",
      "├─ 🤔 JUSTIFICATIVA ───────────────────────────────────┤\n",
      "│ A luz branca do Sol é composta por várias cores, cada uma com um comprimento de onda diferente. Ao a\n",
      "├─ 📊 CONFIANÇA ───────────────────────────────────────┤\n",
      "│ █████████ 99.0%\n",
      "└─ 🕒 2024-06-18T19:00:00Z ──────────────────────────────────┘\n",
      "\n",
      "============================================================\n",
      "\n",
      "❓ PERGUNTA 3: Qual a diferença entre Python e JavaScript?\n",
      "\n",
      "┌─ 💬 RESPOSTA ─────────────────────────────────────────┐\n",
      "│ Python e JavaScript são linguagens de programação diferentes, cada uma com suas características e usos principais. Python é uma linguagem de propósito geral, muito usada em ciência de dados, automação, backend e inteligência artificial. JavaScript é focada principalmente no desenvolvimento web, especialmente para criar interatividade em páginas web.\n",
      "├─ 🤔 JUSTIFICATIVA ───────────────────────────────────┤\n",
      "│ Python tem sintaxe simples, é fortemente tipada (mas dinamicamente), e é conhecida pela legibilidade\n",
      "├─ 📊 CONFIANÇA ───────────────────────────────────────┤\n",
      "│ ██████████ 100.0%\n",
      "└─ 🕒 2024-06-18T19:00:00Z ──────────────────────────────────┘\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstração completa\n",
    "print(\"\\n🎓 DEMONSTRAÇÃO COMPLETA:\")\n",
    "print(\"=\"*60)\n",
    "demo = LangChainPydanticDemo()\n",
    "demo.demo_multiple_questions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61a6bc3",
   "metadata": {},
   "source": [
    "🎓 LIÇÕES APRENDIDAS:\n",
    "\n",
    "1. **Pydantic + LangChain = Dados Estruturados**\n",
    "   - LLM retorna sempre no formato correto\n",
    "   - Validação automática de tipos e valores\n",
    "\n",
    "2. **Benefícios para Produção**\n",
    "   - Code completion no IDE\n",
    "   - Documentação automática\n",
    "   - Integração fácil com APIs\n",
    "\n",
    "3. **Melhor UX**\n",
    "   - Saída formatada e legível\n",
    "   - Sem scroll infinito\n",
    "   - Informações organizadas\n",
    "\n",
    "4. **Type Safety**\n",
    "   - Erros capturados em desenvolvimento\n",
    "   - Código mais robusto\n",
    "   - Manutenção facilitada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f84ea1",
   "metadata": {},
   "source": [
    "**Por que Pydantic é Fundamental no LangChain**:\n",
    "\n",
    "1. Structured Output: Força o LLM a retornar dados no formato exato\n",
    "2. Validação Automática: Tipos e valores são verificados automaticamente\n",
    "3. Integração Nativa: LangChain foi projetado para trabalhar com Pydantic\n",
    "4. Produção Ready: Facilita APIs, banco de dados e frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365e92c",
   "metadata": {},
   "source": [
    "#### Other Machine-Readable Formats with Output Parsers\n",
    "___\n",
    "\n",
    "We can also use an LLM or chat model to produce output in other formats, such as CSV or XML. Using output parsers  that are classes that help us structure large language model responses.\n",
    "\n",
    "    - Providing format instructions\n",
    "    - Validating and parsing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c4a436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'cherry']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "response = parser.invoke(\"apple, banana, cherry\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
