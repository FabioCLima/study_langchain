{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee32b11",
   "metadata": {},
   "source": [
    "# Learning Langchain (The book)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6628b8b3",
   "metadata": {},
   "source": [
    "The main task of the software engineer working with LLMs is not to train an LLM, or even to fine-tune one (usually), but rather to take an existing LLM and work out how to get it to accomplish the task you need for your application. Adapting an existing LLM for your task is called `prompt engineering`\n",
    "\n",
    "`prompt engineering with LangChain` - how to use LangChain to get LLMs to do what do what you have in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e4e15",
   "metadata": {},
   "source": [
    "## Prompt technique\n",
    "___\n",
    "\n",
    "* Zero-Shot Prompting\n",
    "* Chain-of-Thought(CoT)\n",
    "* Retrieval-Augmented Generation - in real applications should be combined with CoT\n",
    "* Tool Calling - consist of prepending the prompt with a list of external functions the LLM can make use of, along with descriptions of what is good for nd instructions on how to use one (or more) of these functions.The developer of the application - should parse the output and call the appropriate functions.\n",
    "* Few-Shot Prompting\n",
    "\n",
    "Important things to keep in mind when prompting LLMs: each prompting technique is most useful when used in combination with (some of) the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364b44ed",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "**RAG** é uma técnica que combina **recuperação de informações** com **geração de texto**. Em vez de depender apenas do conhecimento pré-treinado do LLM, o RAG busca informações relevantes em uma base de dados externa e usa essas informações como contexto para gerar respostas mais precisas e atualizadas.\n",
    "\n",
    "### Funcionamento do RAG:\n",
    "1. **Indexação**: Documentos são vetorizados e armazenados\n",
    "2. **Recuperação**: Query do usuário busca documentos similares\n",
    "3. **Geração**: LLM usa documentos recuperados como contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f89c9",
   "metadata": {},
   "source": [
    "`LLM interface simply takes a string prompt as input, sends the input to the model provider, and then returns the model prediction as output.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7871f",
   "metadata": {},
   "source": [
    "## Capítulo 1\n",
    "___\n",
    "\n",
    "a - Chamada a um llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "58811059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate  # type: ignore\n",
    "from langchain_openai import ChatOpenAI  # type: ignore\n",
    "\n",
    "# * Carrega as variáveis de ambiente\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# * Verifica se a API key está configurada\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY não encontrada no arquivo .env\")\n",
    "\n",
    "# * Configura o modelo com parâmetros específicos\n",
    "model: ChatOpenAI = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,  # Controla a criatividade das respostas\n",
    ")  # type: ignore\n",
    "\n",
    "response = model.invoke(\"The Sky is ?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d956fc3",
   "metadata": {},
   "source": [
    "b - Chatmodel\n",
    "\n",
    "`HumanMessage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7bb8325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Brasil is Brasília.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = [HumanMessage(\"What is the capital of Brasil?\")]\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1725a9b",
   "metadata": {},
   "source": [
    "c - System\n",
    "\n",
    "`SystemMessage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "57d2fcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris!!!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "system_msg = \"\"\"You are a helpful assistant that responds to questions with three exclamations marks.\n",
    "\"\"\"\n",
    "human_msg = HumanMessage(\"What is the capital of France?\")\n",
    "response = model.invoke([system_msg, human_msg])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e89fd3",
   "metadata": {},
   "source": [
    "Prompt instructions significantly influences the model's output. Prompts help the model understand context and generate relevant answers to queries.\n",
    "\n",
    "LangChain provides prompt template interfaces that make it easy to construct prompts with dynamic inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "47b79d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Answer the questions based on the context below.\\nIf the question cannot be answered using the information provided, answer with \"I don\\'t know\".\\n\\nContext: The most recent advancements in NLP are being driven by Large Language Model (LLM). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face\\'s \\'transformers\\' library, or by utilizing OpenAI and cohere\\'s offerings through the `openai` and `cohere` libraries, respectively.\\n\\nQuestion: Which model providers offers LLMs?'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the questions based on the context below.\n",
    "If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\"\"\")\n",
    "\n",
    "response = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Model (LLM). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's 'transformers' library, or by utilizing OpenAI and cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offers LLMs?\",\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "246ae6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responda a questão baseada no contexto a seguir. Se a questão não puder ser respondida usando a informação, responda usando 'Não sei a resposta para essa pergunta!'\n",
      "\n",
      "Contexto: Quais os frameworks para a criação de IAs, chatbots, mais usados para construção de agentes de IA.\n",
      "\n",
      "Pergunta: Qual é o framework mais performático e mais simples de aprender?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "    \"\"\"Responda a questão baseada no contexto a seguir. Se a questão não puder ser respondida usando a informação, responda usando 'Não sei a resposta para essa pergunta!'\n",
    "\n",
    "Contexto: {contexto}\n",
    "\n",
    "Pergunta: {pergunta}\"\"\"\n",
    ")\n",
    "\n",
    "# 2. Invoke corrigido - fora do template e com aspas fechadas\n",
    "response = template.invoke(\n",
    "    {\n",
    "        \"contexto\": \"Quais os frameworks para a criação de IAs, chatbots, mais usados para construção de agentes de IA.\",\n",
    "        \"pergunta\": \"Qual é o framework mais performático e mais simples de aprender?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3. PromptValue não tem .content, usar .text ou apenas print(response)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bbe655",
   "metadata": {},
   "source": [
    "`Both template and model can be reused many times`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10f8f0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hugging Face, OpenAI, and Cohere offer LLMs.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 132, 'total_tokens': 147, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C07tYZMNve7m7F1WJWItcd3KuXOdR', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--0d9513c6-c043-433a-8a43-b082338c1872-0' usage_metadata={'input_tokens': 132, 'output_tokens': 15, 'total_tokens': 147, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "# `prompt` and `completion` are the results of using template and model once\n",
    "\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0abc7470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Não sei a resposta para essa pergunta!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 85, 'total_tokens': 93, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_51e1070cf2', 'id': 'chatcmpl-C07tZAYbUcPzg4OQ1xQuBpro9lEdr', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--f24761cf-de75-4448-a62a-e641b2954c72-0' usage_metadata={'input_tokens': 85, 'output_tokens': 8, 'total_tokens': 93, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "    \"\"\"Responda a questão baseada no contexto a seguir. Se a questão não puder ser respondida usando a informação, responda usando 'Não sei a resposta para essa pergunta!'\n",
    "\n",
    "Contexto: {contexto}\n",
    "\n",
    "Pergunta: {pergunta}\"\"\"\n",
    ")\n",
    "model = ChatOpenAI(model=\"gpt-4.1\")\n",
    "\n",
    "# 2. Invoke corrigido - fora do template e com aspas fechadas\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"contexto\": \"Quais os frameworks para a criação de IAs, chatbots, mais usados para construção de agentes de IA.\",\n",
    "        \"pergunta\": \"Qual é o framework mais performático e mais simples de aprender?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3. PromptValue não tem .content, usar .text ou apenas print(response)\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "64d9448c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não sei a resposta para essa pergunta!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "\"\"\"Responda a pergunta baseada no contexto a seguir. Se a pergunta não puder ser respondida usando a informação,\n",
    "previamente fornecida, responda com um 'Não sei a resposta para essa pergunta!'\n",
    "\n",
    "Contexto: {contexto}\n",
    "Pergunta: {pergunta}\n",
    "\n",
    "\"\"\")\n",
    "model = ChatOpenAI(model=\"gpt-4.1\")\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"contexto\": \"Considerando a programação orientada a objetos no Python, o que é um objeto\",\n",
    "        \"pergunta\": \"Como criar uma classe em Python usando o conceito de encapsulamento?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f80bc9a",
   "metadata": {},
   "source": [
    "Building an AI chat application, the `ChatPromptTemplate` can be used instead to provide dynamic inputs based on the role of the chat message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7484cbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Context: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: Which model providers offer LLMs?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
    "        ),\n",
    "        (\"human\", \"Context: {context}\"),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aeb633d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='OpenAI and Cohere.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 137, 'total_tokens': 143, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C07taxiItparR9kKemjMdGcIiheiS', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--bf198af0-547e-4ff9-9bb8-cb979a680a90-0' usage_metadata={'input_tokens': 137, 'output_tokens': 6, 'total_tokens': 143, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# both `template` and `model` can be reused many times\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
    "        ),\n",
    "        (\"human\", \"Context: {context}\"),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# `prompt` and `completion` are the results of using template and model once\n",
    "\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a632c",
   "metadata": {},
   "source": [
    "1. Padrão de Prompts Dinâmicos - Sim, é um padrão estabelecido!\n",
    "O código que você está vendo segue um padrão muito comum e recomendado no LangChain:\n",
    "Por que este padrão é importante?\n",
    "```python\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Instrução do sistema...\"),\n",
    "    (\"human\", \"Entrada do usuário: {variavel}\"),\n",
    "])\n",
    "```\n",
    "Reutilização: O template pode ser usado múltiplas vezes com diferentes dados\n",
    "* Separação de responsabilidades: O prompt (template) é separado da execução (model)\n",
    "* Manutenibilidade: Fácil de modificar e testar prompts\n",
    "* Escalabilidade: Pode ser usado em pipelines complexos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a95a3",
   "metadata": {},
   "source": [
    "Quando usar:\n",
    "* Definir o papel(role)/comportamento(behave) do modelo.\n",
    "* Estabelecer regras e limitações\n",
    "* Configurar o contexto geral da conversa\n",
    "* Definir o tom e estilo de resposta.\n",
    "\n",
    "\n",
    "```python\n",
    "(\"system\", \"Você é um tutor de programação Python. Sempre explique conceitos de forma simples e forneça exemplos práticos.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b898b536",
   "metadata": {},
   "source": [
    "`HumanMessage` (Mensagem Humana)\n",
    "\n",
    "```python\n",
    "(\"human\", \"Pergunta: {question}\")\n",
    "```\n",
    "\n",
    "Quando Usar:\n",
    "\n",
    "* Entradas do usuário\n",
    "* Perguntas específicas\n",
    "* Dados que precisam ser processados\n",
    "* Contexto que o usuário fornece\n",
    "\n",
    "```python\n",
    "(\"human\", \"Explique o conceito de list comprehension em Python\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f531d8df",
   "metadata": {},
   "source": [
    "`AIMessage (Mensagem da IA)`\n",
    "```python\n",
    "(\"ai\", \"Resposta da IA...\")\n",
    "```\n",
    "\n",
    "Quando Usar:\n",
    "\n",
    "* Simular conversar anteriores\n",
    "* Fornecer exemplos de respostas\n",
    "* Criar contextos de few-shot learning\n",
    "* Manter histórico de conversas\n",
    "\n",
    "Exemplo prático:\n",
    "```python\n",
    "(\"ai\", \"List comprehension é uma forma concisa de criar listas em Python. Exemplo: [x*2 for x in range(5)]\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e2bd6",
   "metadata": {},
   "source": [
    "`Estrutura Recomendada para Prompts Dinâmicos`\n",
    "___\n",
    "```python\n",
    "# 1. Definir o template com tipos apropriados\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Defina o papel/contexto do modelo\"),\n",
    "    (\"human\", \"Forneça o contexto: {context}\"),\n",
    "    (\"human\", \"Faça a pergunta: {question}\"),\n",
    "])\n",
    "\n",
    "# 2. Criar o modelo\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# 3. Invocar com dados dinâmicos\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"seu contexto aqui\",\n",
    "    \"question\": \"sua pergunta aqui\"\n",
    "})\n",
    "\n",
    "# 4. Obter a resposta\n",
    "response = model.invoke(prompt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace97d30",
   "metadata": {},
   "source": [
    "`**Boas Práticas para Desenvolvedores de Agentes**`\n",
    "___\n",
    "\n",
    "1.Sempre use templates para prompts dinâmicos\n",
    "\n",
    "    -Evite strings hardcoded\n",
    "    -Facilita testes e modificações\n",
    "\n",
    "2.Separe claramente os tipos de mensagens\n",
    "\n",
    "    -System: configuração/contexto\n",
    "    -Human: entrada do usuário\n",
    "    -AI: respostas/exemplos\n",
    "\n",
    "3.Use variáveis para dados dinâmicos\n",
    "\n",
    "    -{context}, {question}, {user_input}\n",
    "    -Facilita a reutilização\n",
    "\n",
    "4.Mantenha prompts modulares\n",
    "\n",
    "    -Cada template com responsabilidade específica\n",
    "    -Combine templates para casos complexos\n",
    "\n",
    "**Versão básica**\n",
    "```python\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Responda baseado no contexto.\"),\n",
    "    (\"human\", \"Contexto: {context}\"),\n",
    "    (\"human\", \"Pergunta: {question}\"),\n",
    "])\n",
    "```\n",
    "**Versão avançada(para agentes)**\n",
    "```python\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um assistente especializado em análise de dados.\"),\n",
    "    (\"human\", \"Contexto: {context}\"),\n",
    "    (\"human\", \"Pergunta: {question}\"),\n",
    "    (\"ai\", \"Vou analisar o contexto e responder sua pergunta.\"),\n",
    "    (\"human\", \"Por favor, seja específico e forneça exemplos.\"),\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cfdf0c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O objetivo da jornada de Frodo é destruir o Um Anel, levando-o até a Montanha da Perdição, onde ele foi forjado, para que o anel seja destruído e, assim, impedir que o Lorde das Trevas Sauron recupere seu poder e domine a Terra-média.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# Adicionamos o Output Parser para extrair a resposta final como texto\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1\")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um assistente de IA que é especialista em filmes e séries.\"),\n",
    "    (\"human\", \"Contexto: {contexto}\"),\n",
    "    (\"human\", \"Pergunta: {pergunta}\"),\n",
    "    (\"ai\", \"Analise o contexto e responda a pergunta\")\n",
    "])\n",
    "# Conectamos os componentes na ordem que devem ser executados.\n",
    "chain = template | model | StrOutputParser()\n",
    "\n",
    "# 4. Invocando a Chain (com o input correto)\n",
    "# Agora, fornecemos um dicionário com as chaves que o template espera.\n",
    "# Note que o contexto e a pergunta agora fazem sentido com o prompt do sistema.\n",
    "contexto_exemplo = \"Em 'O Senhor dos Anéis: A Sociedade do Anel', Frodo Bolseiro, um hobbit, herda um anel poderoso de seu tio Bilbo. Ele descobre que o anel é o Um Anel, uma arma do Lorde das Trevas Sauron, e deve embarcar em uma jornada para destruí-lo na Montanha da Perdição.\"\n",
    "pergunta_exemplo = \"Qual é o objetivo da jornada de Frodo?\"\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"contexto\": contexto_exemplo, \n",
    "    \"pergunta\": pergunta_exemplo\n",
    "})\n",
    "\n",
    "# 5. Imprimindo a resposta\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18157cbe",
   "metadata": {},
   "source": [
    "Para lembrar:\n",
    "\n",
    " - `SystemMessage` = \"Quem você é\"\n",
    " - `HumanMessage` = \"O que o usuário quer\"\n",
    " - `AIMessage` = \"Como você responde\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8f0c28b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Um modelo de machine learning é um algoritmo que aprende padrões a partir de dados para fazer previsões ou tomar decisões automatizadas em situações futuras.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"Você é um assistente prestativo, especialista em Matemática.\"\n",
    "    ),\n",
    "    HumanMessage(content=\"Explique o que é um modelo de machine learning, em até 30 palavras\"),\n",
    "]\n",
    "\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3c4aa4",
   "metadata": {},
   "source": [
    "`PromptTemplate` simples é só para texto, `ChatPromptTemplate` é para conversas com roles(especialista em Python, ou matemática, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a9a6d371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain é um framework de desenvolvimento de aplicativos descentralizados (dApps) que utiliza a linguagem de programação Rust. Ele facilita a criação de dApps seguras e escaláveis, integrando-se com blockchains como Ethereum.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"Você é um especialista em {tópico}\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"Explique {assunto} em 35 palavras\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Template\n",
    "formatted_prompt = prompt.format_messages(tópico=\"framework\", assunto=\"langchain\")\n",
    "\n",
    "response = chat.invoke(formatted_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5344a4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em Python, um decorator é uma função que recebe outra função como argumento e retorna uma nova função modificada. Decorators são usados para adicionar funcionalidades a funções existentes sem modificar seu código diretamente, permitindo reutilização de código e separação de preocupações.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Template com papel específico\n",
    "template = ChatPromptTemplate.from_messages([ # type: ignore\n",
    "    (\"system\", \"Você é um tutor de programação Python especializado em explicar conceitos complexos de forma simples.\"),\n",
    "    (\"human\", \"Conceito: {conceito}\"),\n",
    "    (\"human\", \"Explique este conceito em {palavras} palavras ou menos\"),\n",
    "])\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Uso do template\n",
    "response = model.invoke(template.invoke({# type: ignore\n",
    "    \"conceito\": \"decorators em Python\",\n",
    "    \"palavras\": \"50\"\n",
    "}))\n",
    "\n",
    "print(response.content)# type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af41a7e3",
   "metadata": {},
   "source": [
    "## Getting Specific Formats out of LLMs\n",
    "___\n",
    "\n",
    "The most common format to generate with LLMs is JSON, which can be sent over the to your frontend code or be saved to a databased.\n",
    "\n",
    "1. First task, define the output schema (from LLM). Then include that schema in the prompt , along with the text you want to use as the source.\n",
    "2. In order to define a `schema`, this is easiest to do with `Pydantic`(*library* used for validating data against schemas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "115f857d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='They weigh the same.' justification='A pound is a unit of weight, so a pound of bricks and a pound of feathers both weigh exactly one pound. The material does not affect the weight if the quantity is specified as a pound.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    \"\"\"An answer to the user's question along with justification for the answer.\"\"\"\n",
    "\n",
    "    answer: str\n",
    "    \"\"\"The answer to the user's question\"\"\"\n",
    "    justification: str\n",
    "    \"\"\"Justification for the answer\"\"\"\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "structured_model = model.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "response = structured_model.invoke(\n",
    "    \"What weighs more, a pound of bricks or a pound of feathers\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec1ce2d",
   "metadata": {},
   "source": [
    "#### TUTORIAL: LangChain + Pandantic - Structured Output\n",
    "========================================================\n",
    "\n",
    "Este exemplo demonstra como usar Pydantic com LangChain para:\n",
    "1. Validar dados de entrada e saída\n",
    "2. Garantir formato estruturado das respostas\n",
    "3. Tornar o código mais robusto e type-safe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c5aa3276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1066a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerWithJustification(BaseModel):\n",
    "    \"\"\"\n",
    "    Modelo que representa uma resposta estruturada com justificativa.\n",
    "\n",
    "      Por que usar Pydantic aqui?\n",
    "    - Validação automática de tipos\n",
    "    - Documentação clara dos campos\n",
    "    - Serialização JSON automática\n",
    "    - Integração nativa com LangChain\n",
    "    \"\"\"\n",
    "\n",
    "    answer: str = Field(\n",
    "        description=\"Resposta clara e concisa para a pergunta\",\n",
    "        min_length=1,\n",
    "        max_length=500,\n",
    "    )\n",
    "\n",
    "    justification: str = Field(\n",
    "        description=\"Explicação detalhada do raciocínio\", min_length=10, max_length=100\n",
    "    )\n",
    "\n",
    "    confidence_level: float | None = Field(\n",
    "        default=None, description=\"Nível de confiança (0-1) na resposta\", ge=0.0, le=1.0\n",
    "    )\n",
    "\n",
    "    timestamp: str | None = Field(\n",
    "        default_factory=lambda: datetime.now().isoformat(),\n",
    "        description=\"Timestamp da resposta\",\n",
    "    )\n",
    "\n",
    "    def display_response(self) -> str:\n",
    "        \"\"\"\n",
    "        Método para exibir a resposta de forma mais legível.\n",
    "\n",
    "        🔧 BENEFÍCIO: Evita o scroll infinito no output!\n",
    "        \"\"\"\n",
    "        output = f\"\"\"\n",
    "┌─ 💬 RESPOSTA ─────────────────────────────────────────┐\n",
    "│ {self.answer}\n",
    "├─ 🤔 JUSTIFICATIVA ───────────────────────────────────┤\n",
    "│ {self.justification}\n",
    "\"\"\"\n",
    "\n",
    "        if self.confidence_level:\n",
    "            confidence_bar = \"█\" * int(self.confidence_level * 10)\n",
    "            output += f\"\"\"├─ 📊 CONFIANÇA ───────────────────────────────────────┤\n",
    "│ {confidence_bar} {self.confidence_level:.1%}\n",
    "\"\"\"\n",
    "\n",
    "        output += f\"\"\"└─ 🕒 {self.timestamp} ──────────────────────────────────┘\"\"\"\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6881489d",
   "metadata": {},
   "source": [
    "#### 🚀 CLASSE PRINCIPAL PARA DEMONSTRAÇÃO\n",
    "\n",
    "`class LangChainPydanticDemo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1c344c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangChainPydanticDemo:\n",
    "    \"\"\"\n",
    "    Conceitos Importantes:\n",
    "    1. Structured Output: LangChain força o LLM a retornar dados no formato exato do\n",
    "    modelo Pydantic.\n",
    "\n",
    "    2.Type safety: Pydantic valida automaticamente os tipos\n",
    "\n",
    "    3.Error Handling: Falhas na validação são capturadas\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"gpt-4.1\"):\n",
    "        self.model = ChatOpenAI(\n",
    "            model=model_name,\n",
    "            temperature=0.1,  # * baixa criatividade para respostas consistentes\n",
    "        )\n",
    "\n",
    "        # * 🔑 PONTO CHAVE: with_structured_output() força o formato\n",
    "        self.structured_model = self.model.with_structured_output(\n",
    "            AnswerWithJustification\n",
    "        )\n",
    "\n",
    "    def ask_question(self, question: str) -> AnswerWithJustification:\n",
    "        \"\"\"\n",
    "        Faz uma pergunta e retorna resposta estruturada.\n",
    "\n",
    "        🎯 BENEFÍCIOS do Structured Output:\n",
    "        - Resposta sempre no formato esperado\n",
    "        - Validação automática dos dados\n",
    "        - Facilita integração com APIs e bancos de dados\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.structured_model.invoke(question)\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            # * Em caso de erro, retorna resposta padrão\n",
    "            return AnswerWithJustification(\n",
    "                answer=\"Erro ao processar pergunta\",\n",
    "                justification=f\"Erro técnico: {e!s}\",\n",
    "                confidence_level=0.0,\n",
    "            )\n",
    "\n",
    "    def demo_multiple_questions(self):\n",
    "        \"\"\"\n",
    "        Demonstra o uso com múltiplas perguntas para mostrar consistência.\n",
    "        \"\"\"\n",
    "        questions = [\n",
    "            \"O que pesa mais: 1kg de chumbo ou 1kg de algodão?\",\n",
    "            \"Por que o céu é azul?\",\n",
    "            \"Qual a diferença entre Python e JavaScript?\",\n",
    "        ]\n",
    "\n",
    "        print(\"🎓 DEMONSTRAÇÃO: Múltiplas perguntas com formato consistente\\n\")\n",
    "\n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"❓ PERGUNTA {i}: {question}\")\n",
    "            response = self.ask_question(question)\n",
    "            print(response.display_response())\n",
    "            print(\"\\n\" + \"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e19c7e6",
   "metadata": {},
   "source": [
    "#### 📖 EXEMPLOS PRÁTICOS DE USO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "76ae2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exemplo_basico():\n",
    "    \"\"\"Exemplo básico de uso do structured output.\"\"\"\n",
    "    print(\"🔵 EXEMPLO 1: Uso Básico\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    demo = LangChainPydanticDemo()\n",
    "\n",
    "    question = \"Explique o conceito de recursão em programação\"\n",
    "    response = demo.ask_question(question)\n",
    "\n",
    "    # ✨ SAÍDA LIMPA - sem scroll infinito!\n",
    "    print(response.display_response())\n",
    "\n",
    "    # 💾 BONUS: Fácil conversão para JSON\n",
    "    print(\"\\n🔧 BONUS - Dados em JSON:\")\n",
    "    print(json.dumps(response.model_dump(), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1fdb5c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 EXEMPLO 1: Uso Básico\n",
      "----------------------------------------\n",
      "\n",
      "┌─ 💬 RESPOSTA ─────────────────────────────────────────┐\n",
      "│ Recursão em programação é uma técnica onde uma função chama a si mesma, direta ou indiretamente, para resolver um problema dividindo-o em subproblemas menores do mesmo tipo.\n",
      "├─ 🤔 JUSTIFICATIVA ───────────────────────────────────┤\n",
      "│ A recursão é usada quando um problema pode ser decomposto em versões menores de si mesmo. Por meio d\n",
      "├─ 📊 CONFIANÇA ───────────────────────────────────────┤\n",
      "│ █████████ 99.0%\n",
      "└─ 🕒 2024-06-18T18:00:00Z ──────────────────────────────────┘\n",
      "\n",
      "🔧 BONUS - Dados em JSON:\n",
      "{\n",
      "  \"answer\": \"Recursão em programação é uma técnica onde uma função chama a si mesma, direta ou indiretamente, para resolver um problema dividindo-o em subproblemas menores do mesmo tipo.\",\n",
      "  \"justification\": \"A recursão é usada quando um problema pode ser decomposto em versões menores de si mesmo. Por meio d\",\n",
      "  \"confidence_level\": 0.99,\n",
      "  \"timestamp\": \"2024-06-18T18:00:00Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "exemplo_basico()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5a92e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exemplo_avancado():\n",
    "    \"\"\"Exemplo avançado mostrando validação.\"\"\"\n",
    "    print(\"\\n🟢 EXEMPLO 2: Validação com Pydantic\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Tentativa de criar resposta inválida (para mostrar validação)\n",
    "    try:\n",
    "        invalid_response = AnswerWithJustification(\n",
    "            answer=\"\",  # ❌ Inválido: muito curto\n",
    "            justification=\"Curto\",  # ❌ Inválido: muito curto\n",
    "            confidence_level=1.5,  # ❌ Inválido: > 1.0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ VALIDAÇÃO PYDANTIC FUNCIONOU: {e}\")\n",
    "\n",
    "    # ✅ Resposta válida\n",
    "    valid_response = AnswerWithJustification(\n",
    "        answer=\"Python é uma linguagem de programação\",\n",
    "        justification=\"Python é conhecida por sua sintaxe simples e legível\",\n",
    "        confidence_level=0.9,\n",
    "    )\n",
    "\n",
    "    print(\"\\n✅ RESPOSTA VÁLIDA:\")\n",
    "    print(valid_response.display_response())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "96f501d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 EXEMPLO 2: Validação com Pydantic\n",
      "----------------------------------------\n",
      "❌ VALIDAÇÃO PYDANTIC FUNCIONOU: 3 validation errors for AnswerWithJustification\n",
      "answer\n",
      "  String should have at least 1 character [type=string_too_short, input_value='', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_too_short\n",
      "justification\n",
      "  String should have at least 10 characters [type=string_too_short, input_value='Curto', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_too_short\n",
      "confidence_level\n",
      "  Input should be less than or equal to 1 [type=less_than_equal, input_value=1.5, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/less_than_equal\n",
      "\n",
      "✅ RESPOSTA VÁLIDA:\n",
      "\n",
      "┌─ 💬 RESPOSTA ─────────────────────────────────────────┐\n",
      "│ Python é uma linguagem de programação\n",
      "├─ 🤔 JUSTIFICATIVA ───────────────────────────────────┤\n",
      "│ Python é conhecida por sua sintaxe simples e legível\n",
      "├─ 📊 CONFIANÇA ───────────────────────────────────────┤\n",
      "│ █████████ 90.0%\n",
      "└─ 🕒 2025-08-02T11:53:18.773594 ──────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "exemplo_avancado()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2224df6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎓 DEMONSTRAÇÃO COMPLETA:\n",
      "============================================================\n",
      "🎓 DEMONSTRAÇÃO: Múltiplas perguntas com formato consistente\n",
      "\n",
      "❓ PERGUNTA 1: O que pesa mais: 1kg de chumbo ou 1kg de algodão?\n",
      "\n",
      "┌─ 💬 RESPOSTA ─────────────────────────────────────────┐\n",
      "│ 1 kg de chumbo pesa o mesmo que 1 kg de algodão.\n",
      "├─ 🤔 JUSTIFICATIVA ───────────────────────────────────┤\n",
      "│ A unidade de medida 'quilograma' (kg) representa uma quantidade fixa de massa. Portanto, 1 kg de um \n",
      "├─ 📊 CONFIANÇA ───────────────────────────────────────┤\n",
      "│ ██████████ 100.0%\n",
      "└─ 🕒 2024-06-18T20:00:00Z ──────────────────────────────────┘\n",
      "\n",
      "============================================================\n",
      "\n",
      "❓ PERGUNTA 2: Por que o céu é azul?\n",
      "\n",
      "┌─ 💬 RESPOSTA ─────────────────────────────────────────┐\n",
      "│ O céu é azul porque a luz do Sol, ao passar pela atmosfera da Terra, sofre espalhamento pelas moléculas de ar, e a luz azul é espalhada em todas as direções com mais intensidade do que outras cores.\n",
      "├─ 🤔 JUSTIFICATIVA ───────────────────────────────────┤\n",
      "│ A luz branca do Sol é composta por várias cores, cada uma com um comprimento de onda diferente. Ao a\n",
      "├─ 📊 CONFIANÇA ───────────────────────────────────────┤\n",
      "│ ██████████ 100.0%\n",
      "└─ 🕒 2024-06-18T19:00:00Z ──────────────────────────────────┘\n",
      "\n",
      "============================================================\n",
      "\n",
      "❓ PERGUNTA 3: Qual a diferença entre Python e JavaScript?\n",
      "\n",
      "┌─ 💬 RESPOSTA ─────────────────────────────────────────┐\n",
      "│ Python e JavaScript são linguagens de programação diferentes, cada uma com sintaxe, paradigmas e usos distintos. Python é geralmente usado para ciência de dados, automação, backend e scripts, enquanto JavaScript é mais utilizado para desenvolvimento web, especialmente no frontend.\n",
      "├─ 🤔 JUSTIFICATIVA ───────────────────────────────────┤\n",
      "│ Python é uma linguagem de propósito geral, conhecida por sua sintaxe simples e legibilidade, muito a\n",
      "├─ 📊 CONFIANÇA ───────────────────────────────────────┤\n",
      "│ █████████ 95.0%\n",
      "└─ 🕒 2023-07-01T12:00:00Z ──────────────────────────────────┘\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstração completa\n",
    "print(\"\\n🎓 DEMONSTRAÇÃO COMPLETA:\")\n",
    "print(\"=\" * 60)\n",
    "demo = LangChainPydanticDemo()\n",
    "demo.demo_multiple_questions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61a6bc3",
   "metadata": {},
   "source": [
    "🎓 LIÇÕES APRENDIDAS:\n",
    "\n",
    "1. **Pydantic + LangChain = Dados Estruturados**\n",
    "   - LLM retorna sempre no formato correto\n",
    "   - Validação automática de tipos e valores\n",
    "\n",
    "2. **Benefícios para Produção**\n",
    "   - Code completion no IDE\n",
    "   - Documentação automática\n",
    "   - Integração fácil com APIs\n",
    "\n",
    "3. **Melhor UX**\n",
    "   - Saída formatada e legível\n",
    "   - Sem scroll infinito\n",
    "   - Informações organizadas\n",
    "\n",
    "4. **Type Safety**\n",
    "   - Erros capturados em desenvolvimento\n",
    "   - Código mais robusto\n",
    "   - Manutenção facilitada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f84ea1",
   "metadata": {},
   "source": [
    "**Por que Pydantic é Fundamental no LangChain**:\n",
    "\n",
    "1. Structured Output: Força o LLM a retornar dados no formato exato\n",
    "2. Validação Automática: Tipos e valores são verificados automaticamente\n",
    "3. Integração Nativa: LangChain foi projetado para trabalhar com Pydantic\n",
    "4. Produção Ready: Facilita APIs, banco de dados e frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365e92c",
   "metadata": {},
   "source": [
    "#### Other Machine-Readable Formats with Output Parsers\n",
    "___\n",
    "\n",
    "We can also use an LLM or chat model to produce output in other formats, such as CSV or XML. Using output parsers  that are classes that help us structure large language model responses.\n",
    "\n",
    "    - Providing format instructions\n",
    "    - Validating and parsing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5c4a436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'cherry']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "response = parser.invoke(\"apple, banana, cherry\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b243dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7020209",
   "metadata": {},
   "source": [
    "1. Classe Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8d21cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    email: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e8fa8",
   "metadata": {},
   "source": [
    "2. Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e231eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=UserInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55358e78",
   "metadata": {},
   "source": [
    "3. Prompt com instrução de saída estruturada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "27f76dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template = \"\"\"Dado o seguinte texto: '{text}', gere um JSON com as seguintes informações: {format_instructions}\"\"\",\n",
    "    input_variables = [\"text\"],\n",
    "    partial_variables = {\"format_instructions\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1e3ab",
   "metadata": {},
   "source": [
    "4. LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bd150bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "07ee0806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Ana Paula' age=32 email='ana.paula@gmail.com'\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"O nome dela é Ana Paula, tem 32 anos e o email dela é ana.paula@gmail.com, e ela mora em São Paulo\"\"\"\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "response = chain.invoke({\"text\": input_text})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9044c911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nome, preço, categoria\n",
      "Arroz, 20 reais, Alimentos\n",
      "Sabonete, 5 reais, Higiene\n",
      "Caderno, 15 reais, Papelaria\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "csv_prompt = PromptTemplate.from_template(\n",
    "    \"Converta a seguinte lista de produtos em formato CSV com colunas: nome, preço, categoria.\\n\\n{text}\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "input_text = \"\"\"\n",
    "1. Arroz, 20 reais, Alimentos\n",
    "2. Sabonete, 5 reais, Higiene\n",
    "3. Caderno, 15 reais, Papelaria\n",
    "\"\"\"\n",
    "\n",
    "csv_result = llm.invoke(csv_prompt.format(text=input_text))\n",
    "print(csv_result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1de9f708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='1984' author='George Orwell' year=1949\n",
      "{'title': '1984', 'author': 'George Orwell', 'year': 1949}\n"
     ]
    }
   ],
   "source": [
    "# Modelo simples para validar como Python dict\n",
    "class BookInfo(BaseModel):\n",
    "    title: str\n",
    "    author: str\n",
    "    year: int\n",
    "\n",
    "book_parser = PydanticOutputParser(pydantic_object=BookInfo)\n",
    "\n",
    "book_prompt = PromptTemplate(\n",
    "    template=\"Extraia as informações do livro e retorne no formato:\\n{format_instructions}\\n\\nTexto: {text}\",\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": book_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = book_prompt | llm | book_parser\n",
    "\n",
    "text = \"O livro '1984' foi escrito por George Orwell e publicado em 1949.\"\n",
    "book_info = chain.invoke({\"text\": text})\n",
    "print(book_info)\n",
    "print(book_info.model_dump())  # Convertendo para dict usando model_dump()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1df1c1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título: Dom Casmurro\n",
      "Autor: Machado de Assis\n",
      "Ano: 1899\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Extraia o título, autor e ano do seguinte texto:\\n\\nTexto: {text}\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "text = \"O livro 'Dom Casmurro' foi escrito por Machado de Assis em 1899.\"\n",
    "response = chain.invoke({\"text\": text})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "195a71e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Dom Casmurro', 'author': 'Machado de Assis', 'year': 1899}\n"
     ]
    }
   ],
   "source": [
    "class Book(BaseModel):\n",
    "    title: str\n",
    "    author: str\n",
    "    year: int\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Book)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Extraia os dados do livro e retorne em JSON no seguinte formato:\\n{format_instructions}\\n\\nTexto: {text}\",\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "book = chain.invoke({\"text\": text})\n",
    "print(book.model_dump())  # Convertendo para dict usando model_dump()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec747b1a",
   "metadata": {},
   "source": [
    "| Parser                 | Quando usar                                       |\n",
    "| ---------------------- | ------------------------------------------------- |\n",
    "| `StrOutputParser`      | Para capturar texto puro, rascunhos, debugging    |\n",
    "| `PydanticOutputParser` | Quando você quer estrutura, validação e segurança |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e69ad05",
   "metadata": {},
   "source": [
    "* chain of thought\n",
    "* react\n",
    "* tree of thought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce0abb",
   "metadata": {},
   "source": [
    "## Agents Design\n",
    "___\n",
    "\n",
    "1. Defina os objetivos de forma clara; - `foque na tarefa e não no agente`\n",
    "2. Defina como o agente irá coletar ou receber as informações;\n",
    "3. Escolhe o agente a outros sistemas;\n",
    "4. Integre o agente a outros sistemas;\n",
    "5. Monitore e otimize;\n",
    "6. Garanta a segurança e privacidade dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636427fa",
   "metadata": {},
   "source": [
    "### Design de um agente - LANGCHAIN\n",
    "___\n",
    "\n",
    "* Modelo para tomada de decisão;\n",
    "* Escolha as ferramentas;\n",
    "* Escolha um sistema de memória embutido;\n",
    "* Guardrails para limitar comportamento indesejado;\n",
    "* Sistema de logging e observabilidade;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd65dd1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83d6ae18",
   "metadata": {},
   "source": [
    "### Streaming in LangChain\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "783242a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Redes neurais artificiais são modelos computacionais inspirados no funcionamento do cérebro humano, que são utilizados para realizar tarefas de aprendizado de máquina. Elas são compostas por neurônios artificiais interconectados em camadas, onde cada neurônio recebe entradas, realiza um cálculo e produz uma saída.\\n\\nUm exemplo de rede neural artificial é a rede neural convolucional (CNN), amplamente utilizada em tarefas de visão computacional, como reconhecimento de imagens. Neste tipo de rede, as camadas convolucionais são responsáveis por extrair características das imagens, enquanto as camadas de pooling reduzem a dimensionalidade dos dados. Por fim, as camadas totalmente conectadas realizam a classificação final da imagem.\\n\\nEm resumo, as redes neurais artificiais são poderosas ferramentas de aprendizado de máquina que podem ser aplicadas em uma variedade de problemas, desde reconhecimento de padrões até processamento de linguagem natural.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um assistente especialista em IA.\"),\n",
    "    (\"human\", \"{pergunta}\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({\"pergunta\": \"Explique o que são redes neurais artificiais com um exemplo.\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f268ad2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redes neurais artificiais são sistemas computacionais inspirados no funcionamento do cérebro humano. Elas são formadas por unidades chamadas **neurônios artificiais** organizados em camadas. Esses neurônios recebem informações, processam e transmitem para outros neurônios, permitindo que a rede “aprenda” padrões em dados.\n",
      "\n",
      "**Como funciona, de forma simples:**\n",
      "- A camada de entrada recebe os dados (por exemplo, uma imagem).\n",
      "- Os neurônios processam esses dados realizando operações matemáticas (como somas e ativações).\n",
      "- A informação passa por camadas intermediárias (camadas ocultas), onde o conhecimento é refinado.\n",
      "- Por fim, a camada de saída fornece o resultado (por exemplo, identificar se há um gato na imagem).\n",
      "\n",
      "**Exemplo prático: Reconhecimento de dígitos escritos à mão**\n",
      "Imagine que você quer que um computador reconheça números de 0 a 9 escritos à mão.\n",
      "\n",
      "1. Você fornece imagens de dígitos para a rede neural.\n",
      "2. Cada imagem é transformada em uma lista de números (os tons de cinza dos pixels).\n",
      "3. A rede neural analisa os padrões nesses números e aprende a distinguir cada dígito durante o treinamento.\n",
      "4. Após o treinamento, ao receber uma nova imagem de um dígito, a rede consegue identificar (com boa precisão) qual número foi escrito.\n",
      "\n",
      "Portanto, as redes neurais artificiais são modelos que aprendem com exemplos e são amplamente usadas em tarefas como reconhecimento de voz, traduções automáticas, diagnósticos médicos e muito mais.\n",
      "\n",
      "\n",
      "['', 'Red', 'es', ' neur', 'ais', ' artific', 'iais', ' são', ' sistemas', ' comput', 'acionais', ' inspir', 'ados', ' no', ' funcionamento', ' do', ' cérebro', ' humano', '.', ' Elas', ' são', ' form', 'adas', ' por', ' unidades', ' chamadas', ' **', 'neur', 'ô', 'nios', ' artific', 'iais', '**', ' organiz', 'ados', ' em', ' cam', 'adas', '.', ' Esses', ' neur', 'ô', 'nios', ' recebem', ' informações', ',', ' process', 'am', ' e', ' transm', 'item', ' para', ' outros', ' neur', 'ô', 'nios', ',', ' permitindo', ' que', ' a', ' rede', ' “', 'apr', 'enda', '”', ' padrões', ' em', ' dados', '.\\n\\n', '**', 'Como', ' funciona', ',', ' de', ' forma', ' simples', ':', '**\\n', '-', ' A', ' camada', ' de', ' entrada', ' recebe', ' os', ' dados', ' (', 'por', ' exemplo', ',', ' uma', ' imagem', ').\\n', '-', ' Os', ' neur', 'ô', 'nios', ' process', 'am', ' esses', ' dados', ' realizando', ' operações', ' matem', 'áticas', ' (', 'como', ' som', 'as', ' e', ' ativa', 'ções', ').\\n', '-', ' A', ' informação', ' passa', ' por', ' cam', 'adas', ' intermedi', 'árias', ' (', 'cam', 'adas', ' ocult', 'as', '),', ' onde', ' o', ' conhecimento', ' é', ' refin', 'ado', '.\\n', '-', ' Por', ' fim', ',', ' a', ' camada', ' de', ' saída', ' fornece', ' o', ' resultado', ' (', 'por', ' exemplo', ',', ' identificar', ' se', ' há', ' um', ' gato', ' na', ' imagem', ').\\n\\n', '**', 'Ex', 'emplo', ' pr', 'ático', ':', ' Recon', 'hecimento', ' de', ' dí', 'g', 'itos', ' escritos', ' à', ' mão', '**\\n', 'Imagine', ' que', ' você', ' quer', ' que', ' um', ' computador', ' reconhe', 'ça', ' números', ' de', ' ', '0', ' a', ' ', '9', ' escritos', ' à', ' mão', '.\\n\\n', '1', '.', ' Você', ' fornece', ' imagens', ' de', ' dí', 'g', 'itos', ' para', ' a', ' rede', ' neural', '.\\n', '2', '.', ' Cada', ' imagem', ' é', ' transform', 'ada', ' em', ' uma', ' lista', ' de', ' números', ' (', 'os', ' tons', ' de', ' cin', 'za', ' dos', ' pixels', ').\\n', '3', '.', ' A', ' rede', ' neural', ' anal', 'isa', ' os', ' padrões', ' nesses', ' números', ' e', ' aprende', ' a', ' distinguir', ' cada', ' dí', 'g', 'ito', ' durante', ' o', ' treinamento', '.\\n', '4', '.', ' Após', ' o', ' treinamento', ',', ' ao', ' receber', ' uma', ' nova', ' imagem', ' de', ' um', ' dí', 'g', 'ito', ',', ' a', ' rede', ' consegue', ' identificar', ' (', 'com', ' boa', ' precisão', ')', ' qual', ' número', ' foi', ' escrito', '.\\n\\n', 'Port', 'anto', ',', ' as', ' redes', ' neur', 'ais', ' artific', 'iais', ' são', ' modelos', ' que', ' aprend', 'em', ' com', ' exemplos', ' e', ' são', ' ampl', 'amente', ' usadas', ' em', ' tarefas', ' como', ' reconhecimento', ' de', ' voz', ',', ' tradu', 'ções', ' autom', 'áticas', ',', ' diagn', 'óst', 'icos', ' médicos', ' e', ' muito', ' mais', '.', '']\n",
      "Redes neurais artificiais são sistemas computacionais inspirados no funcionamento do cérebro humano. Elas são formadas por unidades chamadas **neurônios artificiais** organizados em camadas. Esses neurônios recebem informações, processam e transmitem para outros neurônios, permitindo que a rede “aprenda” padrões em dados.\n",
      "\n",
      "**Como funciona, de forma simples:**\n",
      "- A camada de entrada recebe os dados (por exemplo, uma imagem).\n",
      "- Os neurônios processam esses dados realizando operações matemáticas (como somas e ativações).\n",
      "- A informação passa por camadas intermediárias (camadas ocultas), onde o conhecimento é refinado.\n",
      "- Por fim, a camada de saída fornece o resultado (por exemplo, identificar se há um gato na imagem).\n",
      "\n",
      "**Exemplo prático: Reconhecimento de dígitos escritos à mão**\n",
      "Imagine que você quer que um computador reconheça números de 0 a 9 escritos à mão.\n",
      "\n",
      "1. Você fornece imagens de dígitos para a rede neural.\n",
      "2. Cada imagem é transformada em uma lista de números (os tons de cinza dos pixels).\n",
      "3. A rede neural analisa os padrões nesses números e aprende a distinguir cada dígito durante o treinamento.\n",
      "4. Após o treinamento, ao receber uma nova imagem de um dígito, a rede consegue identificar (com boa precisão) qual número foi escrito.\n",
      "\n",
      "Portanto, as redes neurais artificiais são modelos que aprendem com exemplos e são amplamente usadas em tarefas como reconhecimento de voz, traduções automáticas, diagnósticos médicos e muito mais.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\", streaming=True)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um assistente especialista em IA.\"),\n",
    "    (\"human\", \"{pergunta}\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chunks =[]\n",
    "for chunk in chain.stream({\"pergunta\": \"Explique o que são redes neurais artificiais com um exemplo.\"}):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(chunks)\n",
    "final_response = \"\".join(chunks)\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afda1099",
   "metadata": {},
   "source": [
    "Entender **quando usar streaming** é essencial para desenvolver aplicações mais eficientes, responsivas e amigáveis.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 O que o streaming resolve?\n",
    "\n",
    "### ✅ **Problema sem streaming:**\n",
    "\n",
    "* Você envia o prompt para o modelo.\n",
    "* O sistema **espera a resposta inteira** do LLM para depois exibir ou processar.\n",
    "* Resultado: tempo de espera maior, experiência menos interativa.\n",
    "\n",
    "### ✅ **Com streaming:**\n",
    "\n",
    "* A resposta do modelo começa a ser **emitida token por token** (palavra por palavra).\n",
    "* Você pode **mostrar, processar ou agir** conforme cada parte chega.\n",
    "* Resultado: resposta **mais rápida e fluida**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Casos típicos onde **streaming é necessário (ou recomendado)**\n",
    "\n",
    "### 1. **Chatbots e assistentes virtuais**\n",
    "\n",
    "📲 **Exemplo**: Aplicativos como ChatGPT, interfaces com assistentes em tempo real.\n",
    "\n",
    "* Exibir a resposta conforme o modelo \"pensa\".\n",
    "* Torna a conversa mais **natural** e **responsiva**.\n",
    "\n",
    "✅ Recomendado: melhora **UX** (experiência do usuário).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Interfaces web ou mobile com atualização ao vivo**\n",
    "\n",
    "🖥️ **Exemplo**: Interfaces React/Vue exibindo respostas em tempo real.\n",
    "\n",
    "* Enquanto o modelo responde, o conteúdo já aparece no componente.\n",
    "* Pode ser integrado com animações ou placeholders.\n",
    "\n",
    "✅ Torna o app mais fluido, parece que o modelo está \"digitando\".\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Experiências de voz (text-to-speech)**\n",
    "\n",
    "🎙️ **Exemplo**: um sistema que \"fala\" a resposta gerada.\n",
    "\n",
    "* Assim que os primeiros tokens chegam, eles já podem ser passados para um sistema de voz.\n",
    "* Isso reduz o tempo entre a pergunta e a resposta falada.\n",
    "\n",
    "✅ Necessário para **naturalidade** em interações por voz.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Carregamento progressivo em terminais e scripts CLI**\n",
    "\n",
    "🧑‍💻 **Exemplo**: Chatbots de terminal, interfaces de linha de comando com LLM.\n",
    "\n",
    "* Usuário vê o conteúdo aparecendo em tempo real (efeito \"digitando...\").\n",
    "* Boa experiência em scripts interativos.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Tarefas longas ou respostas extensas**\n",
    "\n",
    "📚 **Exemplo**: Geração de textos longos, códigos, artigos, análises complexas.\n",
    "\n",
    "* O modelo pode demorar vários segundos para finalizar.\n",
    "* Com streaming, o usuário não fica esperando no escuro.\n",
    "\n",
    "✅ Ideal para evitar **tempo de espera** percebido.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Quando **não** é necessário usar streaming?\n",
    "\n",
    "| Cenário                                                          | Use streaming?  |\n",
    "| ---------------------------------------------------------------- | --------------- |\n",
    "| Respostas muito curtas                                           | ❌ Desnecessário |\n",
    "| Processamento em batch (várias chamadas de uma vez)              | ❌               |\n",
    "| Uso em pipelines de backend com foco em velocidade total, não UX | ❌               |\n",
    "| Quando vai salvar a resposta direto no banco ou arquivo          | ❌               |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Dica de ouro\n",
    "\n",
    "Mesmo quando você **não precisa exibir streaming**, pode usá-lo para:\n",
    "\n",
    "* Processar tokens à medida que chegam.\n",
    "* Implementar validações ou cancelamentos precoces.\n",
    "* Controlar o comportamento do modelo de forma mais granular.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "da6e61bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content='Good' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content='bye' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' If' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' you' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' have' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' any' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' more' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' questions' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' in' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' the' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' future' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=',' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' feel' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' free' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' to' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' ask' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' Take' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' care' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "completion = model.invoke(\"Hi there!\")\n",
    "# Hi!\n",
    "\n",
    "completions = model.batch([\"Hi there!\", \"Bye!\"])\n",
    "# ['Hi!', 'See you!']\n",
    "\n",
    "for token in model.stream(\"Bye!\"):\n",
    "    print(token)\n",
    "    # Good\n",
    "    # bye\n",
    "    # !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0c5f4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class PydanticUserInfo(BaseModel):\n",
    "    \"\"\"User's info.\"\"\"\n",
    "    name: Annotated[str, Field(description=\"User's name. Defaults to ''\", default=None)]\n",
    "    country: Annotated[str, Field(description=\"Where the user lives. Defaults to ''\", default=None, )]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b191e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_structure = llm.with_structured_output(PydanticUserInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "21c035c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_output = llm_with_structure.invoke(\"The sky is blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c0632d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PydanticUserInfo(name='', country='')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "87967a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(structured_output.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b03da435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado:\n",
      "Nome: Joana\n",
      "Idade: 30\n",
      "Interesses: ['leitura', 'yoga', 'viagens']\n",
      "\n",
      "Tipo do objeto: <class '__main__.Pessoa'>\n",
      "Objeto completo: nome='Joana' idade=30 interesses=['leitura', 'yoga', 'viagens']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define a estrutura esperada\n",
    "class Pessoa(BaseModel):\n",
    "    nome: str\n",
    "    idade: int\n",
    "    interesses: list[str]\n",
    "\n",
    "# Cria o parser usando a classe Pessoa\n",
    "parser = PydanticOutputParser(pydantic_object=Pessoa)\n",
    "\n",
    "# Cria o template do prompt incluindo as instruções de formato\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Extraia nome, idade e interesses do seguinte texto:\\nTexto: {texto}\\n{format_instructions}\",\n",
    "    input_variables=[\"texto\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Inicializa o modelo de linguagem\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "# Cria a cadeia (chain) de processamento\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Executa a extração\n",
    "resposta = chain.invoke({\"texto\": \"Joana tem 30 anos e gosta de leitura, yoga e viagens.\"})\n",
    "\n",
    "print(\"Resultado:\")\n",
    "print(f\"Nome: {resposta.nome}\")\n",
    "print(f\"Idade: {resposta.idade}\")\n",
    "print(f\"Interesses: {resposta.interesses}\")\n",
    "print(f\"\\nTipo do objeto: {type(resposta)}\")\n",
    "print(f\"Objeto completo: {resposta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d0be1d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESULTADOS DA EXTRAÇÃO ===\n",
      "\n",
      "TESTE 1:\n",
      "Texto original: Joana tem 30 anos e gosta de leitura, yoga e viagens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520984/179862137.py:33: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  @validator('nome')\n",
      "/tmp/ipykernel_520984/179862137.py:41: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  @validator('interesses')\n",
      "/home/fabiolima/Workdir/langchain/study_langchain/.venv/lib/python3.13/site-packages/pydantic/_internal/_config.py:373: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extração bem-sucedida:\n",
      "   Nome: Joana\n",
      "   Idade: 30\n",
      "   Interesses: ['leitura', 'yoga', 'viagens']\n",
      "   Objeto validado: nome='Joana' idade=30 interesses=['leitura', 'yoga', 'viagens']\n",
      "--------------------------------------------------\n",
      "TESTE 2:\n",
      "Texto original: Pedro é um jovem de 25 anos apaixonado por futebol, culinária e música eletrônica.\n",
      "✅ Extração bem-sucedida:\n",
      "   Nome: Pedro\n",
      "   Idade: 25\n",
      "   Interesses: ['futebol', 'culinária', 'música eletrônica']\n",
      "   Objeto validado: nome='Pedro' idade=25 interesses=['futebol', 'culinária', 'música eletrônica']\n",
      "--------------------------------------------------\n",
      "TESTE 3:\n",
      "Texto original: Maria, 45 anos, adora jardinagem e pintura em aquarela.\n",
      "✅ Extração bem-sucedida:\n",
      "   Nome: Maria\n",
      "   Idade: 45\n",
      "   Interesses: ['jardinagem', 'pintura em aquarela']\n",
      "   Objeto validado: nome='Maria' idade=45 interesses=['jardinagem', 'pintura em aquarela']\n",
      "--------------------------------------------------\n",
      "\n",
      "=== SCHEMA JSON GERADO ===\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"Modelo para representar informações de uma pessoa extraídas de texto.\", \"properties\": {\"nome\": {\"description\": \"Nome completo da pessoa\", \"example\": \"Maria Silva\", \"maxLength\": 100, \"minLength\": 2, \"title\": \"Nome\", \"type\": \"string\"}, \"idade\": {\"description\": \"Idade da pessoa em anos\", \"example\": 30, \"maximum\": 150, \"minimum\": 0, \"title\": \"Idade\", \"type\": \"integer\"}, \"interesses\": {\"description\": \"Lista de hobbies, atividades ou interesses da pessoa\", \"example\": [\"leitura\", \"yoga\", \"viagens\"], \"items\": {\"type\": \"string\"}, \"maxItems\": 10, \"minItems\": 1, \"title\": \"Interesses\", \"type\": \"array\"}}, \"required\": [\"nome\", \"idade\", \"interesses\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define a estrutura esperada com validações e descrições completas\n",
    "class Pessoa(BaseModel):\n",
    "    \"\"\"Modelo para representar informações de uma pessoa extraídas de texto.\"\"\"\n",
    "    \n",
    "    nome: str = Field(\n",
    "        description=\"Nome completo da pessoa\",\n",
    "        min_length=2,\n",
    "        max_length=100,\n",
    "        example=\"Maria Silva\"\n",
    "    )\n",
    "    \n",
    "    idade: int = Field(\n",
    "        description=\"Idade da pessoa em anos\",\n",
    "        ge=0,  # greater or equal (>= 0)\n",
    "        le=150,  # less or equal (<= 150)\n",
    "        example=30\n",
    "    )\n",
    "    \n",
    "    interesses: List[str] = Field(\n",
    "        description=\"Lista de hobbies, atividades ou interesses da pessoa\",\n",
    "        min_items=1,\n",
    "        max_items=10,\n",
    "        example=[\"leitura\", \"yoga\", \"viagens\"]\n",
    "    )\n",
    "    \n",
    "    # Validador customizado para normalizar o nome\n",
    "    @validator('nome')\n",
    "    def validar_nome(cls, v):\n",
    "        if not v.strip():\n",
    "            raise ValueError('Nome não pode estar vazio')\n",
    "        # Capitaliza cada palavra do nome\n",
    "        return ' '.join(word.capitalize() for word in v.strip().split())\n",
    "    \n",
    "    # Validador para normalizar interesses\n",
    "    @validator('interesses')\n",
    "    def validar_interesses(cls, v):\n",
    "        if not v:\n",
    "            raise ValueError('Deve haver pelo menos um interesse')\n",
    "        # Remove duplicatas e normaliza para minúsculas\n",
    "        interesses_limpos = []\n",
    "        for interesse in v:\n",
    "            interesse_limpo = interesse.strip().lower()\n",
    "            if interesse_limpo and interesse_limpo not in interesses_limpos:\n",
    "                interesses_limpos.append(interesse_limpo)\n",
    "        return interesses_limpos\n",
    "    \n",
    "    class Config:\n",
    "        # Configurações do modelo\n",
    "        str_strip_whitespace = True  # Remove espaços em branco automaticamente\n",
    "        validate_assignment = True   # Valida também durante atribuições\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"nome\": \"Ana Costa\",\n",
    "                \"idade\": 28,\n",
    "                \"interesses\": [\"culinária\", \"fotografia\", \"natação\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Cria o parser usando a classe Pessoa\n",
    "parser = PydanticOutputParser(pydantic_object=Pessoa)\n",
    "\n",
    "# Template mais robusto\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Você é um especialista em extração de informações pessoais de textos.\n",
    "    \n",
    "    Extraia nome, idade e interesses do seguinte texto de forma precisa:\n",
    "    \n",
    "    TEXTO: {texto}\n",
    "    \n",
    "    INSTRUÇÕES IMPORTANTES:\n",
    "    - Se a idade não estiver explícita, tente inferir com base no contexto\n",
    "    - Para interesses, inclua hobbies, atividades, gostos pessoais mencionados\n",
    "    - Seja preciso e não invente informações que não estão no texto\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=[\"texto\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Inicializa o modelo de linguagem\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Cria a cadeia (chain) de processamento\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Teste com diferentes exemplos\n",
    "textos_teste = [\n",
    "    \"Joana tem 30 anos e gosta de leitura, yoga e viagens.\",\n",
    "    \"Pedro é um jovem de 25 anos apaixonado por futebol, culinária e música eletrônica.\",\n",
    "    \"Maria, 45 anos, adora jardinagem e pintura em aquarela.\"\n",
    "]\n",
    "\n",
    "print(\"=== RESULTADOS DA EXTRAÇÃO ===\\n\")\n",
    "\n",
    "for i, texto in enumerate(textos_teste, 1):\n",
    "    print(f\"TESTE {i}:\")\n",
    "    print(f\"Texto original: {texto}\")\n",
    "    \n",
    "    try:\n",
    "        resposta = chain.invoke({\"texto\": texto})\n",
    "        print(f\"✅ Extração bem-sucedida:\")\n",
    "        print(f\"   Nome: {resposta.nome}\")\n",
    "        print(f\"   Idade: {resposta.idade}\")\n",
    "        print(f\"   Interesses: {resposta.interesses}\")\n",
    "        print(f\"   Objeto validado: {resposta}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro na extração: {e}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Exemplo de como acessar o schema JSON gerado\n",
    "print(\"\\n=== SCHEMA JSON GERADO ===\")\n",
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf588a7f",
   "metadata": {},
   "source": [
    "## LCEL - LangChain Expression Language \n",
    "___\n",
    "\n",
    "`A declarative way to compose AI workflows`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d6f621",
   "metadata": {},
   "source": [
    "## LangGraph\n",
    "\n",
    "A framework for `agentic workgflows` with complex state management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bbb011",
   "metadata": {},
   "source": [
    "Vamos explorar a ideia da **interface `Runnable` no LangChain**, que é uma das peças centrais da **LangChain Expression Language (LCEL)**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 O que é `Runnable`?\n",
    "\n",
    "Em termos simples:\n",
    "\n",
    "> **`Runnable` é uma interface padrão que define como um componente da LangChain pode ser executado.**\n",
    "\n",
    "Tudo o que pode ser **invocado, encadeado ou executado** — como um modelo, um prompt, um parser — implementa a interface `Runnable`.\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Por que isso é importante?\n",
    "\n",
    "A interface `Runnable` é o que permite você fazer coisas como:\n",
    "\n",
    "```python\n",
    "prompt | llm | parser\n",
    "```\n",
    "\n",
    "Cada componente aqui (`prompt`, `llm`, `parser`) é um **Runnable** — eles sabem **como receber entrada e produzir saída** de forma padronizada.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Métodos principais de `Runnable`\n",
    "\n",
    "| Método           | O que faz                                                   |\n",
    "| ---------------- | ----------------------------------------------------------- |\n",
    "| `invoke(input)`  | Executa uma única chamada sincrônica                        |\n",
    "| `stream(input)`  | Executa com **streaming**, útil para chat/respostas ao vivo |\n",
    "| `batch(inputs)`  | Executa várias entradas de uma vez                          |\n",
    "| `ainvoke(input)` | Versão assíncrona de `invoke()`                             |\n",
    "| `astream(input)` | Versão assíncrona de `stream()`                             |\n",
    "| `abatch(inputs)` | Versão assíncrona de `batch()`                              |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Exemplo didático\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Cria um componente que dobra o valor de entrada\n",
    "dobro = RunnableLambda(lambda x: x * 2)\n",
    "\n",
    "print(dobro.invoke(10))  # → 20\n",
    "```\n",
    "\n",
    "Esse `RunnableLambda` é uma função simples transformada num `Runnable`.\n",
    "\n",
    "---\n",
    "\n",
    "## 📐 Por que isso é poderoso?\n",
    "\n",
    "Porque você pode **compor componentes diferentes** como se fossem \"filtros de dados\" — tipo Unix Pipes (`|`).\n",
    "\n",
    "Por exemplo:\n",
    "\n",
    "```python\n",
    "chain = prompt | llm | parser\n",
    "```\n",
    "\n",
    "Internamente, isso é equivalente a:\n",
    "\n",
    "```python\n",
    "output_prompt = prompt.invoke(inputs)\n",
    "output_llm = llm.invoke(output_prompt)\n",
    "output_final = parser.invoke(output_llm)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🤖 Todo LLM, prompt, parser ou chain em LCEL é um `Runnable`\n",
    "\n",
    "### Exemplo:\n",
    "\n",
    "* `ChatPromptTemplate` → `Runnable`: recebe `dict` → gera prompt formatado.\n",
    "* `ChatOpenAI` → `Runnable`: recebe prompt → gera resposta.\n",
    "* `StrOutputParser` → `Runnable`: recebe resposta → extrai string.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Curiosidade: você pode criar suas próprias chains\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "formatador = RunnableLambda(lambda x: f\"Olá, {x}!\")\n",
    "maiusculo = RunnableLambda(lambda x: x.upper())\n",
    "\n",
    "saudacao_chain = formatador | maiusculo\n",
    "\n",
    "print(saudacao_chain.invoke(\"fabio\"))  # → OLÁ, FABIO!\n",
    "```\n",
    "\n",
    "Simples, poderoso e reutilizável.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Resumo final\n",
    "\n",
    "A interface `Runnable` é:\n",
    "\n",
    "✅ O **coração do encadeamento** em LangChain\n",
    "✅ Uma maneira de garantir que **todos os componentes se comportem de forma previsível**\n",
    "✅ Uma forma de criar **pipelines reutilizáveis e composáveis**\n",
    "✅ O que permite usar métodos como `.invoke()`, `.stream()`, `.batch()` em qualquer etapa da sua chain\n",
    "\n",
    "---\n",
    "\n",
    "Se quiser, posso montar um gráfico tipo \"pipeline visual\" explicando como os Runnables se conectam.\n",
    "\n",
    "Ou gerar um notebook com exemplos didáticos de `RunnableLambda`, `RunnableMap`, `Prompt | LLM | Parser`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b1b8490d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python é uma linguagem de programação de alto nível, muito popular por sua simplicidade e legibilidade. Ela foi criada por Guido van Rossum e lançada pela primeira vez em 1991. Aqui estão alguns pontos principais sobre Python:\n",
      "\n",
      "1. **Fácil de Aprender**: A sintaxe do Python é clara e intuitiva, o que a torna uma ótima escolha para iniciantes.\n",
      "\n",
      "2. **Versátil**: Python pode ser usado para uma ampla variedade de aplicações, incluindo desenvolvimento web, análise de dados, inteligência artificial, automação de tarefas, e muito mais.\n",
      "\n",
      "3. **Bibliotecas e Frameworks**: Python possui uma vasta coleção de bibliotecas e frameworks que facilitam o desenvolvimento. Por exemplo, o Django é usado para desenvolvimento web, enquanto o Pandas é popular para análise de dados.\n",
      "\n",
      "4. **Comunidade Ativa**: Python tem uma grande comunidade de desenvolvedores, o que significa que há muitos recursos, tutoriais e suporte disponíveis.\n",
      "\n",
      "5. **Multiplataforma**: Python pode ser executado em diferentes sistemas operacionais, como Windows, macOS e Linux.\n",
      "\n",
      "Em resumo, Python é uma linguagem poderosa e flexível, ideal tanto para iniciantes quanto para programadores experientes.\n",
      "Machine learning, ou aprendizado de máquina, é uma área da inteligência artificial que permite que computadores aprendam a partir de dados e melhorem seu desempenho em tarefas específicas sem serem programados explicitamente para isso. Em vez de seguir instruções fixas, os algoritmos de machine learning analisam padrões nos dados e fazem previsões ou tomam decisões com base nessas análises.\n",
      "\n",
      "Por exemplo, um algoritmo de machine learning pode ser treinado com um conjunto de dados de e-mails para identificar quais são spam. Ele aprende a reconhecer características comuns nos e-mails indesejados e, assim, consegue classificar novos e-mails como spam ou não.\n",
      "\n",
      "Em resumo, machine learning é sobre ensinar máquinas a aprender com a experiência, usando dados para melhorar suas habilidades ao longo do tempo.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Componentes base\n",
    "prompt = ChatPromptTemplate.from_template(\"Me dê uma explicação simples sobre {topic}\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # Use o modelo mais novo disponível\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Encadeamento moderno com LCEL (LangChain Expression Language)\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Executa batch de múltiplas entradas\n",
    "result = chain.batch([\n",
    "    {'topic': 'python'},\n",
    "    {'topic': 'machine learning'}\n",
    "])\n",
    "\n",
    "for item in result:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e880c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplos Práticos: Imperative Composition vs LCEL\n",
    "# Certifique-se de ter instalado: pip install langchain langchain-openai\n",
    "\n",
    "import os\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Configure sua API key da OpenAI\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sua-api-key-aqui\"\n",
    "\n",
    "# ============================================================================\n",
    "# EXEMPLO 1: TRADUTOR SIMPLES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=== EXEMPLO 1: TRADUTOR SIMPLES ===\\n\")\n",
    "\n",
    "# Componentes base\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "translator_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um tradutor especializado. Traduza o texto para {idioma}.\"),\n",
    "    (\"human\", \"{texto}\")\n",
    "])\n",
    "\n",
    "# VERSÃO IMPERATIVE COM @chain\n",
    "@chain\n",
    "def tradutor_imperativo(values):\n",
    "    \"\"\"Tradutor usando composição imperativa\"\"\"\n",
    "    prompt = translator_template.invoke(values)\n",
    "    response = model.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "# VERSÃO LCEL\n",
    "tradutor_lcel = translator_template | model | StrOutputParser()\n",
    "\n",
    "# TESTANDO AMBOS\n",
    "texto_teste = \"Hello, how are you today?\"\n",
    "idioma_teste = \"português\"\n",
    "\n",
    "print(\"🔧 IMPERATIVO:\")\n",
    "resultado_imp = tradutor_imperativo.invoke({\n",
    "    \"texto\": texto_teste, \n",
    "    \"idioma\": idioma_teste\n",
    "})\n",
    "print(f\"Resultado: {resultado_imp}\\n\")\n",
    "\n",
    "print(\"⚡ LCEL:\")\n",
    "resultado_lcel = tradutor_lcel.invoke({\n",
    "    \"texto\": texto_teste, \n",
    "    \"idioma\": idioma_teste\n",
    "})\n",
    "print(f\"Resultado: {resultado_lcel}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXEMPLO 2: ANALISADOR DE SENTIMENTOS COM LÓGICA CONDICIONAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=== EXEMPLO 2: ANALISADOR DE SENTIMENTOS ===\\n\")\n",
    "\n",
    "sentiment_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Analise o sentimento do texto: POSITIVO, NEGATIVO ou NEUTRO. Responda apenas com uma palavra.\"),\n",
    "    (\"human\", \"{texto}\")\n",
    "])\n",
    "\n",
    "response_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Com base no sentimento {sentimento}, gere uma resposta {tipo_resposta} ao texto original.\"),\n",
    "    (\"human\", \"Texto original: {texto}\")\n",
    "])\n",
    "\n",
    "# VERSÃO IMPERATIVE COM LÓGICA CONDICIONAL\n",
    "@chain\n",
    "def analisador_imperativo(values):\n",
    "    \"\"\"Analisador com lógica condicional complexa\"\"\"\n",
    "    texto = values[\"texto\"]\n",
    "    \n",
    "    # Passo 1: Analisar sentimento\n",
    "    sentiment_prompt = sentiment_template.invoke({\"texto\": texto})\n",
    "    sentimento = model.invoke(sentiment_prompt).content.strip().upper()\n",
    "    \n",
    "    # Passo 2: Lógica condicional baseada no sentimento\n",
    "    if sentimento == \"POSITIVO\":\n",
    "        tipo_resposta = \"encorajadora e positiva\"\n",
    "    elif sentimento == \"NEGATIVO\":\n",
    "        tipo_resposta = \"empática e de apoio\"\n",
    "    else:\n",
    "        tipo_resposta = \"equilibrada e informativa\"\n",
    "    \n",
    "    # Passo 3: Gerar resposta personalizada\n",
    "    response_prompt = response_template.invoke({\n",
    "        \"sentimento\": sentimento,\n",
    "        \"tipo_resposta\": tipo_resposta,\n",
    "        \"texto\": texto\n",
    "    })\n",
    "    \n",
    "    resposta_final = model.invoke(response_prompt).content\n",
    "    \n",
    "    # Retorna resultado estruturado\n",
    "    return {\n",
    "        \"texto_original\": texto,\n",
    "        \"sentimento_detectado\": sentimento,\n",
    "        \"tipo_resposta\": tipo_resposta,\n",
    "        \"resposta_gerada\": resposta_final\n",
    "    }\n",
    "\n",
    "# VERSÃO LCEL (mais limitada para lógica condicional)\n",
    "analisador_lcel_simples = sentiment_template | model | StrOutputParser()\n",
    "\n",
    "# TESTANDO O ANALISADOR COMPLEXO\n",
    "textos_teste = [\n",
    "    \"Estou muito feliz com meu novo emprego!\",\n",
    "    \"Estou muito triste e desanimado hoje...\",\n",
    "    \"O clima está nublado hoje.\"\n",
    "]\n",
    "\n",
    "print(\"🔧 IMPERATIVO (com lógica condicional):\")\n",
    "for texto in textos_teste:\n",
    "    resultado = analisador_imperativo.invoke({\"texto\": texto})\n",
    "    print(f\"📝 Texto: {resultado['texto_original']}\")\n",
    "    print(f\"😊 Sentimento: {resultado['sentimento_detectado']}\")\n",
    "    print(f\"🎯 Tipo de resposta: {resultado['tipo_resposta']}\")\n",
    "    print(f\"💬 Resposta: {resultado['resposta_gerada']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n⚡ LCEL (apenas análise de sentimento):\")\n",
    "for texto in textos_teste:\n",
    "    sentimento = analisador_lcel_simples.invoke({\"texto\": texto})\n",
    "    print(f\"📝 '{texto}' → 😊 {sentimento}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXEMPLO 3: PIPELINE DE PROCESSAMENTO DE TEXTO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== EXEMPLO 3: PIPELINE DE PROCESSAMENTO ===\\n\")\n",
    "\n",
    "# Templates para o pipeline\n",
    "extractor_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extraia as palavras-chave principais do texto. Liste apenas as palavras separadas por vírgula.\"),\n",
    "    (\"human\", \"{texto}\")\n",
    "])\n",
    "\n",
    "summarizer_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Resuma o texto em uma frase focando nas palavras-chave: {keywords}\"),\n",
    "    (\"human\", \"{texto}\")\n",
    "])\n",
    "\n",
    "# VERSÃO IMPERATIVE\n",
    "@chain\n",
    "def pipeline_imperativo(values):\n",
    "    \"\"\"Pipeline completo de processamento\"\"\"\n",
    "    texto = values[\"texto\"]\n",
    "    \n",
    "    # Etapa 1: Extrair palavras-chave\n",
    "    keywords_prompt = extractor_template.invoke({\"texto\": texto})\n",
    "    keywords = model.invoke(keywords_prompt).content.strip()\n",
    "    \n",
    "    # Etapa 2: Resumir com base nas keywords\n",
    "    summary_prompt = summarizer_template.invoke({\n",
    "        \"texto\": texto,\n",
    "        \"keywords\": keywords\n",
    "    })\n",
    "    summary = model.invoke(summary_prompt).content\n",
    "    \n",
    "    return {\n",
    "        \"texto_original\": texto,\n",
    "        \"palavras_chave\": keywords,\n",
    "        \"resumo\": summary\n",
    "    }\n",
    "\n",
    "# VERSÃO LCEL com RunnablePassthrough para dados complexos\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def extrair_keywords(values):\n",
    "    \"\"\"Função auxiliar para extrair keywords\"\"\"\n",
    "    prompt = extractor_template.invoke(values)\n",
    "    keywords = model.invoke(prompt).content.strip()\n",
    "    return {**values, \"keywords\": keywords}\n",
    "\n",
    "pipeline_lcel = (\n",
    "    RunnablePassthrough() \n",
    "    | extrair_keywords \n",
    "    | summarizer_template \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# TESTE DO PIPELINE\n",
    "texto_pipeline = \"\"\"\n",
    "A inteligência artificial está transformando rapidamente diversos setores da economia. \n",
    "Empresas estão investindo pesadamente em machine learning e automação para melhorar \n",
    "a eficiência operacional. No entanto, surgem preocupações sobre o impacto no emprego \n",
    "e a necessidade de regulamentação adequada para garantir o uso ético da tecnologia.\n",
    "\"\"\"\n",
    "\n",
    "print(\"🔧 PIPELINE IMPERATIVO:\")\n",
    "resultado_pipeline = pipeline_imperativo.invoke({\"texto\": texto_pipeline})\n",
    "print(f\"📝 Original: {resultado_pipeline['texto_original'][:100]}...\")\n",
    "print(f\"🔑 Keywords: {resultado_pipeline['palavras_chave']}\")\n",
    "print(f\"📄 Resumo: {resultado_pipeline['resumo']}\\n\")\n",
    "\n",
    "print(\"⚡ PIPELINE LCEL:\")\n",
    "resumo_lcel = pipeline_lcel.invoke({\"texto\": texto_pipeline})\n",
    "print(f\"📄 Resumo: {resumo_lcel}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXEMPLO 4: CHATBOT COM MEMÓRIA (IMPERATIVO)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== EXEMPLO 4: CHATBOT COM HISTÓRICO ===\\n\")\n",
    "\n",
    "# Este exemplo mostra como o imperativo é melhor para estados complexos\n",
    "@chain\n",
    "def chatbot_com_memoria(values):\n",
    "    \"\"\"Chatbot que mantém contexto da conversa\"\"\"\n",
    "    pergunta_atual = values[\"pergunta\"]\n",
    "    historico = values.get(\"historico\", [])\n",
    "    \n",
    "    # Constrói o contexto com histórico\n",
    "    messages = [(\"system\", \"Você é um assistente útil que lembra da conversa anterior.\")]\n",
    "    \n",
    "    # Adiciona histórico\n",
    "    for h in historico:\n",
    "        messages.append((\"human\", h[\"pergunta\"]))\n",
    "        messages.append((\"assistant\", h[\"resposta\"]))\n",
    "    \n",
    "    # Adiciona pergunta atual\n",
    "    messages.append((\"human\", pergunta_atual))\n",
    "    \n",
    "    # Cria template dinâmico\n",
    "    template_dinamico = ChatPromptTemplate.from_messages(messages)\n",
    "    \n",
    "    # Gera resposta\n",
    "    prompt = template_dinamico.invoke({})\n",
    "    resposta = model.invoke(prompt).content\n",
    "    \n",
    "    # Atualiza histórico\n",
    "    novo_historico = historico + [{\n",
    "        \"pergunta\": pergunta_atual,\n",
    "        \"resposta\": resposta\n",
    "    }]\n",
    "    \n",
    "    return {\n",
    "        \"resposta\": resposta,\n",
    "        \"historico_atualizado\": novo_historico\n",
    "    }\n",
    "\n",
    "# SIMULAÇÃO DE CONVERSA\n",
    "print(\"🤖 CHATBOT COM MEMÓRIA:\")\n",
    "historico_conversa = []\n",
    "\n",
    "perguntas = [\n",
    "    \"Qual é a capital do Brasil?\",\n",
    "    \"Quantos habitantes ela tem aproximadamente?\",\n",
    "    \"Qual é o principal ponto turístico dessa cidade?\"\n",
    "]\n",
    "\n",
    "for pergunta in perguntas:\n",
    "    resultado = chatbot_com_memoria.invoke({\n",
    "        \"pergunta\": pergunta,\n",
    "        \"historico\": historico_conversa\n",
    "    })\n",
    "    \n",
    "    print(f\"👤 Você: {pergunta}\")\n",
    "    print(f\"🤖 Bot: {resultado['resposta']}\")\n",
    "    print()\n",
    "    \n",
    "    historico_conversa = resultado[\"historico_atualizado\"]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"🎯 RESUMO DAS DIFERENÇAS:\")\n",
    "print(\"• IMPERATIVO: Melhor para lógica complexa, estados, condicionais\")\n",
    "print(\"• LCEL: Melhor para pipelines lineares, performance, simplicidade\")\n",
    "print(\"• Use @chain para combinar o melhor dos dois mundos!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06354500",
   "metadata": {},
   "source": [
    "### 1. Conceito Fundamental: Runnable\n",
    "\n",
    "\n",
    "O Runnable é a interface base do LangChain que padroniza como diferentes componentes podem ser executados e combinados. Qualquer objeto que implementa Runnable possui métodos como .invoke(), .stream(), .batch(), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6d3961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemplo\n",
    "# Todos estes são objetos Runnable:\n",
    "translator_template = ChatPromptTemplate.from_messages([...])  # Runnable\n",
    "model = ChatOpenAI(model=\"gpt-4.1\", temperature=0.0)          # Runnable  \n",
    "parser_output = StrOutputParser()                             # Runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668cd79f",
   "metadata": {},
   "source": [
    "### 2. RunnableSequence(operador pipe | )\n",
    "\n",
    "Quando você usa o `(operador |)` , está criando uma RunnableSequence - uma cadeia onde a saída de um componente vira entrada do próximo.\n",
    "\n",
    "```python\n",
    "translator_chain = translator_template | model | parser_output\n",
    "# Isso é equivalente a:\n",
    "# RunnableSequence(first=translator_template, middle=[model], last=parser_output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338882cf",
   "metadata": {},
   "source": [
    "Input: {\"texto\": \"Hello...\", \"idioma\": \"português\"}\n",
    "  ↓\n",
    "translator_template → Gera prompt formatado\n",
    "  ↓\n",
    "model → Processa prompt e gera resposta\n",
    "  ↓\n",
    "parser_output → Extrai string da resposta\n",
    "  ↓\n",
    "Output: \"Olá, como você está hoje?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fad4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
