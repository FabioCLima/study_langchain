{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee32b11",
   "metadata": {},
   "source": [
    "# Learning Langchain (The book)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6628b8b3",
   "metadata": {},
   "source": [
    "The main task of the software engineer working with LLMs is not to train an LLM, or even to fine-tune one (usually), but rather to take an existing LLM and work out how to get it to accomplish the task you need for your application. Adapting an existing LLM for your task is called `prompt engineering`\n",
    "\n",
    "`prompt engineering with LangChain` - how to use LangChain to get LLMs to do what do what you have in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e4e15",
   "metadata": {},
   "source": [
    "## Prompt technique\n",
    "___\n",
    "\n",
    "* Zero-Shot Prompting\n",
    "* Chain-of-Thought(CoT)\n",
    "* Retrieval-Augmented Generation - in real applications should be combined with CoT\n",
    "* Tool Calling - consist of prepending the prompt with a list of external functions the LLM can make use of, along with descriptions of what is good for nd instructions on how to use one (or more) of these functions.The developer of the application - should parse the output and call the appropriate functions.\n",
    "* Few-Shot Prompting\n",
    "\n",
    "Important things to keep in mind when prompting LLMs: each prompting technique is most useful when used in combination with (some of) the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364b44ed",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "**RAG** Ã© uma tÃ©cnica que combina **recuperaÃ§Ã£o de informaÃ§Ãµes** com **geraÃ§Ã£o de texto**. Em vez de depender apenas do conhecimento prÃ©-treinado do LLM, o RAG busca informaÃ§Ãµes relevantes em uma base de dados externa e usa essas informaÃ§Ãµes como contexto para gerar respostas mais precisas e atualizadas.\n",
    "\n",
    "### Funcionamento do RAG:\n",
    "1. **IndexaÃ§Ã£o**: Documentos sÃ£o vetorizados e armazenados\n",
    "2. **RecuperaÃ§Ã£o**: Query do usuÃ¡rio busca documentos similares\n",
    "3. **GeraÃ§Ã£o**: LLM usa documentos recuperados como contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f89c9",
   "metadata": {},
   "source": [
    "`LLM interface simply takes a string prompt as input, sends the input to the model provider, and then returns the model prediction as output.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7871f",
   "metadata": {},
   "source": [
    "## CapÃ­tulo 1\n",
    "___\n",
    "\n",
    "a - Chamada a um llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "58811059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate  # type: ignore\n",
    "from langchain_openai import ChatOpenAI  # type: ignore\n",
    "\n",
    "# * Carrega as variÃ¡veis de ambiente\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# * Verifica se a API key estÃ¡ configurada\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY nÃ£o encontrada no arquivo .env\")\n",
    "\n",
    "# * Configura o modelo com parÃ¢metros especÃ­ficos\n",
    "model: ChatOpenAI = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,  # Controla a criatividade das respostas\n",
    ")  # type: ignore\n",
    "\n",
    "response = model.invoke(\"The Sky is ?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d956fc3",
   "metadata": {},
   "source": [
    "b - Chatmodel\n",
    "\n",
    "`HumanMessage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7bb8325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Brasil is BrasÃ­lia.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = [HumanMessage(\"What is the capital of Brasil?\")]\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1725a9b",
   "metadata": {},
   "source": [
    "c - System\n",
    "\n",
    "`SystemMessage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "57d2fcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris!!!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "system_msg = \"\"\"You are a helpful assistant that responds to questions with three exclamations marks.\n",
    "\"\"\"\n",
    "human_msg = HumanMessage(\"What is the capital of France?\")\n",
    "response = model.invoke([system_msg, human_msg])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e89fd3",
   "metadata": {},
   "source": [
    "Prompt instructions significantly influences the model's output. Prompts help the model understand context and generate relevant answers to queries.\n",
    "\n",
    "LangChain provides prompt template interfaces that make it easy to construct prompts with dynamic inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "47b79d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Answer the questions based on the context below.\\nIf the question cannot be answered using the information provided, answer with \"I don\\'t know\".\\n\\nContext: The most recent advancements in NLP are being driven by Large Language Model (LLM). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face\\'s \\'transformers\\' library, or by utilizing OpenAI and cohere\\'s offerings through the `openai` and `cohere` libraries, respectively.\\n\\nQuestion: Which model providers offers LLMs?'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the questions based on the context below.\n",
    "If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\"\"\")\n",
    "\n",
    "response = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Model (LLM). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's 'transformers' library, or by utilizing OpenAI and cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offers LLMs?\",\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "246ae6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responda a questÃ£o baseada no contexto a seguir. Se a questÃ£o nÃ£o puder ser respondida usando a informaÃ§Ã£o, responda usando 'NÃ£o sei a resposta para essa pergunta!'\n",
      "\n",
      "Contexto: Quais os frameworks para a criaÃ§Ã£o de IAs, chatbots, mais usados para construÃ§Ã£o de agentes de IA.\n",
      "\n",
      "Pergunta: Qual Ã© o framework mais performÃ¡tico e mais simples de aprender?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "    \"\"\"Responda a questÃ£o baseada no contexto a seguir. Se a questÃ£o nÃ£o puder ser respondida usando a informaÃ§Ã£o, responda usando 'NÃ£o sei a resposta para essa pergunta!'\n",
    "\n",
    "Contexto: {contexto}\n",
    "\n",
    "Pergunta: {pergunta}\"\"\"\n",
    ")\n",
    "\n",
    "# 2. Invoke corrigido - fora do template e com aspas fechadas\n",
    "response = template.invoke(\n",
    "    {\n",
    "        \"contexto\": \"Quais os frameworks para a criaÃ§Ã£o de IAs, chatbots, mais usados para construÃ§Ã£o de agentes de IA.\",\n",
    "        \"pergunta\": \"Qual Ã© o framework mais performÃ¡tico e mais simples de aprender?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3. PromptValue nÃ£o tem .content, usar .text ou apenas print(response)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bbe655",
   "metadata": {},
   "source": [
    "`Both template and model can be reused many times`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10f8f0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hugging Face, OpenAI, and Cohere offer LLMs.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 132, 'total_tokens': 147, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C07tYZMNve7m7F1WJWItcd3KuXOdR', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--0d9513c6-c043-433a-8a43-b082338c1872-0' usage_metadata={'input_tokens': 132, 'output_tokens': 15, 'total_tokens': 147, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "# `prompt` and `completion` are the results of using template and model once\n",
    "\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0abc7470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='NÃ£o sei a resposta para essa pergunta!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 85, 'total_tokens': 93, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_51e1070cf2', 'id': 'chatcmpl-C07tZAYbUcPzg4OQ1xQuBpro9lEdr', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--f24761cf-de75-4448-a62a-e641b2954c72-0' usage_metadata={'input_tokens': 85, 'output_tokens': 8, 'total_tokens': 93, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "    \"\"\"Responda a questÃ£o baseada no contexto a seguir. Se a questÃ£o nÃ£o puder ser respondida usando a informaÃ§Ã£o, responda usando 'NÃ£o sei a resposta para essa pergunta!'\n",
    "\n",
    "Contexto: {contexto}\n",
    "\n",
    "Pergunta: {pergunta}\"\"\"\n",
    ")\n",
    "model = ChatOpenAI(model=\"gpt-4.1\")\n",
    "\n",
    "# 2. Invoke corrigido - fora do template e com aspas fechadas\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"contexto\": \"Quais os frameworks para a criaÃ§Ã£o de IAs, chatbots, mais usados para construÃ§Ã£o de agentes de IA.\",\n",
    "        \"pergunta\": \"Qual Ã© o framework mais performÃ¡tico e mais simples de aprender?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3. PromptValue nÃ£o tem .content, usar .text ou apenas print(response)\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "64d9448c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NÃ£o sei a resposta para essa pergunta!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "\"\"\"Responda a pergunta baseada no contexto a seguir. Se a pergunta nÃ£o puder ser respondida usando a informaÃ§Ã£o,\n",
    "previamente fornecida, responda com um 'NÃ£o sei a resposta para essa pergunta!'\n",
    "\n",
    "Contexto: {contexto}\n",
    "Pergunta: {pergunta}\n",
    "\n",
    "\"\"\")\n",
    "model = ChatOpenAI(model=\"gpt-4.1\")\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"contexto\": \"Considerando a programaÃ§Ã£o orientada a objetos no Python, o que Ã© um objeto\",\n",
    "        \"pergunta\": \"Como criar uma classe em Python usando o conceito de encapsulamento?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f80bc9a",
   "metadata": {},
   "source": [
    "Building an AI chat application, the `ChatPromptTemplate` can be used instead to provide dynamic inputs based on the role of the chat message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7484cbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Context: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: Which model providers offer LLMs?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
    "        ),\n",
    "        (\"human\", \"Context: {context}\"),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aeb633d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='OpenAI and Cohere.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 137, 'total_tokens': 143, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C07taxiItparR9kKemjMdGcIiheiS', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--bf198af0-547e-4ff9-9bb8-cb979a680a90-0' usage_metadata={'input_tokens': 137, 'output_tokens': 6, 'total_tokens': 143, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# both `template` and `model` can be reused many times\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
    "        ),\n",
    "        (\"human\", \"Context: {context}\"),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# `prompt` and `completion` are the results of using template and model once\n",
    "\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a632c",
   "metadata": {},
   "source": [
    "1. PadrÃ£o de Prompts DinÃ¢micos - Sim, Ã© um padrÃ£o estabelecido!\n",
    "O cÃ³digo que vocÃª estÃ¡ vendo segue um padrÃ£o muito comum e recomendado no LangChain:\n",
    "Por que este padrÃ£o Ã© importante?\n",
    "```python\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"InstruÃ§Ã£o do sistema...\"),\n",
    "    (\"human\", \"Entrada do usuÃ¡rio: {variavel}\"),\n",
    "])\n",
    "```\n",
    "ReutilizaÃ§Ã£o: O template pode ser usado mÃºltiplas vezes com diferentes dados\n",
    "* SeparaÃ§Ã£o de responsabilidades: O prompt (template) Ã© separado da execuÃ§Ã£o (model)\n",
    "* Manutenibilidade: FÃ¡cil de modificar e testar prompts\n",
    "* Escalabilidade: Pode ser usado em pipelines complexos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a95a3",
   "metadata": {},
   "source": [
    "Quando usar:\n",
    "* Definir o papel(role)/comportamento(behave) do modelo.\n",
    "* Estabelecer regras e limitaÃ§Ãµes\n",
    "* Configurar o contexto geral da conversa\n",
    "* Definir o tom e estilo de resposta.\n",
    "\n",
    "\n",
    "```python\n",
    "(\"system\", \"VocÃª Ã© um tutor de programaÃ§Ã£o Python. Sempre explique conceitos de forma simples e forneÃ§a exemplos prÃ¡ticos.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b898b536",
   "metadata": {},
   "source": [
    "`HumanMessage` (Mensagem Humana)\n",
    "\n",
    "```python\n",
    "(\"human\", \"Pergunta: {question}\")\n",
    "```\n",
    "\n",
    "Quando Usar:\n",
    "\n",
    "* Entradas do usuÃ¡rio\n",
    "* Perguntas especÃ­ficas\n",
    "* Dados que precisam ser processados\n",
    "* Contexto que o usuÃ¡rio fornece\n",
    "\n",
    "```python\n",
    "(\"human\", \"Explique o conceito de list comprehension em Python\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f531d8df",
   "metadata": {},
   "source": [
    "`AIMessage (Mensagem da IA)`\n",
    "```python\n",
    "(\"ai\", \"Resposta da IA...\")\n",
    "```\n",
    "\n",
    "Quando Usar:\n",
    "\n",
    "* Simular conversar anteriores\n",
    "* Fornecer exemplos de respostas\n",
    "* Criar contextos de few-shot learning\n",
    "* Manter histÃ³rico de conversas\n",
    "\n",
    "Exemplo prÃ¡tico:\n",
    "```python\n",
    "(\"ai\", \"List comprehension Ã© uma forma concisa de criar listas em Python. Exemplo: [x*2 for x in range(5)]\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e2bd6",
   "metadata": {},
   "source": [
    "`Estrutura Recomendada para Prompts DinÃ¢micos`\n",
    "___\n",
    "```python\n",
    "# 1. Definir o template com tipos apropriados\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Defina o papel/contexto do modelo\"),\n",
    "    (\"human\", \"ForneÃ§a o contexto: {context}\"),\n",
    "    (\"human\", \"FaÃ§a a pergunta: {question}\"),\n",
    "])\n",
    "\n",
    "# 2. Criar o modelo\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# 3. Invocar com dados dinÃ¢micos\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"seu contexto aqui\",\n",
    "    \"question\": \"sua pergunta aqui\"\n",
    "})\n",
    "\n",
    "# 4. Obter a resposta\n",
    "response = model.invoke(prompt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace97d30",
   "metadata": {},
   "source": [
    "`**Boas PrÃ¡ticas para Desenvolvedores de Agentes**`\n",
    "___\n",
    "\n",
    "1.Sempre use templates para prompts dinÃ¢micos\n",
    "\n",
    "    -Evite strings hardcoded\n",
    "    -Facilita testes e modificaÃ§Ãµes\n",
    "\n",
    "2.Separe claramente os tipos de mensagens\n",
    "\n",
    "    -System: configuraÃ§Ã£o/contexto\n",
    "    -Human: entrada do usuÃ¡rio\n",
    "    -AI: respostas/exemplos\n",
    "\n",
    "3.Use variÃ¡veis para dados dinÃ¢micos\n",
    "\n",
    "    -{context}, {question}, {user_input}\n",
    "    -Facilita a reutilizaÃ§Ã£o\n",
    "\n",
    "4.Mantenha prompts modulares\n",
    "\n",
    "    -Cada template com responsabilidade especÃ­fica\n",
    "    -Combine templates para casos complexos\n",
    "\n",
    "**VersÃ£o bÃ¡sica**\n",
    "```python\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Responda baseado no contexto.\"),\n",
    "    (\"human\", \"Contexto: {context}\"),\n",
    "    (\"human\", \"Pergunta: {question}\"),\n",
    "])\n",
    "```\n",
    "**VersÃ£o avanÃ§ada(para agentes)**\n",
    "```python\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"VocÃª Ã© um assistente especializado em anÃ¡lise de dados.\"),\n",
    "    (\"human\", \"Contexto: {context}\"),\n",
    "    (\"human\", \"Pergunta: {question}\"),\n",
    "    (\"ai\", \"Vou analisar o contexto e responder sua pergunta.\"),\n",
    "    (\"human\", \"Por favor, seja especÃ­fico e forneÃ§a exemplos.\"),\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cfdf0c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O objetivo da jornada de Frodo Ã© destruir o Um Anel, levando-o atÃ© a Montanha da PerdiÃ§Ã£o, onde ele foi forjado, para que o anel seja destruÃ­do e, assim, impedir que o Lorde das Trevas Sauron recupere seu poder e domine a Terra-mÃ©dia.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# Adicionamos o Output Parser para extrair a resposta final como texto\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1\")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"VocÃª Ã© um assistente de IA que Ã© especialista em filmes e sÃ©ries.\"),\n",
    "    (\"human\", \"Contexto: {contexto}\"),\n",
    "    (\"human\", \"Pergunta: {pergunta}\"),\n",
    "    (\"ai\", \"Analise o contexto e responda a pergunta\")\n",
    "])\n",
    "# Conectamos os componentes na ordem que devem ser executados.\n",
    "chain = template | model | StrOutputParser()\n",
    "\n",
    "# 4. Invocando a Chain (com o input correto)\n",
    "# Agora, fornecemos um dicionÃ¡rio com as chaves que o template espera.\n",
    "# Note que o contexto e a pergunta agora fazem sentido com o prompt do sistema.\n",
    "contexto_exemplo = \"Em 'O Senhor dos AnÃ©is: A Sociedade do Anel', Frodo Bolseiro, um hobbit, herda um anel poderoso de seu tio Bilbo. Ele descobre que o anel Ã© o Um Anel, uma arma do Lorde das Trevas Sauron, e deve embarcar em uma jornada para destruÃ­-lo na Montanha da PerdiÃ§Ã£o.\"\n",
    "pergunta_exemplo = \"Qual Ã© o objetivo da jornada de Frodo?\"\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"contexto\": contexto_exemplo, \n",
    "    \"pergunta\": pergunta_exemplo\n",
    "})\n",
    "\n",
    "# 5. Imprimindo a resposta\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18157cbe",
   "metadata": {},
   "source": [
    "Para lembrar:\n",
    "\n",
    " - `SystemMessage` = \"Quem vocÃª Ã©\"\n",
    " - `HumanMessage` = \"O que o usuÃ¡rio quer\"\n",
    " - `AIMessage` = \"Como vocÃª responde\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8f0c28b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Um modelo de machine learning Ã© um algoritmo que aprende padrÃµes a partir de dados para fazer previsÃµes ou tomar decisÃµes automatizadas em situaÃ§Ãµes futuras.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"VocÃª Ã© um assistente prestativo, especialista em MatemÃ¡tica.\"\n",
    "    ),\n",
    "    HumanMessage(content=\"Explique o que Ã© um modelo de machine learning, em atÃ© 30 palavras\"),\n",
    "]\n",
    "\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3c4aa4",
   "metadata": {},
   "source": [
    "`PromptTemplate` simples Ã© sÃ³ para texto, `ChatPromptTemplate` Ã© para conversas com roles(especialista em Python, ou matemÃ¡tica, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a9a6d371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain Ã© um framework de desenvolvimento de aplicativos descentralizados (dApps) que utiliza a linguagem de programaÃ§Ã£o Rust. Ele facilita a criaÃ§Ã£o de dApps seguras e escalÃ¡veis, integrando-se com blockchains como Ethereum.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"VocÃª Ã© um especialista em {tÃ³pico}\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"Explique {assunto} em 35 palavras\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Template\n",
    "formatted_prompt = prompt.format_messages(tÃ³pico=\"framework\", assunto=\"langchain\")\n",
    "\n",
    "response = chat.invoke(formatted_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5344a4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em Python, um decorator Ã© uma funÃ§Ã£o que recebe outra funÃ§Ã£o como argumento e retorna uma nova funÃ§Ã£o modificada. Decorators sÃ£o usados para adicionar funcionalidades a funÃ§Ãµes existentes sem modificar seu cÃ³digo diretamente, permitindo reutilizaÃ§Ã£o de cÃ³digo e separaÃ§Ã£o de preocupaÃ§Ãµes.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Template com papel especÃ­fico\n",
    "template = ChatPromptTemplate.from_messages([ # type: ignore\n",
    "    (\"system\", \"VocÃª Ã© um tutor de programaÃ§Ã£o Python especializado em explicar conceitos complexos de forma simples.\"),\n",
    "    (\"human\", \"Conceito: {conceito}\"),\n",
    "    (\"human\", \"Explique este conceito em {palavras} palavras ou menos\"),\n",
    "])\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Uso do template\n",
    "response = model.invoke(template.invoke({# type: ignore\n",
    "    \"conceito\": \"decorators em Python\",\n",
    "    \"palavras\": \"50\"\n",
    "}))\n",
    "\n",
    "print(response.content)# type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af41a7e3",
   "metadata": {},
   "source": [
    "## Getting Specific Formats out of LLMs\n",
    "___\n",
    "\n",
    "The most common format to generate with LLMs is JSON, which can be sent over the to your frontend code or be saved to a databased.\n",
    "\n",
    "1. First task, define the output schema (from LLM). Then include that schema in the prompt , along with the text you want to use as the source.\n",
    "2. In order to define a `schema`, this is easiest to do with `Pydantic`(*library* used for validating data against schemas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "115f857d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='They weigh the same.' justification='A pound is a unit of weight, so a pound of bricks and a pound of feathers both weigh exactly one pound. The material does not affect the weight if the quantity is specified as a pound.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    \"\"\"An answer to the user's question along with justification for the answer.\"\"\"\n",
    "\n",
    "    answer: str\n",
    "    \"\"\"The answer to the user's question\"\"\"\n",
    "    justification: str\n",
    "    \"\"\"Justification for the answer\"\"\"\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "structured_model = model.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "response = structured_model.invoke(\n",
    "    \"What weighs more, a pound of bricks or a pound of feathers\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec1ce2d",
   "metadata": {},
   "source": [
    "#### TUTORIAL: LangChain + Pandantic - Structured Output\n",
    "========================================================\n",
    "\n",
    "Este exemplo demonstra como usar Pydantic com LangChain para:\n",
    "1. Validar dados de entrada e saÃ­da\n",
    "2. Garantir formato estruturado das respostas\n",
    "3. Tornar o cÃ³digo mais robusto e type-safe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c5aa3276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1066a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerWithJustification(BaseModel):\n",
    "    \"\"\"\n",
    "    Modelo que representa uma resposta estruturada com justificativa.\n",
    "\n",
    "      Por que usar Pydantic aqui?\n",
    "    - ValidaÃ§Ã£o automÃ¡tica de tipos\n",
    "    - DocumentaÃ§Ã£o clara dos campos\n",
    "    - SerializaÃ§Ã£o JSON automÃ¡tica\n",
    "    - IntegraÃ§Ã£o nativa com LangChain\n",
    "    \"\"\"\n",
    "\n",
    "    answer: str = Field(\n",
    "        description=\"Resposta clara e concisa para a pergunta\",\n",
    "        min_length=1,\n",
    "        max_length=500,\n",
    "    )\n",
    "\n",
    "    justification: str = Field(\n",
    "        description=\"ExplicaÃ§Ã£o detalhada do raciocÃ­nio\", min_length=10, max_length=100\n",
    "    )\n",
    "\n",
    "    confidence_level: float | None = Field(\n",
    "        default=None, description=\"NÃ­vel de confianÃ§a (0-1) na resposta\", ge=0.0, le=1.0\n",
    "    )\n",
    "\n",
    "    timestamp: str | None = Field(\n",
    "        default_factory=lambda: datetime.now().isoformat(),\n",
    "        description=\"Timestamp da resposta\",\n",
    "    )\n",
    "\n",
    "    def display_response(self) -> str:\n",
    "        \"\"\"\n",
    "        MÃ©todo para exibir a resposta de forma mais legÃ­vel.\n",
    "\n",
    "        ğŸ”§ BENEFÃCIO: Evita o scroll infinito no output!\n",
    "        \"\"\"\n",
    "        output = f\"\"\"\n",
    "â”Œâ”€ ğŸ’¬ RESPOSTA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ {self.answer}\n",
    "â”œâ”€ ğŸ¤” JUSTIFICATIVA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ {self.justification}\n",
    "\"\"\"\n",
    "\n",
    "        if self.confidence_level:\n",
    "            confidence_bar = \"â–ˆ\" * int(self.confidence_level * 10)\n",
    "            output += f\"\"\"â”œâ”€ ğŸ“Š CONFIANÃ‡A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ {confidence_bar} {self.confidence_level:.1%}\n",
    "\"\"\"\n",
    "\n",
    "        output += f\"\"\"â””â”€ ğŸ•’ {self.timestamp} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\"\"\"\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6881489d",
   "metadata": {},
   "source": [
    "#### ğŸš€ CLASSE PRINCIPAL PARA DEMONSTRAÃ‡ÃƒO\n",
    "\n",
    "`class LangChainPydanticDemo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1c344c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangChainPydanticDemo:\n",
    "    \"\"\"\n",
    "    Conceitos Importantes:\n",
    "    1. Structured Output: LangChain forÃ§a o LLM a retornar dados no formato exato do\n",
    "    modelo Pydantic.\n",
    "\n",
    "    2.Type safety: Pydantic valida automaticamente os tipos\n",
    "\n",
    "    3.Error Handling: Falhas na validaÃ§Ã£o sÃ£o capturadas\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"gpt-4.1\"):\n",
    "        self.model = ChatOpenAI(\n",
    "            model=model_name,\n",
    "            temperature=0.1,  # * baixa criatividade para respostas consistentes\n",
    "        )\n",
    "\n",
    "        # * ğŸ”‘ PONTO CHAVE: with_structured_output() forÃ§a o formato\n",
    "        self.structured_model = self.model.with_structured_output(\n",
    "            AnswerWithJustification\n",
    "        )\n",
    "\n",
    "    def ask_question(self, question: str) -> AnswerWithJustification:\n",
    "        \"\"\"\n",
    "        Faz uma pergunta e retorna resposta estruturada.\n",
    "\n",
    "        ğŸ¯ BENEFÃCIOS do Structured Output:\n",
    "        - Resposta sempre no formato esperado\n",
    "        - ValidaÃ§Ã£o automÃ¡tica dos dados\n",
    "        - Facilita integraÃ§Ã£o com APIs e bancos de dados\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.structured_model.invoke(question)\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            # * Em caso de erro, retorna resposta padrÃ£o\n",
    "            return AnswerWithJustification(\n",
    "                answer=\"Erro ao processar pergunta\",\n",
    "                justification=f\"Erro tÃ©cnico: {e!s}\",\n",
    "                confidence_level=0.0,\n",
    "            )\n",
    "\n",
    "    def demo_multiple_questions(self):\n",
    "        \"\"\"\n",
    "        Demonstra o uso com mÃºltiplas perguntas para mostrar consistÃªncia.\n",
    "        \"\"\"\n",
    "        questions = [\n",
    "            \"O que pesa mais: 1kg de chumbo ou 1kg de algodÃ£o?\",\n",
    "            \"Por que o cÃ©u Ã© azul?\",\n",
    "            \"Qual a diferenÃ§a entre Python e JavaScript?\",\n",
    "        ]\n",
    "\n",
    "        print(\"ğŸ“ DEMONSTRAÃ‡ÃƒO: MÃºltiplas perguntas com formato consistente\\n\")\n",
    "\n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"â“ PERGUNTA {i}: {question}\")\n",
    "            response = self.ask_question(question)\n",
    "            print(response.display_response())\n",
    "            print(\"\\n\" + \"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e19c7e6",
   "metadata": {},
   "source": [
    "#### ğŸ“– EXEMPLOS PRÃTICOS DE USO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "76ae2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exemplo_basico():\n",
    "    \"\"\"Exemplo bÃ¡sico de uso do structured output.\"\"\"\n",
    "    print(\"ğŸ”µ EXEMPLO 1: Uso BÃ¡sico\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    demo = LangChainPydanticDemo()\n",
    "\n",
    "    question = \"Explique o conceito de recursÃ£o em programaÃ§Ã£o\"\n",
    "    response = demo.ask_question(question)\n",
    "\n",
    "    # âœ¨ SAÃDA LIMPA - sem scroll infinito!\n",
    "    print(response.display_response())\n",
    "\n",
    "    # ğŸ’¾ BONUS: FÃ¡cil conversÃ£o para JSON\n",
    "    print(\"\\nğŸ”§ BONUS - Dados em JSON:\")\n",
    "    print(json.dumps(response.model_dump(), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1fdb5c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”µ EXEMPLO 1: Uso BÃ¡sico\n",
      "----------------------------------------\n",
      "\n",
      "â”Œâ”€ ğŸ’¬ RESPOSTA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ RecursÃ£o em programaÃ§Ã£o Ã© uma tÃ©cnica onde uma funÃ§Ã£o chama a si mesma, direta ou indiretamente, para resolver um problema dividindo-o em subproblemas menores do mesmo tipo.\n",
      "â”œâ”€ ğŸ¤” JUSTIFICATIVA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ A recursÃ£o Ã© usada quando um problema pode ser decomposto em versÃµes menores de si mesmo. Por meio d\n",
      "â”œâ”€ ğŸ“Š CONFIANÃ‡A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 99.0%\n",
      "â””â”€ ğŸ•’ 2024-06-18T18:00:00Z â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ”§ BONUS - Dados em JSON:\n",
      "{\n",
      "  \"answer\": \"RecursÃ£o em programaÃ§Ã£o Ã© uma tÃ©cnica onde uma funÃ§Ã£o chama a si mesma, direta ou indiretamente, para resolver um problema dividindo-o em subproblemas menores do mesmo tipo.\",\n",
      "  \"justification\": \"A recursÃ£o Ã© usada quando um problema pode ser decomposto em versÃµes menores de si mesmo. Por meio d\",\n",
      "  \"confidence_level\": 0.99,\n",
      "  \"timestamp\": \"2024-06-18T18:00:00Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "exemplo_basico()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5a92e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exemplo_avancado():\n",
    "    \"\"\"Exemplo avanÃ§ado mostrando validaÃ§Ã£o.\"\"\"\n",
    "    print(\"\\nğŸŸ¢ EXEMPLO 2: ValidaÃ§Ã£o com Pydantic\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Tentativa de criar resposta invÃ¡lida (para mostrar validaÃ§Ã£o)\n",
    "    try:\n",
    "        invalid_response = AnswerWithJustification(\n",
    "            answer=\"\",  # âŒ InvÃ¡lido: muito curto\n",
    "            justification=\"Curto\",  # âŒ InvÃ¡lido: muito curto\n",
    "            confidence_level=1.5,  # âŒ InvÃ¡lido: > 1.0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ VALIDAÃ‡ÃƒO PYDANTIC FUNCIONOU: {e}\")\n",
    "\n",
    "    # âœ… Resposta vÃ¡lida\n",
    "    valid_response = AnswerWithJustification(\n",
    "        answer=\"Python Ã© uma linguagem de programaÃ§Ã£o\",\n",
    "        justification=\"Python Ã© conhecida por sua sintaxe simples e legÃ­vel\",\n",
    "        confidence_level=0.9,\n",
    "    )\n",
    "\n",
    "    print(\"\\nâœ… RESPOSTA VÃLIDA:\")\n",
    "    print(valid_response.display_response())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "96f501d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŸ¢ EXEMPLO 2: ValidaÃ§Ã£o com Pydantic\n",
      "----------------------------------------\n",
      "âŒ VALIDAÃ‡ÃƒO PYDANTIC FUNCIONOU: 3 validation errors for AnswerWithJustification\n",
      "answer\n",
      "  String should have at least 1 character [type=string_too_short, input_value='', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_too_short\n",
      "justification\n",
      "  String should have at least 10 characters [type=string_too_short, input_value='Curto', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_too_short\n",
      "confidence_level\n",
      "  Input should be less than or equal to 1 [type=less_than_equal, input_value=1.5, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/less_than_equal\n",
      "\n",
      "âœ… RESPOSTA VÃLIDA:\n",
      "\n",
      "â”Œâ”€ ğŸ’¬ RESPOSTA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ Python Ã© uma linguagem de programaÃ§Ã£o\n",
      "â”œâ”€ ğŸ¤” JUSTIFICATIVA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Python Ã© conhecida por sua sintaxe simples e legÃ­vel\n",
      "â”œâ”€ ğŸ“Š CONFIANÃ‡A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 90.0%\n",
      "â””â”€ ğŸ•’ 2025-08-02T11:53:18.773594 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "exemplo_avancado()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2224df6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ DEMONSTRAÃ‡ÃƒO COMPLETA:\n",
      "============================================================\n",
      "ğŸ“ DEMONSTRAÃ‡ÃƒO: MÃºltiplas perguntas com formato consistente\n",
      "\n",
      "â“ PERGUNTA 1: O que pesa mais: 1kg de chumbo ou 1kg de algodÃ£o?\n",
      "\n",
      "â”Œâ”€ ğŸ’¬ RESPOSTA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ 1 kg de chumbo pesa o mesmo que 1 kg de algodÃ£o.\n",
      "â”œâ”€ ğŸ¤” JUSTIFICATIVA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ A unidade de medida 'quilograma' (kg) representa uma quantidade fixa de massa. Portanto, 1 kg de umÂ \n",
      "â”œâ”€ ğŸ“Š CONFIANÃ‡A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100.0%\n",
      "â””â”€ ğŸ•’ 2024-06-18T20:00:00Z â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "============================================================\n",
      "\n",
      "â“ PERGUNTA 2: Por que o cÃ©u Ã© azul?\n",
      "\n",
      "â”Œâ”€ ğŸ’¬ RESPOSTA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ O cÃ©u Ã© azul porque a luz do Sol, ao passar pela atmosfera da Terra, sofre espalhamento pelas molÃ©culas de ar, e a luz azul Ã© espalhada em todas as direÃ§Ãµes com mais intensidade do que outras cores.\n",
      "â”œâ”€ ğŸ¤” JUSTIFICATIVA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ A luz branca do Sol Ã© composta por vÃ¡rias cores, cada uma com um comprimento de onda diferente. Ao a\n",
      "â”œâ”€ ğŸ“Š CONFIANÃ‡A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100.0%\n",
      "â””â”€ ğŸ•’ 2024-06-18T19:00:00Z â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "============================================================\n",
      "\n",
      "â“ PERGUNTA 3: Qual a diferenÃ§a entre Python e JavaScript?\n",
      "\n",
      "â”Œâ”€ ğŸ’¬ RESPOSTA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ Python e JavaScript sÃ£o linguagens de programaÃ§Ã£o diferentes, cada uma com sintaxe, paradigmas e usos distintos. Python Ã© geralmente usado para ciÃªncia de dados, automaÃ§Ã£o, backend e scripts, enquanto JavaScript Ã© mais utilizado para desenvolvimento web, especialmente no frontend.\n",
      "â”œâ”€ ğŸ¤” JUSTIFICATIVA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Python Ã© uma linguagem de propÃ³sito geral, conhecida por sua sintaxe simples e legibilidade, muito a\n",
      "â”œâ”€ ğŸ“Š CONFIANÃ‡A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 95.0%\n",
      "â””â”€ ğŸ•’ 2023-07-01T12:00:00Z â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DemonstraÃ§Ã£o completa\n",
    "print(\"\\nğŸ“ DEMONSTRAÃ‡ÃƒO COMPLETA:\")\n",
    "print(\"=\" * 60)\n",
    "demo = LangChainPydanticDemo()\n",
    "demo.demo_multiple_questions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61a6bc3",
   "metadata": {},
   "source": [
    "ğŸ“ LIÃ‡Ã•ES APRENDIDAS:\n",
    "\n",
    "1. **Pydantic + LangChain = Dados Estruturados**\n",
    "   - LLM retorna sempre no formato correto\n",
    "   - ValidaÃ§Ã£o automÃ¡tica de tipos e valores\n",
    "\n",
    "2. **BenefÃ­cios para ProduÃ§Ã£o**\n",
    "   - Code completion no IDE\n",
    "   - DocumentaÃ§Ã£o automÃ¡tica\n",
    "   - IntegraÃ§Ã£o fÃ¡cil com APIs\n",
    "\n",
    "3. **Melhor UX**\n",
    "   - SaÃ­da formatada e legÃ­vel\n",
    "   - Sem scroll infinito\n",
    "   - InformaÃ§Ãµes organizadas\n",
    "\n",
    "4. **Type Safety**\n",
    "   - Erros capturados em desenvolvimento\n",
    "   - CÃ³digo mais robusto\n",
    "   - ManutenÃ§Ã£o facilitada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f84ea1",
   "metadata": {},
   "source": [
    "**Por que Pydantic Ã© Fundamental no LangChain**:\n",
    "\n",
    "1. Structured Output: ForÃ§a o LLM a retornar dados no formato exato\n",
    "2. ValidaÃ§Ã£o AutomÃ¡tica: Tipos e valores sÃ£o verificados automaticamente\n",
    "3. IntegraÃ§Ã£o Nativa: LangChain foi projetado para trabalhar com Pydantic\n",
    "4. ProduÃ§Ã£o Ready: Facilita APIs, banco de dados e frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365e92c",
   "metadata": {},
   "source": [
    "#### Other Machine-Readable Formats with Output Parsers\n",
    "___\n",
    "\n",
    "We can also use an LLM or chat model to produce output in other formats, such as CSV or XML. Using output parsers  that are classes that help us structure large language model responses.\n",
    "\n",
    "    - Providing format instructions\n",
    "    - Validating and parsing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5c4a436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'cherry']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "response = parser.invoke(\"apple, banana, cherry\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b243dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7020209",
   "metadata": {},
   "source": [
    "1. Classe Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8d21cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    email: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e8fa8",
   "metadata": {},
   "source": [
    "2. Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e231eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=UserInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55358e78",
   "metadata": {},
   "source": [
    "3. Prompt com instruÃ§Ã£o de saÃ­da estruturada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "27f76dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template = \"\"\"Dado o seguinte texto: '{text}', gere um JSON com as seguintes informaÃ§Ãµes: {format_instructions}\"\"\",\n",
    "    input_variables = [\"text\"],\n",
    "    partial_variables = {\"format_instructions\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1e3ab",
   "metadata": {},
   "source": [
    "4. LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bd150bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "07ee0806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Ana Paula' age=32 email='ana.paula@gmail.com'\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"O nome dela Ã© Ana Paula, tem 32 anos e o email dela Ã© ana.paula@gmail.com, e ela mora em SÃ£o Paulo\"\"\"\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "response = chain.invoke({\"text\": input_text})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9044c911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nome, preÃ§o, categoria\n",
      "Arroz, 20 reais, Alimentos\n",
      "Sabonete, 5 reais, Higiene\n",
      "Caderno, 15 reais, Papelaria\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "csv_prompt = PromptTemplate.from_template(\n",
    "    \"Converta a seguinte lista de produtos em formato CSV com colunas: nome, preÃ§o, categoria.\\n\\n{text}\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "input_text = \"\"\"\n",
    "1. Arroz, 20 reais, Alimentos\n",
    "2. Sabonete, 5 reais, Higiene\n",
    "3. Caderno, 15 reais, Papelaria\n",
    "\"\"\"\n",
    "\n",
    "csv_result = llm.invoke(csv_prompt.format(text=input_text))\n",
    "print(csv_result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1de9f708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='1984' author='George Orwell' year=1949\n",
      "{'title': '1984', 'author': 'George Orwell', 'year': 1949}\n"
     ]
    }
   ],
   "source": [
    "# Modelo simples para validar como Python dict\n",
    "class BookInfo(BaseModel):\n",
    "    title: str\n",
    "    author: str\n",
    "    year: int\n",
    "\n",
    "book_parser = PydanticOutputParser(pydantic_object=BookInfo)\n",
    "\n",
    "book_prompt = PromptTemplate(\n",
    "    template=\"Extraia as informaÃ§Ãµes do livro e retorne no formato:\\n{format_instructions}\\n\\nTexto: {text}\",\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": book_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = book_prompt | llm | book_parser\n",
    "\n",
    "text = \"O livro '1984' foi escrito por George Orwell e publicado em 1949.\"\n",
    "book_info = chain.invoke({\"text\": text})\n",
    "print(book_info)\n",
    "print(book_info.model_dump())  # Convertendo para dict usando model_dump()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1df1c1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TÃ­tulo: Dom Casmurro\n",
      "Autor: Machado de Assis\n",
      "Ano: 1899\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Extraia o tÃ­tulo, autor e ano do seguinte texto:\\n\\nTexto: {text}\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "text = \"O livro 'Dom Casmurro' foi escrito por Machado de Assis em 1899.\"\n",
    "response = chain.invoke({\"text\": text})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "195a71e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Dom Casmurro', 'author': 'Machado de Assis', 'year': 1899}\n"
     ]
    }
   ],
   "source": [
    "class Book(BaseModel):\n",
    "    title: str\n",
    "    author: str\n",
    "    year: int\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Book)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Extraia os dados do livro e retorne em JSON no seguinte formato:\\n{format_instructions}\\n\\nTexto: {text}\",\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "book = chain.invoke({\"text\": text})\n",
    "print(book.model_dump())  # Convertendo para dict usando model_dump()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec747b1a",
   "metadata": {},
   "source": [
    "| Parser                 | Quando usar                                       |\n",
    "| ---------------------- | ------------------------------------------------- |\n",
    "| `StrOutputParser`      | Para capturar texto puro, rascunhos, debugging    |\n",
    "| `PydanticOutputParser` | Quando vocÃª quer estrutura, validaÃ§Ã£o e seguranÃ§a |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e69ad05",
   "metadata": {},
   "source": [
    "* chain of thought\n",
    "* react\n",
    "* tree of thought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce0abb",
   "metadata": {},
   "source": [
    "## Agents Design\n",
    "___\n",
    "\n",
    "1. Defina os objetivos de forma clara; - `foque na tarefa e nÃ£o no agente`\n",
    "2. Defina como o agente irÃ¡ coletar ou receber as informaÃ§Ãµes;\n",
    "3. Escolhe o agente a outros sistemas;\n",
    "4. Integre o agente a outros sistemas;\n",
    "5. Monitore e otimize;\n",
    "6. Garanta a seguranÃ§a e privacidade dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636427fa",
   "metadata": {},
   "source": [
    "### Design de um agente - LANGCHAIN\n",
    "___\n",
    "\n",
    "* Modelo para tomada de decisÃ£o;\n",
    "* Escolha as ferramentas;\n",
    "* Escolha um sistema de memÃ³ria embutido;\n",
    "* Guardrails para limitar comportamento indesejado;\n",
    "* Sistema de logging e observabilidade;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd65dd1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83d6ae18",
   "metadata": {},
   "source": [
    "### Streaming in LangChain\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "783242a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Redes neurais artificiais sÃ£o modelos computacionais inspirados no funcionamento do cÃ©rebro humano, que sÃ£o utilizados para realizar tarefas de aprendizado de mÃ¡quina. Elas sÃ£o compostas por neurÃ´nios artificiais interconectados em camadas, onde cada neurÃ´nio recebe entradas, realiza um cÃ¡lculo e produz uma saÃ­da.\\n\\nUm exemplo de rede neural artificial Ã© a rede neural convolucional (CNN), amplamente utilizada em tarefas de visÃ£o computacional, como reconhecimento de imagens. Neste tipo de rede, as camadas convolucionais sÃ£o responsÃ¡veis por extrair caracterÃ­sticas das imagens, enquanto as camadas de pooling reduzem a dimensionalidade dos dados. Por fim, as camadas totalmente conectadas realizam a classificaÃ§Ã£o final da imagem.\\n\\nEm resumo, as redes neurais artificiais sÃ£o poderosas ferramentas de aprendizado de mÃ¡quina que podem ser aplicadas em uma variedade de problemas, desde reconhecimento de padrÃµes atÃ© processamento de linguagem natural.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"VocÃª Ã© um assistente especialista em IA.\"),\n",
    "    (\"human\", \"{pergunta}\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({\"pergunta\": \"Explique o que sÃ£o redes neurais artificiais com um exemplo.\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f268ad2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redes neurais artificiais sÃ£o sistemas computacionais inspirados no funcionamento do cÃ©rebro humano. Elas sÃ£o formadas por unidades chamadas **neurÃ´nios artificiais** organizados em camadas. Esses neurÃ´nios recebem informaÃ§Ãµes, processam e transmitem para outros neurÃ´nios, permitindo que a rede â€œaprendaâ€ padrÃµes em dados.\n",
      "\n",
      "**Como funciona, de forma simples:**\n",
      "- A camada de entrada recebe os dados (por exemplo, uma imagem).\n",
      "- Os neurÃ´nios processam esses dados realizando operaÃ§Ãµes matemÃ¡ticas (como somas e ativaÃ§Ãµes).\n",
      "- A informaÃ§Ã£o passa por camadas intermediÃ¡rias (camadas ocultas), onde o conhecimento Ã© refinado.\n",
      "- Por fim, a camada de saÃ­da fornece o resultado (por exemplo, identificar se hÃ¡ um gato na imagem).\n",
      "\n",
      "**Exemplo prÃ¡tico: Reconhecimento de dÃ­gitos escritos Ã  mÃ£o**\n",
      "Imagine que vocÃª quer que um computador reconheÃ§a nÃºmeros de 0 a 9 escritos Ã  mÃ£o.\n",
      "\n",
      "1. VocÃª fornece imagens de dÃ­gitos para a rede neural.\n",
      "2. Cada imagem Ã© transformada em uma lista de nÃºmeros (os tons de cinza dos pixels).\n",
      "3. A rede neural analisa os padrÃµes nesses nÃºmeros e aprende a distinguir cada dÃ­gito durante o treinamento.\n",
      "4. ApÃ³s o treinamento, ao receber uma nova imagem de um dÃ­gito, a rede consegue identificar (com boa precisÃ£o) qual nÃºmero foi escrito.\n",
      "\n",
      "Portanto, as redes neurais artificiais sÃ£o modelos que aprendem com exemplos e sÃ£o amplamente usadas em tarefas como reconhecimento de voz, traduÃ§Ãµes automÃ¡ticas, diagnÃ³sticos mÃ©dicos e muito mais.\n",
      "\n",
      "\n",
      "['', 'Red', 'es', ' neur', 'ais', ' artific', 'iais', ' sÃ£o', ' sistemas', ' comput', 'acionais', ' inspir', 'ados', ' no', ' funcionamento', ' do', ' cÃ©rebro', ' humano', '.', ' Elas', ' sÃ£o', ' form', 'adas', ' por', ' unidades', ' chamadas', ' **', 'neur', 'Ã´', 'nios', ' artific', 'iais', '**', ' organiz', 'ados', ' em', ' cam', 'adas', '.', ' Esses', ' neur', 'Ã´', 'nios', ' recebem', ' informaÃ§Ãµes', ',', ' process', 'am', ' e', ' transm', 'item', ' para', ' outros', ' neur', 'Ã´', 'nios', ',', ' permitindo', ' que', ' a', ' rede', ' â€œ', 'apr', 'enda', 'â€', ' padrÃµes', ' em', ' dados', '.\\n\\n', '**', 'Como', ' funciona', ',', ' de', ' forma', ' simples', ':', '**\\n', '-', ' A', ' camada', ' de', ' entrada', ' recebe', ' os', ' dados', ' (', 'por', ' exemplo', ',', ' uma', ' imagem', ').\\n', '-', ' Os', ' neur', 'Ã´', 'nios', ' process', 'am', ' esses', ' dados', ' realizando', ' operaÃ§Ãµes', ' matem', 'Ã¡ticas', ' (', 'como', ' som', 'as', ' e', ' ativa', 'Ã§Ãµes', ').\\n', '-', ' A', ' informaÃ§Ã£o', ' passa', ' por', ' cam', 'adas', ' intermedi', 'Ã¡rias', ' (', 'cam', 'adas', ' ocult', 'as', '),', ' onde', ' o', ' conhecimento', ' Ã©', ' refin', 'ado', '.\\n', '-', ' Por', ' fim', ',', ' a', ' camada', ' de', ' saÃ­da', ' fornece', ' o', ' resultado', ' (', 'por', ' exemplo', ',', ' identificar', ' se', ' hÃ¡', ' um', ' gato', ' na', ' imagem', ').\\n\\n', '**', 'Ex', 'emplo', ' pr', 'Ã¡tico', ':', ' Recon', 'hecimento', ' de', ' dÃ­', 'g', 'itos', ' escritos', ' Ã ', ' mÃ£o', '**\\n', 'Imagine', ' que', ' vocÃª', ' quer', ' que', ' um', ' computador', ' reconhe', 'Ã§a', ' nÃºmeros', ' de', ' ', '0', ' a', ' ', '9', ' escritos', ' Ã ', ' mÃ£o', '.\\n\\n', '1', '.', ' VocÃª', ' fornece', ' imagens', ' de', ' dÃ­', 'g', 'itos', ' para', ' a', ' rede', ' neural', '.\\n', '2', '.', ' Cada', ' imagem', ' Ã©', ' transform', 'ada', ' em', ' uma', ' lista', ' de', ' nÃºmeros', ' (', 'os', ' tons', ' de', ' cin', 'za', ' dos', ' pixels', ').\\n', '3', '.', ' A', ' rede', ' neural', ' anal', 'isa', ' os', ' padrÃµes', ' nesses', ' nÃºmeros', ' e', ' aprende', ' a', ' distinguir', ' cada', ' dÃ­', 'g', 'ito', ' durante', ' o', ' treinamento', '.\\n', '4', '.', ' ApÃ³s', ' o', ' treinamento', ',', ' ao', ' receber', ' uma', ' nova', ' imagem', ' de', ' um', ' dÃ­', 'g', 'ito', ',', ' a', ' rede', ' consegue', ' identificar', ' (', 'com', ' boa', ' precisÃ£o', ')', ' qual', ' nÃºmero', ' foi', ' escrito', '.\\n\\n', 'Port', 'anto', ',', ' as', ' redes', ' neur', 'ais', ' artific', 'iais', ' sÃ£o', ' modelos', ' que', ' aprend', 'em', ' com', ' exemplos', ' e', ' sÃ£o', ' ampl', 'amente', ' usadas', ' em', ' tarefas', ' como', ' reconhecimento', ' de', ' voz', ',', ' tradu', 'Ã§Ãµes', ' autom', 'Ã¡ticas', ',', ' diagn', 'Ã³st', 'icos', ' mÃ©dicos', ' e', ' muito', ' mais', '.', '']\n",
      "Redes neurais artificiais sÃ£o sistemas computacionais inspirados no funcionamento do cÃ©rebro humano. Elas sÃ£o formadas por unidades chamadas **neurÃ´nios artificiais** organizados em camadas. Esses neurÃ´nios recebem informaÃ§Ãµes, processam e transmitem para outros neurÃ´nios, permitindo que a rede â€œaprendaâ€ padrÃµes em dados.\n",
      "\n",
      "**Como funciona, de forma simples:**\n",
      "- A camada de entrada recebe os dados (por exemplo, uma imagem).\n",
      "- Os neurÃ´nios processam esses dados realizando operaÃ§Ãµes matemÃ¡ticas (como somas e ativaÃ§Ãµes).\n",
      "- A informaÃ§Ã£o passa por camadas intermediÃ¡rias (camadas ocultas), onde o conhecimento Ã© refinado.\n",
      "- Por fim, a camada de saÃ­da fornece o resultado (por exemplo, identificar se hÃ¡ um gato na imagem).\n",
      "\n",
      "**Exemplo prÃ¡tico: Reconhecimento de dÃ­gitos escritos Ã  mÃ£o**\n",
      "Imagine que vocÃª quer que um computador reconheÃ§a nÃºmeros de 0 a 9 escritos Ã  mÃ£o.\n",
      "\n",
      "1. VocÃª fornece imagens de dÃ­gitos para a rede neural.\n",
      "2. Cada imagem Ã© transformada em uma lista de nÃºmeros (os tons de cinza dos pixels).\n",
      "3. A rede neural analisa os padrÃµes nesses nÃºmeros e aprende a distinguir cada dÃ­gito durante o treinamento.\n",
      "4. ApÃ³s o treinamento, ao receber uma nova imagem de um dÃ­gito, a rede consegue identificar (com boa precisÃ£o) qual nÃºmero foi escrito.\n",
      "\n",
      "Portanto, as redes neurais artificiais sÃ£o modelos que aprendem com exemplos e sÃ£o amplamente usadas em tarefas como reconhecimento de voz, traduÃ§Ãµes automÃ¡ticas, diagnÃ³sticos mÃ©dicos e muito mais.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\", streaming=True)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"VocÃª Ã© um assistente especialista em IA.\"),\n",
    "    (\"human\", \"{pergunta}\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chunks =[]\n",
    "for chunk in chain.stream({\"pergunta\": \"Explique o que sÃ£o redes neurais artificiais com um exemplo.\"}):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(chunks)\n",
    "final_response = \"\".join(chunks)\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afda1099",
   "metadata": {},
   "source": [
    "Entender **quando usar streaming** Ã© essencial para desenvolver aplicaÃ§Ãµes mais eficientes, responsivas e amigÃ¡veis.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  O que o streaming resolve?\n",
    "\n",
    "### âœ… **Problema sem streaming:**\n",
    "\n",
    "* VocÃª envia o prompt para o modelo.\n",
    "* O sistema **espera a resposta inteira** do LLM para depois exibir ou processar.\n",
    "* Resultado: tempo de espera maior, experiÃªncia menos interativa.\n",
    "\n",
    "### âœ… **Com streaming:**\n",
    "\n",
    "* A resposta do modelo comeÃ§a a ser **emitida token por token** (palavra por palavra).\n",
    "* VocÃª pode **mostrar, processar ou agir** conforme cada parte chega.\n",
    "* Resultado: resposta **mais rÃ¡pida e fluida**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¦ Casos tÃ­picos onde **streaming Ã© necessÃ¡rio (ou recomendado)**\n",
    "\n",
    "### 1. **Chatbots e assistentes virtuais**\n",
    "\n",
    "ğŸ“² **Exemplo**: Aplicativos como ChatGPT, interfaces com assistentes em tempo real.\n",
    "\n",
    "* Exibir a resposta conforme o modelo \"pensa\".\n",
    "* Torna a conversa mais **natural** e **responsiva**.\n",
    "\n",
    "âœ… Recomendado: melhora **UX** (experiÃªncia do usuÃ¡rio).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Interfaces web ou mobile com atualizaÃ§Ã£o ao vivo**\n",
    "\n",
    "ğŸ–¥ï¸ **Exemplo**: Interfaces React/Vue exibindo respostas em tempo real.\n",
    "\n",
    "* Enquanto o modelo responde, o conteÃºdo jÃ¡ aparece no componente.\n",
    "* Pode ser integrado com animaÃ§Ãµes ou placeholders.\n",
    "\n",
    "âœ… Torna o app mais fluido, parece que o modelo estÃ¡ \"digitando\".\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **ExperiÃªncias de voz (text-to-speech)**\n",
    "\n",
    "ğŸ™ï¸ **Exemplo**: um sistema que \"fala\" a resposta gerada.\n",
    "\n",
    "* Assim que os primeiros tokens chegam, eles jÃ¡ podem ser passados para um sistema de voz.\n",
    "* Isso reduz o tempo entre a pergunta e a resposta falada.\n",
    "\n",
    "âœ… NecessÃ¡rio para **naturalidade** em interaÃ§Ãµes por voz.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Carregamento progressivo em terminais e scripts CLI**\n",
    "\n",
    "ğŸ§‘â€ğŸ’» **Exemplo**: Chatbots de terminal, interfaces de linha de comando com LLM.\n",
    "\n",
    "* UsuÃ¡rio vÃª o conteÃºdo aparecendo em tempo real (efeito \"digitando...\").\n",
    "* Boa experiÃªncia em scripts interativos.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Tarefas longas ou respostas extensas**\n",
    "\n",
    "ğŸ“š **Exemplo**: GeraÃ§Ã£o de textos longos, cÃ³digos, artigos, anÃ¡lises complexas.\n",
    "\n",
    "* O modelo pode demorar vÃ¡rios segundos para finalizar.\n",
    "* Com streaming, o usuÃ¡rio nÃ£o fica esperando no escuro.\n",
    "\n",
    "âœ… Ideal para evitar **tempo de espera** percebido.\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Quando **nÃ£o** Ã© necessÃ¡rio usar streaming?\n",
    "\n",
    "| CenÃ¡rio                                                          | Use streaming?  |\n",
    "| ---------------------------------------------------------------- | --------------- |\n",
    "| Respostas muito curtas                                           | âŒ DesnecessÃ¡rio |\n",
    "| Processamento em batch (vÃ¡rias chamadas de uma vez)              | âŒ               |\n",
    "| Uso em pipelines de backend com foco em velocidade total, nÃ£o UX | âŒ               |\n",
    "| Quando vai salvar a resposta direto no banco ou arquivo          | âŒ               |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª Dica de ouro\n",
    "\n",
    "Mesmo quando vocÃª **nÃ£o precisa exibir streaming**, pode usÃ¡-lo para:\n",
    "\n",
    "* Processar tokens Ã  medida que chegam.\n",
    "* Implementar validaÃ§Ãµes ou cancelamentos precoces.\n",
    "* Controlar o comportamento do modelo de forma mais granular.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "da6e61bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content='Good' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content='bye' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' If' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' you' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' have' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' any' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' more' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' questions' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' in' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' the' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' future' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=',' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' feel' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' free' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' to' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' ask' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' Take' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content=' care' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'} id='run--8b916096-d536-4fd8-b8da-4f8621eea056'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "completion = model.invoke(\"Hi there!\")\n",
    "# Hi!\n",
    "\n",
    "completions = model.batch([\"Hi there!\", \"Bye!\"])\n",
    "# ['Hi!', 'See you!']\n",
    "\n",
    "for token in model.stream(\"Bye!\"):\n",
    "    print(token)\n",
    "    # Good\n",
    "    # bye\n",
    "    # !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0c5f4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class PydanticUserInfo(BaseModel):\n",
    "    \"\"\"User's info.\"\"\"\n",
    "    name: Annotated[str, Field(description=\"User's name. Defaults to ''\", default=None)]\n",
    "    country: Annotated[str, Field(description=\"Where the user lives. Defaults to ''\", default=None, )]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b191e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_structure = llm.with_structured_output(PydanticUserInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "21c035c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_output = llm_with_structure.invoke(\"The sky is blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c0632d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PydanticUserInfo(name='', country='')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "87967a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(structured_output.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b03da435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado:\n",
      "Nome: Joana\n",
      "Idade: 30\n",
      "Interesses: ['leitura', 'yoga', 'viagens']\n",
      "\n",
      "Tipo do objeto: <class '__main__.Pessoa'>\n",
      "Objeto completo: nome='Joana' idade=30 interesses=['leitura', 'yoga', 'viagens']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define a estrutura esperada\n",
    "class Pessoa(BaseModel):\n",
    "    nome: str\n",
    "    idade: int\n",
    "    interesses: list[str]\n",
    "\n",
    "# Cria o parser usando a classe Pessoa\n",
    "parser = PydanticOutputParser(pydantic_object=Pessoa)\n",
    "\n",
    "# Cria o template do prompt incluindo as instruÃ§Ãµes de formato\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Extraia nome, idade e interesses do seguinte texto:\\nTexto: {texto}\\n{format_instructions}\",\n",
    "    input_variables=[\"texto\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Inicializa o modelo de linguagem\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "# Cria a cadeia (chain) de processamento\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Executa a extraÃ§Ã£o\n",
    "resposta = chain.invoke({\"texto\": \"Joana tem 30 anos e gosta de leitura, yoga e viagens.\"})\n",
    "\n",
    "print(\"Resultado:\")\n",
    "print(f\"Nome: {resposta.nome}\")\n",
    "print(f\"Idade: {resposta.idade}\")\n",
    "print(f\"Interesses: {resposta.interesses}\")\n",
    "print(f\"\\nTipo do objeto: {type(resposta)}\")\n",
    "print(f\"Objeto completo: {resposta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d0be1d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESULTADOS DA EXTRAÃ‡ÃƒO ===\n",
      "\n",
      "TESTE 1:\n",
      "Texto original: Joana tem 30 anos e gosta de leitura, yoga e viagens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520984/179862137.py:33: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  @validator('nome')\n",
      "/tmp/ipykernel_520984/179862137.py:41: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  @validator('interesses')\n",
      "/home/fabiolima/Workdir/langchain/study_langchain/.venv/lib/python3.13/site-packages/pydantic/_internal/_config.py:373: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ExtraÃ§Ã£o bem-sucedida:\n",
      "   Nome: Joana\n",
      "   Idade: 30\n",
      "   Interesses: ['leitura', 'yoga', 'viagens']\n",
      "   Objeto validado: nome='Joana' idade=30 interesses=['leitura', 'yoga', 'viagens']\n",
      "--------------------------------------------------\n",
      "TESTE 2:\n",
      "Texto original: Pedro Ã© um jovem de 25 anos apaixonado por futebol, culinÃ¡ria e mÃºsica eletrÃ´nica.\n",
      "âœ… ExtraÃ§Ã£o bem-sucedida:\n",
      "   Nome: Pedro\n",
      "   Idade: 25\n",
      "   Interesses: ['futebol', 'culinÃ¡ria', 'mÃºsica eletrÃ´nica']\n",
      "   Objeto validado: nome='Pedro' idade=25 interesses=['futebol', 'culinÃ¡ria', 'mÃºsica eletrÃ´nica']\n",
      "--------------------------------------------------\n",
      "TESTE 3:\n",
      "Texto original: Maria, 45 anos, adora jardinagem e pintura em aquarela.\n",
      "âœ… ExtraÃ§Ã£o bem-sucedida:\n",
      "   Nome: Maria\n",
      "   Idade: 45\n",
      "   Interesses: ['jardinagem', 'pintura em aquarela']\n",
      "   Objeto validado: nome='Maria' idade=45 interesses=['jardinagem', 'pintura em aquarela']\n",
      "--------------------------------------------------\n",
      "\n",
      "=== SCHEMA JSON GERADO ===\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"Modelo para representar informaÃ§Ãµes de uma pessoa extraÃ­das de texto.\", \"properties\": {\"nome\": {\"description\": \"Nome completo da pessoa\", \"example\": \"Maria Silva\", \"maxLength\": 100, \"minLength\": 2, \"title\": \"Nome\", \"type\": \"string\"}, \"idade\": {\"description\": \"Idade da pessoa em anos\", \"example\": 30, \"maximum\": 150, \"minimum\": 0, \"title\": \"Idade\", \"type\": \"integer\"}, \"interesses\": {\"description\": \"Lista de hobbies, atividades ou interesses da pessoa\", \"example\": [\"leitura\", \"yoga\", \"viagens\"], \"items\": {\"type\": \"string\"}, \"maxItems\": 10, \"minItems\": 1, \"title\": \"Interesses\", \"type\": \"array\"}}, \"required\": [\"nome\", \"idade\", \"interesses\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define a estrutura esperada com validaÃ§Ãµes e descriÃ§Ãµes completas\n",
    "class Pessoa(BaseModel):\n",
    "    \"\"\"Modelo para representar informaÃ§Ãµes de uma pessoa extraÃ­das de texto.\"\"\"\n",
    "    \n",
    "    nome: str = Field(\n",
    "        description=\"Nome completo da pessoa\",\n",
    "        min_length=2,\n",
    "        max_length=100,\n",
    "        example=\"Maria Silva\"\n",
    "    )\n",
    "    \n",
    "    idade: int = Field(\n",
    "        description=\"Idade da pessoa em anos\",\n",
    "        ge=0,  # greater or equal (>= 0)\n",
    "        le=150,  # less or equal (<= 150)\n",
    "        example=30\n",
    "    )\n",
    "    \n",
    "    interesses: List[str] = Field(\n",
    "        description=\"Lista de hobbies, atividades ou interesses da pessoa\",\n",
    "        min_items=1,\n",
    "        max_items=10,\n",
    "        example=[\"leitura\", \"yoga\", \"viagens\"]\n",
    "    )\n",
    "    \n",
    "    # Validador customizado para normalizar o nome\n",
    "    @validator('nome')\n",
    "    def validar_nome(cls, v):\n",
    "        if not v.strip():\n",
    "            raise ValueError('Nome nÃ£o pode estar vazio')\n",
    "        # Capitaliza cada palavra do nome\n",
    "        return ' '.join(word.capitalize() for word in v.strip().split())\n",
    "    \n",
    "    # Validador para normalizar interesses\n",
    "    @validator('interesses')\n",
    "    def validar_interesses(cls, v):\n",
    "        if not v:\n",
    "            raise ValueError('Deve haver pelo menos um interesse')\n",
    "        # Remove duplicatas e normaliza para minÃºsculas\n",
    "        interesses_limpos = []\n",
    "        for interesse in v:\n",
    "            interesse_limpo = interesse.strip().lower()\n",
    "            if interesse_limpo and interesse_limpo not in interesses_limpos:\n",
    "                interesses_limpos.append(interesse_limpo)\n",
    "        return interesses_limpos\n",
    "    \n",
    "    class Config:\n",
    "        # ConfiguraÃ§Ãµes do modelo\n",
    "        str_strip_whitespace = True  # Remove espaÃ§os em branco automaticamente\n",
    "        validate_assignment = True   # Valida tambÃ©m durante atribuiÃ§Ãµes\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"nome\": \"Ana Costa\",\n",
    "                \"idade\": 28,\n",
    "                \"interesses\": [\"culinÃ¡ria\", \"fotografia\", \"nataÃ§Ã£o\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Cria o parser usando a classe Pessoa\n",
    "parser = PydanticOutputParser(pydantic_object=Pessoa)\n",
    "\n",
    "# Template mais robusto\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    VocÃª Ã© um especialista em extraÃ§Ã£o de informaÃ§Ãµes pessoais de textos.\n",
    "    \n",
    "    Extraia nome, idade e interesses do seguinte texto de forma precisa:\n",
    "    \n",
    "    TEXTO: {texto}\n",
    "    \n",
    "    INSTRUÃ‡Ã•ES IMPORTANTES:\n",
    "    - Se a idade nÃ£o estiver explÃ­cita, tente inferir com base no contexto\n",
    "    - Para interesses, inclua hobbies, atividades, gostos pessoais mencionados\n",
    "    - Seja preciso e nÃ£o invente informaÃ§Ãµes que nÃ£o estÃ£o no texto\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=[\"texto\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Inicializa o modelo de linguagem\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Cria a cadeia (chain) de processamento\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Teste com diferentes exemplos\n",
    "textos_teste = [\n",
    "    \"Joana tem 30 anos e gosta de leitura, yoga e viagens.\",\n",
    "    \"Pedro Ã© um jovem de 25 anos apaixonado por futebol, culinÃ¡ria e mÃºsica eletrÃ´nica.\",\n",
    "    \"Maria, 45 anos, adora jardinagem e pintura em aquarela.\"\n",
    "]\n",
    "\n",
    "print(\"=== RESULTADOS DA EXTRAÃ‡ÃƒO ===\\n\")\n",
    "\n",
    "for i, texto in enumerate(textos_teste, 1):\n",
    "    print(f\"TESTE {i}:\")\n",
    "    print(f\"Texto original: {texto}\")\n",
    "    \n",
    "    try:\n",
    "        resposta = chain.invoke({\"texto\": texto})\n",
    "        print(f\"âœ… ExtraÃ§Ã£o bem-sucedida:\")\n",
    "        print(f\"   Nome: {resposta.nome}\")\n",
    "        print(f\"   Idade: {resposta.idade}\")\n",
    "        print(f\"   Interesses: {resposta.interesses}\")\n",
    "        print(f\"   Objeto validado: {resposta}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro na extraÃ§Ã£o: {e}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Exemplo de como acessar o schema JSON gerado\n",
    "print(\"\\n=== SCHEMA JSON GERADO ===\")\n",
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf588a7f",
   "metadata": {},
   "source": [
    "## LCEL - LangChain Expression Language \n",
    "___\n",
    "\n",
    "`A declarative way to compose AI workflows`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d6f621",
   "metadata": {},
   "source": [
    "## LangGraph\n",
    "\n",
    "A framework for `agentic workgflows` with complex state management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bbb011",
   "metadata": {},
   "source": [
    "Vamos explorar a ideia da **interface `Runnable` no LangChain**, que Ã© uma das peÃ§as centrais da **LangChain Expression Language (LCEL)**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  O que Ã© `Runnable`?\n",
    "\n",
    "Em termos simples:\n",
    "\n",
    "> **`Runnable` Ã© uma interface padrÃ£o que define como um componente da LangChain pode ser executado.**\n",
    "\n",
    "Tudo o que pode ser **invocado, encadeado ou executado** â€” como um modelo, um prompt, um parser â€” implementa a interface `Runnable`.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¦ Por que isso Ã© importante?\n",
    "\n",
    "A interface `Runnable` Ã© o que permite vocÃª fazer coisas como:\n",
    "\n",
    "```python\n",
    "prompt | llm | parser\n",
    "```\n",
    "\n",
    "Cada componente aqui (`prompt`, `llm`, `parser`) Ã© um **Runnable** â€” eles sabem **como receber entrada e produzir saÃ­da** de forma padronizada.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© MÃ©todos principais de `Runnable`\n",
    "\n",
    "| MÃ©todo           | O que faz                                                   |\n",
    "| ---------------- | ----------------------------------------------------------- |\n",
    "| `invoke(input)`  | Executa uma Ãºnica chamada sincrÃ´nica                        |\n",
    "| `stream(input)`  | Executa com **streaming**, Ãºtil para chat/respostas ao vivo |\n",
    "| `batch(inputs)`  | Executa vÃ¡rias entradas de uma vez                          |\n",
    "| `ainvoke(input)` | VersÃ£o assÃ­ncrona de `invoke()`                             |\n",
    "| `astream(input)` | VersÃ£o assÃ­ncrona de `stream()`                             |\n",
    "| `abatch(inputs)` | VersÃ£o assÃ­ncrona de `batch()`                              |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Exemplo didÃ¡tico\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Cria um componente que dobra o valor de entrada\n",
    "dobro = RunnableLambda(lambda x: x * 2)\n",
    "\n",
    "print(dobro.invoke(10))  # â†’ 20\n",
    "```\n",
    "\n",
    "Esse `RunnableLambda` Ã© uma funÃ§Ã£o simples transformada num `Runnable`.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Por que isso Ã© poderoso?\n",
    "\n",
    "Porque vocÃª pode **compor componentes diferentes** como se fossem \"filtros de dados\" â€” tipo Unix Pipes (`|`).\n",
    "\n",
    "Por exemplo:\n",
    "\n",
    "```python\n",
    "chain = prompt | llm | parser\n",
    "```\n",
    "\n",
    "Internamente, isso Ã© equivalente a:\n",
    "\n",
    "```python\n",
    "output_prompt = prompt.invoke(inputs)\n",
    "output_llm = llm.invoke(output_prompt)\n",
    "output_final = parser.invoke(output_llm)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¤– Todo LLM, prompt, parser ou chain em LCEL Ã© um `Runnable`\n",
    "\n",
    "### Exemplo:\n",
    "\n",
    "* `ChatPromptTemplate` â†’ `Runnable`: recebe `dict` â†’ gera prompt formatado.\n",
    "* `ChatOpenAI` â†’ `Runnable`: recebe prompt â†’ gera resposta.\n",
    "* `StrOutputParser` â†’ `Runnable`: recebe resposta â†’ extrai string.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ Curiosidade: vocÃª pode criar suas prÃ³prias chains\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "formatador = RunnableLambda(lambda x: f\"OlÃ¡, {x}!\")\n",
    "maiusculo = RunnableLambda(lambda x: x.upper())\n",
    "\n",
    "saudacao_chain = formatador | maiusculo\n",
    "\n",
    "print(saudacao_chain.invoke(\"fabio\"))  # â†’ OLÃ, FABIO!\n",
    "```\n",
    "\n",
    "Simples, poderoso e reutilizÃ¡vel.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Resumo final\n",
    "\n",
    "A interface `Runnable` Ã©:\n",
    "\n",
    "âœ… O **coraÃ§Ã£o do encadeamento** em LangChain\n",
    "âœ… Uma maneira de garantir que **todos os componentes se comportem de forma previsÃ­vel**\n",
    "âœ… Uma forma de criar **pipelines reutilizÃ¡veis e composÃ¡veis**\n",
    "âœ… O que permite usar mÃ©todos como `.invoke()`, `.stream()`, `.batch()` em qualquer etapa da sua chain\n",
    "\n",
    "---\n",
    "\n",
    "Se quiser, posso montar um grÃ¡fico tipo \"pipeline visual\" explicando como os Runnables se conectam.\n",
    "\n",
    "Ou gerar um notebook com exemplos didÃ¡ticos de `RunnableLambda`, `RunnableMap`, `Prompt | LLM | Parser`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b1b8490d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Ã© uma linguagem de programaÃ§Ã£o de alto nÃ­vel, muito popular por sua simplicidade e legibilidade. Ela foi criada por Guido van Rossum e lanÃ§ada pela primeira vez em 1991. Aqui estÃ£o alguns pontos principais sobre Python:\n",
      "\n",
      "1. **FÃ¡cil de Aprender**: A sintaxe do Python Ã© clara e intuitiva, o que a torna uma Ã³tima escolha para iniciantes.\n",
      "\n",
      "2. **VersÃ¡til**: Python pode ser usado para uma ampla variedade de aplicaÃ§Ãµes, incluindo desenvolvimento web, anÃ¡lise de dados, inteligÃªncia artificial, automaÃ§Ã£o de tarefas, e muito mais.\n",
      "\n",
      "3. **Bibliotecas e Frameworks**: Python possui uma vasta coleÃ§Ã£o de bibliotecas e frameworks que facilitam o desenvolvimento. Por exemplo, o Django Ã© usado para desenvolvimento web, enquanto o Pandas Ã© popular para anÃ¡lise de dados.\n",
      "\n",
      "4. **Comunidade Ativa**: Python tem uma grande comunidade de desenvolvedores, o que significa que hÃ¡ muitos recursos, tutoriais e suporte disponÃ­veis.\n",
      "\n",
      "5. **Multiplataforma**: Python pode ser executado em diferentes sistemas operacionais, como Windows, macOS e Linux.\n",
      "\n",
      "Em resumo, Python Ã© uma linguagem poderosa e flexÃ­vel, ideal tanto para iniciantes quanto para programadores experientes.\n",
      "Machine learning, ou aprendizado de mÃ¡quina, Ã© uma Ã¡rea da inteligÃªncia artificial que permite que computadores aprendam a partir de dados e melhorem seu desempenho em tarefas especÃ­ficas sem serem programados explicitamente para isso. Em vez de seguir instruÃ§Ãµes fixas, os algoritmos de machine learning analisam padrÃµes nos dados e fazem previsÃµes ou tomam decisÃµes com base nessas anÃ¡lises.\n",
      "\n",
      "Por exemplo, um algoritmo de machine learning pode ser treinado com um conjunto de dados de e-mails para identificar quais sÃ£o spam. Ele aprende a reconhecer caracterÃ­sticas comuns nos e-mails indesejados e, assim, consegue classificar novos e-mails como spam ou nÃ£o.\n",
      "\n",
      "Em resumo, machine learning Ã© sobre ensinar mÃ¡quinas a aprender com a experiÃªncia, usando dados para melhorar suas habilidades ao longo do tempo.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Componentes base\n",
    "prompt = ChatPromptTemplate.from_template(\"Me dÃª uma explicaÃ§Ã£o simples sobre {topic}\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # Use o modelo mais novo disponÃ­vel\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Encadeamento moderno com LCEL (LangChain Expression Language)\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Executa batch de mÃºltiplas entradas\n",
    "result = chain.batch([\n",
    "    {'topic': 'python'},\n",
    "    {'topic': 'machine learning'}\n",
    "])\n",
    "\n",
    "for item in result:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e880c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplos PrÃ¡ticos: Imperative Composition vs LCEL\n",
    "# Certifique-se de ter instalado: pip install langchain langchain-openai\n",
    "\n",
    "import os\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Configure sua API key da OpenAI\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sua-api-key-aqui\"\n",
    "\n",
    "# ============================================================================\n",
    "# EXEMPLO 1: TRADUTOR SIMPLES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=== EXEMPLO 1: TRADUTOR SIMPLES ===\\n\")\n",
    "\n",
    "# Componentes base\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "translator_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"VocÃª Ã© um tradutor especializado. Traduza o texto para {idioma}.\"),\n",
    "    (\"human\", \"{texto}\")\n",
    "])\n",
    "\n",
    "# VERSÃƒO IMPERATIVE COM @chain\n",
    "@chain\n",
    "def tradutor_imperativo(values):\n",
    "    \"\"\"Tradutor usando composiÃ§Ã£o imperativa\"\"\"\n",
    "    prompt = translator_template.invoke(values)\n",
    "    response = model.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "# VERSÃƒO LCEL\n",
    "tradutor_lcel = translator_template | model | StrOutputParser()\n",
    "\n",
    "# TESTANDO AMBOS\n",
    "texto_teste = \"Hello, how are you today?\"\n",
    "idioma_teste = \"portuguÃªs\"\n",
    "\n",
    "print(\"ğŸ”§ IMPERATIVO:\")\n",
    "resultado_imp = tradutor_imperativo.invoke({\n",
    "    \"texto\": texto_teste, \n",
    "    \"idioma\": idioma_teste\n",
    "})\n",
    "print(f\"Resultado: {resultado_imp}\\n\")\n",
    "\n",
    "print(\"âš¡ LCEL:\")\n",
    "resultado_lcel = tradutor_lcel.invoke({\n",
    "    \"texto\": texto_teste, \n",
    "    \"idioma\": idioma_teste\n",
    "})\n",
    "print(f\"Resultado: {resultado_lcel}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXEMPLO 2: ANALISADOR DE SENTIMENTOS COM LÃ“GICA CONDICIONAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=== EXEMPLO 2: ANALISADOR DE SENTIMENTOS ===\\n\")\n",
    "\n",
    "sentiment_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Analise o sentimento do texto: POSITIVO, NEGATIVO ou NEUTRO. Responda apenas com uma palavra.\"),\n",
    "    (\"human\", \"{texto}\")\n",
    "])\n",
    "\n",
    "response_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Com base no sentimento {sentimento}, gere uma resposta {tipo_resposta} ao texto original.\"),\n",
    "    (\"human\", \"Texto original: {texto}\")\n",
    "])\n",
    "\n",
    "# VERSÃƒO IMPERATIVE COM LÃ“GICA CONDICIONAL\n",
    "@chain\n",
    "def analisador_imperativo(values):\n",
    "    \"\"\"Analisador com lÃ³gica condicional complexa\"\"\"\n",
    "    texto = values[\"texto\"]\n",
    "    \n",
    "    # Passo 1: Analisar sentimento\n",
    "    sentiment_prompt = sentiment_template.invoke({\"texto\": texto})\n",
    "    sentimento = model.invoke(sentiment_prompt).content.strip().upper()\n",
    "    \n",
    "    # Passo 2: LÃ³gica condicional baseada no sentimento\n",
    "    if sentimento == \"POSITIVO\":\n",
    "        tipo_resposta = \"encorajadora e positiva\"\n",
    "    elif sentimento == \"NEGATIVO\":\n",
    "        tipo_resposta = \"empÃ¡tica e de apoio\"\n",
    "    else:\n",
    "        tipo_resposta = \"equilibrada e informativa\"\n",
    "    \n",
    "    # Passo 3: Gerar resposta personalizada\n",
    "    response_prompt = response_template.invoke({\n",
    "        \"sentimento\": sentimento,\n",
    "        \"tipo_resposta\": tipo_resposta,\n",
    "        \"texto\": texto\n",
    "    })\n",
    "    \n",
    "    resposta_final = model.invoke(response_prompt).content\n",
    "    \n",
    "    # Retorna resultado estruturado\n",
    "    return {\n",
    "        \"texto_original\": texto,\n",
    "        \"sentimento_detectado\": sentimento,\n",
    "        \"tipo_resposta\": tipo_resposta,\n",
    "        \"resposta_gerada\": resposta_final\n",
    "    }\n",
    "\n",
    "# VERSÃƒO LCEL (mais limitada para lÃ³gica condicional)\n",
    "analisador_lcel_simples = sentiment_template | model | StrOutputParser()\n",
    "\n",
    "# TESTANDO O ANALISADOR COMPLEXO\n",
    "textos_teste = [\n",
    "    \"Estou muito feliz com meu novo emprego!\",\n",
    "    \"Estou muito triste e desanimado hoje...\",\n",
    "    \"O clima estÃ¡ nublado hoje.\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ”§ IMPERATIVO (com lÃ³gica condicional):\")\n",
    "for texto in textos_teste:\n",
    "    resultado = analisador_imperativo.invoke({\"texto\": texto})\n",
    "    print(f\"ğŸ“ Texto: {resultado['texto_original']}\")\n",
    "    print(f\"ğŸ˜Š Sentimento: {resultado['sentimento_detectado']}\")\n",
    "    print(f\"ğŸ¯ Tipo de resposta: {resultado['tipo_resposta']}\")\n",
    "    print(f\"ğŸ’¬ Resposta: {resultado['resposta_gerada']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nâš¡ LCEL (apenas anÃ¡lise de sentimento):\")\n",
    "for texto in textos_teste:\n",
    "    sentimento = analisador_lcel_simples.invoke({\"texto\": texto})\n",
    "    print(f\"ğŸ“ '{texto}' â†’ ğŸ˜Š {sentimento}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXEMPLO 3: PIPELINE DE PROCESSAMENTO DE TEXTO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== EXEMPLO 3: PIPELINE DE PROCESSAMENTO ===\\n\")\n",
    "\n",
    "# Templates para o pipeline\n",
    "extractor_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extraia as palavras-chave principais do texto. Liste apenas as palavras separadas por vÃ­rgula.\"),\n",
    "    (\"human\", \"{texto}\")\n",
    "])\n",
    "\n",
    "summarizer_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Resuma o texto em uma frase focando nas palavras-chave: {keywords}\"),\n",
    "    (\"human\", \"{texto}\")\n",
    "])\n",
    "\n",
    "# VERSÃƒO IMPERATIVE\n",
    "@chain\n",
    "def pipeline_imperativo(values):\n",
    "    \"\"\"Pipeline completo de processamento\"\"\"\n",
    "    texto = values[\"texto\"]\n",
    "    \n",
    "    # Etapa 1: Extrair palavras-chave\n",
    "    keywords_prompt = extractor_template.invoke({\"texto\": texto})\n",
    "    keywords = model.invoke(keywords_prompt).content.strip()\n",
    "    \n",
    "    # Etapa 2: Resumir com base nas keywords\n",
    "    summary_prompt = summarizer_template.invoke({\n",
    "        \"texto\": texto,\n",
    "        \"keywords\": keywords\n",
    "    })\n",
    "    summary = model.invoke(summary_prompt).content\n",
    "    \n",
    "    return {\n",
    "        \"texto_original\": texto,\n",
    "        \"palavras_chave\": keywords,\n",
    "        \"resumo\": summary\n",
    "    }\n",
    "\n",
    "# VERSÃƒO LCEL com RunnablePassthrough para dados complexos\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def extrair_keywords(values):\n",
    "    \"\"\"FunÃ§Ã£o auxiliar para extrair keywords\"\"\"\n",
    "    prompt = extractor_template.invoke(values)\n",
    "    keywords = model.invoke(prompt).content.strip()\n",
    "    return {**values, \"keywords\": keywords}\n",
    "\n",
    "pipeline_lcel = (\n",
    "    RunnablePassthrough() \n",
    "    | extrair_keywords \n",
    "    | summarizer_template \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# TESTE DO PIPELINE\n",
    "texto_pipeline = \"\"\"\n",
    "A inteligÃªncia artificial estÃ¡ transformando rapidamente diversos setores da economia. \n",
    "Empresas estÃ£o investindo pesadamente em machine learning e automaÃ§Ã£o para melhorar \n",
    "a eficiÃªncia operacional. No entanto, surgem preocupaÃ§Ãµes sobre o impacto no emprego \n",
    "e a necessidade de regulamentaÃ§Ã£o adequada para garantir o uso Ã©tico da tecnologia.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸ”§ PIPELINE IMPERATIVO:\")\n",
    "resultado_pipeline = pipeline_imperativo.invoke({\"texto\": texto_pipeline})\n",
    "print(f\"ğŸ“ Original: {resultado_pipeline['texto_original'][:100]}...\")\n",
    "print(f\"ğŸ”‘ Keywords: {resultado_pipeline['palavras_chave']}\")\n",
    "print(f\"ğŸ“„ Resumo: {resultado_pipeline['resumo']}\\n\")\n",
    "\n",
    "print(\"âš¡ PIPELINE LCEL:\")\n",
    "resumo_lcel = pipeline_lcel.invoke({\"texto\": texto_pipeline})\n",
    "print(f\"ğŸ“„ Resumo: {resumo_lcel}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXEMPLO 4: CHATBOT COM MEMÃ“RIA (IMPERATIVO)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== EXEMPLO 4: CHATBOT COM HISTÃ“RICO ===\\n\")\n",
    "\n",
    "# Este exemplo mostra como o imperativo Ã© melhor para estados complexos\n",
    "@chain\n",
    "def chatbot_com_memoria(values):\n",
    "    \"\"\"Chatbot que mantÃ©m contexto da conversa\"\"\"\n",
    "    pergunta_atual = values[\"pergunta\"]\n",
    "    historico = values.get(\"historico\", [])\n",
    "    \n",
    "    # ConstrÃ³i o contexto com histÃ³rico\n",
    "    messages = [(\"system\", \"VocÃª Ã© um assistente Ãºtil que lembra da conversa anterior.\")]\n",
    "    \n",
    "    # Adiciona histÃ³rico\n",
    "    for h in historico:\n",
    "        messages.append((\"human\", h[\"pergunta\"]))\n",
    "        messages.append((\"assistant\", h[\"resposta\"]))\n",
    "    \n",
    "    # Adiciona pergunta atual\n",
    "    messages.append((\"human\", pergunta_atual))\n",
    "    \n",
    "    # Cria template dinÃ¢mico\n",
    "    template_dinamico = ChatPromptTemplate.from_messages(messages)\n",
    "    \n",
    "    # Gera resposta\n",
    "    prompt = template_dinamico.invoke({})\n",
    "    resposta = model.invoke(prompt).content\n",
    "    \n",
    "    # Atualiza histÃ³rico\n",
    "    novo_historico = historico + [{\n",
    "        \"pergunta\": pergunta_atual,\n",
    "        \"resposta\": resposta\n",
    "    }]\n",
    "    \n",
    "    return {\n",
    "        \"resposta\": resposta,\n",
    "        \"historico_atualizado\": novo_historico\n",
    "    }\n",
    "\n",
    "# SIMULAÃ‡ÃƒO DE CONVERSA\n",
    "print(\"ğŸ¤– CHATBOT COM MEMÃ“RIA:\")\n",
    "historico_conversa = []\n",
    "\n",
    "perguntas = [\n",
    "    \"Qual Ã© a capital do Brasil?\",\n",
    "    \"Quantos habitantes ela tem aproximadamente?\",\n",
    "    \"Qual Ã© o principal ponto turÃ­stico dessa cidade?\"\n",
    "]\n",
    "\n",
    "for pergunta in perguntas:\n",
    "    resultado = chatbot_com_memoria.invoke({\n",
    "        \"pergunta\": pergunta,\n",
    "        \"historico\": historico_conversa\n",
    "    })\n",
    "    \n",
    "    print(f\"ğŸ‘¤ VocÃª: {pergunta}\")\n",
    "    print(f\"ğŸ¤– Bot: {resultado['resposta']}\")\n",
    "    print()\n",
    "    \n",
    "    historico_conversa = resultado[\"historico_atualizado\"]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ¯ RESUMO DAS DIFERENÃ‡AS:\")\n",
    "print(\"â€¢ IMPERATIVO: Melhor para lÃ³gica complexa, estados, condicionais\")\n",
    "print(\"â€¢ LCEL: Melhor para pipelines lineares, performance, simplicidade\")\n",
    "print(\"â€¢ Use @chain para combinar o melhor dos dois mundos!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06354500",
   "metadata": {},
   "source": [
    "### 1. Conceito Fundamental: Runnable\n",
    "\n",
    "\n",
    "O Runnable Ã© a interface base do LangChain que padroniza como diferentes componentes podem ser executados e combinados. Qualquer objeto que implementa Runnable possui mÃ©todos como .invoke(), .stream(), .batch(), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6d3961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemplo\n",
    "# Todos estes sÃ£o objetos Runnable:\n",
    "translator_template = ChatPromptTemplate.from_messages([...])  # Runnable\n",
    "model = ChatOpenAI(model=\"gpt-4.1\", temperature=0.0)          # Runnable  \n",
    "parser_output = StrOutputParser()                             # Runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668cd79f",
   "metadata": {},
   "source": [
    "### 2. RunnableSequence(operador pipe | )\n",
    "\n",
    "Quando vocÃª usa o `(operador |)` , estÃ¡ criando uma RunnableSequence - uma cadeia onde a saÃ­da de um componente vira entrada do prÃ³ximo.\n",
    "\n",
    "```python\n",
    "translator_chain = translator_template | model | parser_output\n",
    "# Isso Ã© equivalente a:\n",
    "# RunnableSequence(first=translator_template, middle=[model], last=parser_output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338882cf",
   "metadata": {},
   "source": [
    "Input: {\"texto\": \"Hello...\", \"idioma\": \"portuguÃªs\"}\n",
    "  â†“\n",
    "translator_template â†’ Gera prompt formatado\n",
    "  â†“\n",
    "model â†’ Processa prompt e gera resposta\n",
    "  â†“\n",
    "parser_output â†’ Extrai string da resposta\n",
    "  â†“\n",
    "Output: \"OlÃ¡, como vocÃª estÃ¡ hoje?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fad4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
