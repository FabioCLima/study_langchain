{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee32b11",
   "metadata": {},
   "source": [
    "# Learning Langchain (The book)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6628b8b3",
   "metadata": {},
   "source": [
    "The main task of the software engineer working with LLMs is not to train an LLM, or even to fine-tune one (usually), but rather to take an existing LLM and work out how to get it to accomplish the task you need for your application. Adapting an existing LLM for your task is called `prompt engineering`\n",
    "\n",
    "`prompt engineering with LangChain` - how to use LangChain to get LLMs to do what do what you have in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e4e15",
   "metadata": {},
   "source": [
    "## Prompt technique\n",
    "___\n",
    "\n",
    "* Zero-Shot Prompting\n",
    "* Chain-of-Thought(CoT)\n",
    "* Retrieval-Augmented Generation - in real applications should be combined with CoT\n",
    "* Tool Calling - consist of prepending the prompt with a list of external functions the LLM can make use of, along with descriptions of what is good for nd instructions on how to use one (or more) of these functions.The developer of the application - should parse the output and call the appropriate functions.\n",
    "* Few-Shot Prompting\n",
    "\n",
    "Important things to keep in mind when prompting LLMs: each prompting technique is most useful when used in combination with (some of) the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364b44ed",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "**RAG** Ã© uma tÃ©cnica que combina **recuperaÃ§Ã£o de informaÃ§Ãµes** com **geraÃ§Ã£o de texto**. Em vez de depender apenas do conhecimento prÃ©-treinado do LLM, o RAG busca informaÃ§Ãµes relevantes em uma base de dados externa e usa essas informaÃ§Ãµes como contexto para gerar respostas mais precisas e atualizadas.\n",
    "\n",
    "### Funcionamento do RAG:\n",
    "1. **IndexaÃ§Ã£o**: Documentos sÃ£o vetorizados e armazenados\n",
    "2. **RecuperaÃ§Ã£o**: Query do usuÃ¡rio busca documentos similares\n",
    "3. **GeraÃ§Ã£o**: LLM usa documentos recuperados como contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f89c9",
   "metadata": {},
   "source": [
    "`LLM interface simply takes a string prompt as input, sends the input to the model provider, and then returns the model prediction as output.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7871f",
   "metadata": {},
   "source": [
    "## CapÃ­tulo 1\n",
    "___\n",
    "\n",
    "a - Chamada a um llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58811059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI  # type: ignore\n",
    "from langchain_core.prompts import ChatPromptTemplate  # type: ignore\n",
    "import os\n",
    "\n",
    "#* Carrega as variÃ¡veis de ambiente\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "#* Verifica se a API key estÃ¡ configurada\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY nÃ£o encontrada no arquivo .env\")\n",
    "\n",
    "#* Configura o modelo com parÃ¢metros especÃ­ficos\n",
    "model: ChatOpenAI = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,  # Controla a criatividade das respostas\n",
    ")  # type: ignore\n",
    "\n",
    "response = model.invoke(\"The Sky is ?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d956fc3",
   "metadata": {},
   "source": [
    "b - Chatmodel\n",
    "\n",
    "`HumanMessage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bb8325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Brazil is BrasÃ­lia.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = [HumanMessage(\"What is the capital of Brasil?\")]\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1725a9b",
   "metadata": {},
   "source": [
    "c - System\n",
    "\n",
    "`SystemMessage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57d2fcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris!!!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = ChatOpenAI()\n",
    "system_msg = \"\"\"You are a helpful assistant that responds to questions with three exclamations marks.\n",
    "\"\"\"\n",
    "human_msg = HumanMessage('What is the capital of France?')\n",
    "response = model.invoke([system_msg, human_msg])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e89fd3",
   "metadata": {},
   "source": [
    "Prompt instructions significantly influences the model's output. Prompts help the model understand context and generate relevant answers to queries.\n",
    "\n",
    "LangChain provides prompt template interfaces that make it easy to construct prompts with dynamic inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47b79d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Answer the questions based on the context below.\\nIf the question cannot be answered using the information provided, answer with \"I don\\'t know\".\\n\\nContext: The most recent advancements in NLP are being driven by Large Language Model (LLM). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face\\'s \\'transformers\\' library, or by utilizing OpenAI and cohere\\'s offerings through the `openai` and `cohere` libraries, respectively.\\n\\nQuestion: Which model providers offers LLMs?'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the questions based on the context below.\n",
    "If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\"\"\")\n",
    "\n",
    "response = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Model (LLM). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's 'transformers' library, or by utilizing OpenAI and cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offers LLMs?\"\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "246ae6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responda a questÃ£o baseada no contexto a seguir. Se a questÃ£o nÃ£o puder ser respondida usando a informaÃ§Ã£o, responda usando 'NÃ£o sei a resposta para essa pergunta!'\n",
      "\n",
      "Contexto: Quais os frameworks para a criaÃ§Ã£o de IAs, chatbots, mais usados para construÃ§Ã£o de agentes de IA.\n",
      "\n",
      "Pergunta: Qual Ã© o framework mais performÃ¡tico e mais simples de aprender?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "    \"\"\"Responda a questÃ£o baseada no contexto a seguir. Se a questÃ£o nÃ£o puder ser respondida usando a informaÃ§Ã£o, responda usando 'NÃ£o sei a resposta para essa pergunta!'\n",
    "\n",
    "Contexto: {contexto}\n",
    "\n",
    "Pergunta: {pergunta}\"\"\"\n",
    ")\n",
    "\n",
    "# 2. Invoke corrigido - fora do template e com aspas fechadas\n",
    "response = template.invoke({\n",
    "    \"contexto\": \"Quais os frameworks para a criaÃ§Ã£o de IAs, chatbots, mais usados para construÃ§Ã£o de agentes de IA.\",\n",
    "    \"pergunta\": \"Qual Ã© o framework mais performÃ¡tico e mais simples de aprender?\"\n",
    "})\n",
    "\n",
    "# 3. PromptValue nÃ£o tem .content, usar .text ou apenas print(response)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bbe655",
   "metadata": {},
   "source": [
    "`Both template and model can be reused many times`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10f8f0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hugging Face, OpenAI, and Cohere offer Large Language Models (LLMs).' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 132, 'total_tokens': 150, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BzRyPTzlGfPMaGQdcaqegGsWcTvLo', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--e6d0d13a-ff28-4384-b09e-5d30e07e2e85-0' usage_metadata={'input_tokens': 132, 'output_tokens': 18, 'total_tokens': 150, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "# `prompt` and `completion` are the results of using template and model once\n",
    "\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0abc7470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='NÃ£o sei a resposta para essa pergunta!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 85, 'total_tokens': 93, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_799e4ca3f1', 'id': 'chatcmpl-BzRyPYcRwFRjXEEnwe7DqZOQWMbi3', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--668e05b7-76bd-4e18-b404-b8b91833c0f4-0' usage_metadata={'input_tokens': 85, 'output_tokens': 8, 'total_tokens': 93, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "template = PromptTemplate.from_template(\n",
    "    \"\"\"Responda a questÃ£o baseada no contexto a seguir. Se a questÃ£o nÃ£o puder ser respondida usando a informaÃ§Ã£o, responda usando 'NÃ£o sei a resposta para essa pergunta!'\n",
    "\n",
    "Contexto: {contexto}\n",
    "\n",
    "Pergunta: {pergunta}\"\"\"\n",
    ")\n",
    "model = ChatOpenAI(model='gpt-4.1')\n",
    "\n",
    "# 2. Invoke corrigido - fora do template e com aspas fechadas\n",
    "prompt = template.invoke({\n",
    "    \"contexto\": \"Quais os frameworks para a criaÃ§Ã£o de IAs, chatbots, mais usados para construÃ§Ã£o de agentes de IA.\",\n",
    "    \"pergunta\": \"Qual Ã© o framework mais performÃ¡tico e mais simples de aprender?\"\n",
    "})\n",
    "\n",
    "# 3. PromptValue nÃ£o tem .content, usar .text ou apenas print(response)\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f80bc9a",
   "metadata": {},
   "source": [
    "Building an AI chat application, the `ChatPromptTemplate` can be used instead to provide dynamic inputs based on the role of the chat message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7484cbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Context: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: Which model providers offer LLMs?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
    "        ),\n",
    "        (\"human\", \"Context: {context}\"),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aeb633d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='OpenAI and Cohere offer Large Language Models (LLMs).' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 137, 'total_tokens': 150, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BzRyQBtK9vgTLnOqDe0ONBm2dqgS9', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--e849bb9e-d8a1-4700-9f78-025ea1f40d93-0' usage_metadata={'input_tokens': 137, 'output_tokens': 13, 'total_tokens': 150, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# both `template` and `model` can be reused many times\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
    "        ),\n",
    "        (\"human\", \"Context: {context}\"),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# `prompt` and `completion` are the results of using template and model once\n",
    "\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a632c",
   "metadata": {},
   "source": [
    "1. PadrÃ£o de Prompts DinÃ¢micos - Sim, Ã© um padrÃ£o estabelecido!\n",
    "O cÃ³digo que vocÃª estÃ¡ vendo segue um padrÃ£o muito comum e recomendado no LangChain:\n",
    "Por que este padrÃ£o Ã© importante?\n",
    "```python\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"InstruÃ§Ã£o do sistema...\"),\n",
    "    (\"human\", \"Entrada do usuÃ¡rio: {variavel}\"),\n",
    "])\n",
    "```\n",
    "ReutilizaÃ§Ã£o: O template pode ser usado mÃºltiplas vezes com diferentes dados\n",
    "* SeparaÃ§Ã£o de responsabilidades: O prompt (template) Ã© separado da execuÃ§Ã£o (model)\n",
    "* Manutenibilidade: FÃ¡cil de modificar e testar prompts\n",
    "* Escalabilidade: Pode ser usado em pipelines complexos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a95a3",
   "metadata": {},
   "source": [
    "Quando usar:\n",
    "* Definir o papel(role)/comportamento(behave) do modelo.\n",
    "* Estabelecer regras e limitaÃ§Ãµes\n",
    "* Configurar o contexto geral da conversa\n",
    "* Definir o tom e estilo de resposta.\n",
    "\n",
    "\n",
    "```python\n",
    "(\"system\", \"VocÃª Ã© um tutor de programaÃ§Ã£o Python. Sempre explique conceitos de forma simples e forneÃ§a exemplos prÃ¡ticos.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b898b536",
   "metadata": {},
   "source": [
    "`HumanMessage` (Mensagem Humana)\n",
    "\n",
    "```python\n",
    "(\"human\", \"Pergunta: {question}\")\n",
    "```\n",
    "\n",
    "Quando Usar:\n",
    "\n",
    "* Entradas do usuÃ¡rio\n",
    "* Perguntas especÃ­ficas\n",
    "* Dados que precisam ser processados\n",
    "* Contexto que o usuÃ¡rio fornece\n",
    "\n",
    "```python\n",
    "(\"human\", \"Explique o conceito de list comprehension em Python\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f531d8df",
   "metadata": {},
   "source": [
    "`AIMessage (Mensagem da IA)`\n",
    "```python\n",
    "(\"ai\", \"Resposta da IA...\")\n",
    "```\n",
    "\n",
    "Quando Usar:\n",
    "\n",
    "* Simular conversar anteriores\n",
    "* Fornecer exemplos de respostas\n",
    "* Criar contextos de few-shot learning\n",
    "* Manter histÃ³rico de conversas\n",
    "\n",
    "Exemplo prÃ¡tico:\n",
    "```python\n",
    "(\"ai\", \"List comprehension Ã© uma forma concisa de criar listas em Python. Exemplo: [x*2 for x in range(5)]\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e2bd6",
   "metadata": {},
   "source": [
    "`Estrutura Recomendada para Prompts DinÃ¢micos`\n",
    "___\n",
    "```python\n",
    "# 1. Definir o template com tipos apropriados\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Defina o papel/contexto do modelo\"),\n",
    "    (\"human\", \"ForneÃ§a o contexto: {context}\"),\n",
    "    (\"human\", \"FaÃ§a a pergunta: {question}\"),\n",
    "])\n",
    "\n",
    "# 2. Criar o modelo\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# 3. Invocar com dados dinÃ¢micos\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"seu contexto aqui\",\n",
    "    \"question\": \"sua pergunta aqui\"\n",
    "})\n",
    "\n",
    "# 4. Obter a resposta\n",
    "response = model.invoke(prompt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace97d30",
   "metadata": {},
   "source": [
    "`**Boas PrÃ¡ticas para Desenvolvedores de Agentes**`\n",
    "___\n",
    "\n",
    "1.Sempre use templates para prompts dinÃ¢micos\n",
    "\n",
    "    -Evite strings hardcoded\n",
    "    -Facilita testes e modificaÃ§Ãµes\n",
    "\n",
    "2.Separe claramente os tipos de mensagens\n",
    "\n",
    "    -System: configuraÃ§Ã£o/contexto\n",
    "    -Human: entrada do usuÃ¡rio\n",
    "    -AI: respostas/exemplos\n",
    "\n",
    "3.Use variÃ¡veis para dados dinÃ¢micos\n",
    "\n",
    "    -{context}, {question}, {user_input}\n",
    "    -Facilita a reutilizaÃ§Ã£o\n",
    "\n",
    "4.Mantenha prompts modulares\n",
    "\n",
    "    -Cada template com responsabilidade especÃ­fica\n",
    "    -Combine templates para casos complexos\n",
    "\n",
    "**VersÃ£o bÃ¡sica**\n",
    "```python\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Responda baseado no contexto.\"),\n",
    "    (\"human\", \"Contexto: {context}\"),\n",
    "    (\"human\", \"Pergunta: {question}\"),\n",
    "])\n",
    "```\n",
    "**VersÃ£o avanÃ§ada(para agentes)**\n",
    "```python\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"VocÃª Ã© um assistente especializado em anÃ¡lise de dados.\"),\n",
    "    (\"human\", \"Contexto: {context}\"),\n",
    "    (\"human\", \"Pergunta: {question}\"),\n",
    "    (\"ai\", \"Vou analisar o contexto e responder sua pergunta.\"),\n",
    "    (\"human\", \"Por favor, seja especÃ­fico e forneÃ§a exemplos.\"),\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18157cbe",
   "metadata": {},
   "source": [
    "Para lembrar:\n",
    "\n",
    " - `SystemMessage` = \"Quem vocÃª Ã©\"\n",
    " - `HumanMessage` = \"O que o usuÃ¡rio quer\"\n",
    " - `AIMessage` = \"Como vocÃª responde\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f0c28b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning Ã© um campo da inteligÃªncia artificial que se concentra no desenvolvimento de algoritmos e modelos que permitem aos computadores aprender e melhorar a partir de dados. Esses modelos sÃ£o treinados para fazer previsÃµes, identificar padrÃµes e tomar decisÃµes sem serem explicitamente programados para cada tarefa.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"VocÃª Ã© um assistente prestativo, especialista em MatemÃ¡tica.\"),\n",
    "    HumanMessage(content=\"Explique o que Ã© Machine Learning em 50 palavras.\")\n",
    "]\n",
    "\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3c4aa4",
   "metadata": {},
   "source": [
    "`PromptTemplate` simples Ã© sÃ³ para texto, `ChatPromptTemplate` Ã© para conversas com roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9a6d371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain Ã© um framework de cÃ³digo aberto que facilita algoritmos e modelos de machine learning de linguagem natural. Ele fornece componentes prÃ©-construÃ­dos e ferramentas para desenvolvedores criarem aplicaÃ§Ãµes de processamento de linguagem.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"VocÃª Ã© um especialista em {tÃ³pico}\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"Explique {assunto} em 35 palavras\")\n",
    "])\n",
    "\n",
    "# Template\n",
    "formatted_prompt = prompt.format_messages(\n",
    "    tÃ³pico=\"framework\",\n",
    "    assunto=\"langchain\"\n",
    "    )\n",
    "\n",
    "response = chat.invoke(formatted_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af41a7e3",
   "metadata": {},
   "source": [
    "## Getting Specific Formats out of LLMs\n",
    "___\n",
    "\n",
    "The most common format to generate with LLMs is JSON, which can be sent over the to your frontend code or be saved to a databased.\n",
    "\n",
    "1. First task, define the output schema (from LLM). Then include that schema in the prompt , along with the text you want to use as the source.\n",
    "2. In order to define a `schema`, this is easiest to do with `Pydantic`(*library* used for validating data against schemas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "115f857d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='They weigh the same.' justification='A pound is a unit of weight. Therefore, a pound of bricks and a pound of feathers both weigh exactly one pound, regardless of the material. The difference is in their volume and density, not their weight.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    \"\"\"An answer to the user's question along with justification for the answer.\"\"\"\n",
    "    answer: str\n",
    "    \"\"\"The answer to the user's question\"\"\"\n",
    "    justification: str\n",
    "    \"\"\"Justification for the answer\"\"\"\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "structured_model = model.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "response = structured_model.invoke(\n",
    "    \"What weighs more, a pound of bricks or a pound of feathers\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec1ce2d",
   "metadata": {},
   "source": [
    "#### TUTORIAL: LangChain + Pandantic - Structured Output\n",
    "========================================================\n",
    "\n",
    "Este exemplo demonstra como usar Pydantic com LangChain para:\n",
    "1. Validar dados de entrada e saÃ­da\n",
    "2. Garantir formato estruturado das respostas\n",
    "3. Tornar o cÃ³digo mais robusto e type-safe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c5aa3276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1066a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerWithJustification(BaseModel):\n",
    "    \"\"\"\n",
    "    Modelo que representa uma resposta estruturada com justificativa.\n",
    "\n",
    "    ğŸ’¡ Por que usar Pydantic aqui?\n",
    "    - ValidaÃ§Ã£o automÃ¡tica de tipos\n",
    "    - DocumentaÃ§Ã£o clara dos campos\n",
    "    - SerializaÃ§Ã£o JSON automÃ¡tica\n",
    "    - IntegraÃ§Ã£o nativa com LangChain\n",
    "    \"\"\"\n",
    "    answer: str = Field(\n",
    "        description=\"Resposta clara e concisa para a pergunta\",\n",
    "        min_length=1,\n",
    "        max_length=500\n",
    "    )\n",
    "\n",
    "    justification: str = Field(\n",
    "        description = \"ExplicaÃ§Ã£o detalhada do raciocÃ­nio\",\n",
    "        min_length=10,\n",
    "        max_length=100\n",
    "    )\n",
    "\n",
    "    confidence_level: Optional[float] = Field(\n",
    "        default=None,\n",
    "        description=\"NÃ­vel de confianÃ§a (0-1) na resposta\",\n",
    "        ge=0.0,\n",
    "        le=1.0\n",
    "    )\n",
    "\n",
    "    timestamp: Optional[str] = Field(\n",
    "        default_factory=lambda: datetime.now().isoformat(),\n",
    "        description=\"Timestamp da resposta\"\n",
    "    )\n",
    "\n",
    "    def display_response(self) ->str:\n",
    "        \"\"\"\n",
    "        MÃ©todo para exibir a resposta de forma mais legÃ­vel.\n",
    "\n",
    "        ğŸ”§ BENEFÃCIO: Evita o scroll infinito no output!\n",
    "        \"\"\"\n",
    "        output = f\"\"\"\n",
    "â”Œâ”€ ğŸ’¬ RESPOSTA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ {self.answer}\n",
    "â”œâ”€ ğŸ¤” JUSTIFICATIVA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ {self.justification}\n",
    "\"\"\"\n",
    "\n",
    "        if self.confidence_level:\n",
    "            confidence_bar = \"â–ˆ\" * int(self.confidence_level * 10)\n",
    "            output += f\"\"\"â”œâ”€ ğŸ“Š CONFIANÃ‡A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ {confidence_bar} {self.confidence_level:.1%}\n",
    "\"\"\"\n",
    "\n",
    "        output += f\"\"\"â””â”€ ğŸ•’ {self.timestamp} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\"\"\"\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6881489d",
   "metadata": {},
   "source": [
    "#### ğŸš€ CLASSE PRINCIPAL PARA DEMONSTRAÃ‡ÃƒO\n",
    "\n",
    "`class LangChainPydanticDemo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c344c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangChainPydanticDemo:\n",
    "    \"\"\"\n",
    "    Conceitos Importantes:\n",
    "    1. Structured Output: LangChain forÃ§a o LLM a retornar dados no formato exato do\n",
    "    modelo Pydantic.\n",
    "\n",
    "    2.Type safety: Pydantic valida automaticamente os tipos\n",
    "\n",
    "    3.Error Handling: Falhas na validaÃ§Ã£o sÃ£o capturadas\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name = \"gpt-4.1\"):\n",
    "        self.model = ChatOpenAI(\n",
    "                model = model_name,\n",
    "                temperature=0.1, #* baixa criatividade para respostas consistentes\n",
    "        )\n",
    "\n",
    "    #* ğŸ”‘ PONTO CHAVE: with_structured_output() forÃ§a o formato\n",
    "        self.structured_model = self.model.with_structured_output(\n",
    "            AnswerWithJustification\n",
    "        )\n",
    "\n",
    "    def ask_question(self, question: str) -> AnswerWithJustification:\n",
    "        \"\"\"\n",
    "        Faz uma pergunta e retorna resposta estruturada.\n",
    "\n",
    "        ğŸ¯ BENEFÃCIOS do Structured Output:\n",
    "        - Resposta sempre no formato esperado\n",
    "        - ValidaÃ§Ã£o automÃ¡tica dos dados\n",
    "        - Facilita integraÃ§Ã£o com APIs e bancos de dados\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.structured_model.invoke(question)\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            #* Em caso de erro, retorna resposta padrÃ£o\n",
    "            return AnswerWithJustification(\n",
    "                answer=\"Erro ao processar pergunta\",\n",
    "                justification=f\"Erro tÃ©cnico: {str(e)}\",\n",
    "                confidence_level=0.0\n",
    "            )\n",
    "\n",
    "    def demo_multiple_questions(self):\n",
    "        \"\"\"\n",
    "        Demonstra o uso com mÃºltiplas perguntas para mostrar consistÃªncia.\n",
    "        \"\"\"\n",
    "        questions = [\n",
    "            \"O que pesa mais: 1kg de chumbo ou 1kg de algodÃ£o?\",\n",
    "            \"Por que o cÃ©u Ã© azul?\",\n",
    "            \"Qual a diferenÃ§a entre Python e JavaScript?\"\n",
    "        ]\n",
    "\n",
    "        print(\"ğŸ“ DEMONSTRAÃ‡ÃƒO: MÃºltiplas perguntas com formato consistente\\n\")\n",
    "\n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"â“ PERGUNTA {i}: {question}\")\n",
    "            response = self.ask_question(question)\n",
    "            print(response.display_response())\n",
    "            print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e19c7e6",
   "metadata": {},
   "source": [
    "#### ğŸ“– EXEMPLOS PRÃTICOS DE USO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "76ae2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exemplo_basico():\n",
    "    \"\"\"Exemplo bÃ¡sico de uso do structured output.\"\"\"\n",
    "    print(\"ğŸ”µ EXEMPLO 1: Uso BÃ¡sico\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    demo = LangChainPydanticDemo()\n",
    "\n",
    "    question = \"Explique o conceito de recursÃ£o em programaÃ§Ã£o\"\n",
    "    response = demo.ask_question(question)\n",
    "\n",
    "    # âœ¨ SAÃDA LIMPA - sem scroll infinito!\n",
    "    print(response.display_response())\n",
    "\n",
    "    # ğŸ’¾ BONUS: FÃ¡cil conversÃ£o para JSON\n",
    "    print(\"\\nğŸ”§ BONUS - Dados em JSON:\")\n",
    "    print(json.dumps(response.model_dump(), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1fdb5c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”µ EXEMPLO 1: Uso BÃ¡sico\n",
      "----------------------------------------\n",
      "\n",
      "â”Œâ”€ ğŸ’¬ RESPOSTA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ RecursÃ£o Ã© um conceito em programaÃ§Ã£o onde uma funÃ§Ã£o chama a si mesma para resolver um problema, geralmente dividindo-o em subproblemas menores e semelhantes ao original.\n",
      "â”œâ”€ ğŸ¤” JUSTIFICATIVA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ A recursÃ£o Ã© usada quando um problema pode ser decomposto em versÃµes menores de si mesmo. Por meio d\n",
      "â”œâ”€ ğŸ“Š CONFIANÃ‡A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 98.0%\n",
      "â””â”€ ğŸ•’ 2024-06-19T18:00:00Z â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ”§ BONUS - Dados em JSON:\n",
      "{\n",
      "  \"answer\": \"RecursÃ£o Ã© um conceito em programaÃ§Ã£o onde uma funÃ§Ã£o chama a si mesma para resolver um problema, geralmente dividindo-o em subproblemas menores e semelhantes ao original.\",\n",
      "  \"justification\": \"A recursÃ£o Ã© usada quando um problema pode ser decomposto em versÃµes menores de si mesmo. Por meio d\",\n",
      "  \"confidence_level\": 0.98,\n",
      "  \"timestamp\": \"2024-06-19T18:00:00Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "exemplo_basico()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a92e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exemplo_avancado():\n",
    "    \"\"\"Exemplo avanÃ§ado mostrando validaÃ§Ã£o.\"\"\"\n",
    "    print(\"\\nğŸŸ¢ EXEMPLO 2: ValidaÃ§Ã£o com Pydantic\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Tentativa de criar resposta invÃ¡lida (para mostrar validaÃ§Ã£o)\n",
    "    try:\n",
    "        invalid_response = AnswerWithJustification(\n",
    "            answer=\"\",  # âŒ InvÃ¡lido: muito curto\n",
    "            justification=\"Curto\",  # âŒ InvÃ¡lido: muito curto\n",
    "            confidence_level=1.5  # âŒ InvÃ¡lido: > 1.0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ VALIDAÃ‡ÃƒO PYDANTIC FUNCIONOU: {e}\")\n",
    "\n",
    "    # âœ… Resposta vÃ¡lida\n",
    "    valid_response = AnswerWithJustification(\n",
    "        answer=\"Python Ã© uma linguagem de programaÃ§Ã£o\",\n",
    "        justification=\"Python Ã© conhecida por sua sintaxe simples e legÃ­vel\",\n",
    "        confidence_level=0.9\n",
    "    )\n",
    "\n",
    "    print(\"\\nâœ… RESPOSTA VÃLIDA:\")\n",
    "    print(valid_response.display_response())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "96f501d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŸ¢ EXEMPLO 2: ValidaÃ§Ã£o com Pydantic\n",
      "----------------------------------------\n",
      "âŒ VALIDAÃ‡ÃƒO PYDANTIC FUNCIONOU: 3 validation errors for AnswerWithJustification\n",
      "answer\n",
      "  String should have at least 1 character [type=string_too_short, input_value='', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_too_short\n",
      "justification\n",
      "  String should have at least 10 characters [type=string_too_short, input_value='Curto', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_too_short\n",
      "confidence_level\n",
      "  Input should be less than or equal to 1 [type=less_than_equal, input_value=1.5, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/less_than_equal\n",
      "\n",
      "âœ… RESPOSTA VÃLIDA:\n",
      "\n",
      "â”Œâ”€ ğŸ’¬ RESPOSTA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ Python Ã© uma linguagem de programaÃ§Ã£o\n",
      "â”œâ”€ ğŸ¤” JUSTIFICATIVA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Python Ã© conhecida por sua sintaxe simples e legÃ­vel\n",
      "â”œâ”€ ğŸ“Š CONFIANÃ‡A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 90.0%\n",
      "â””â”€ ğŸ•’ 2025-07-31T19:40:09.245567 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "exemplo_avancado()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2224df6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ DEMONSTRAÃ‡ÃƒO COMPLETA:\n",
      "============================================================\n",
      "ğŸ“ DEMONSTRAÃ‡ÃƒO: MÃºltiplas perguntas com formato consistente\n",
      "\n",
      "â“ PERGUNTA 1: O que pesa mais: 1kg de chumbo ou 1kg de algodÃ£o?\n",
      "\n",
      "â”Œâ”€ ğŸ’¬ RESPOSTA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ 1kg de chumbo e 1kg de algodÃ£o pesam exatamente o mesmo: 1kg cada.\n",
      "â”œâ”€ ğŸ¤” JUSTIFICATIVA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ A pergunta compara massas iguais de materiais diferentes. O peso Ã© uma medida da forÃ§a gravitacional\n",
      "â”œâ”€ ğŸ“Š CONFIANÃ‡A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100.0%\n",
      "â””â”€ ğŸ•’ 2023-06-19T12:00:00Z â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "============================================================\n",
      "\n",
      "â“ PERGUNTA 2: Por que o cÃ©u Ã© azul?\n",
      "\n",
      "â”Œâ”€ ğŸ’¬ RESPOSTA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ O cÃ©u Ã© azul porque a luz do Sol, ao passar pela atmosfera da Terra, sofre espalhamento pelas molÃ©culas de ar, e a luz azul Ã© espalhada em todas as direÃ§Ãµes com mais intensidade do que outras cores.\n",
      "â”œâ”€ ğŸ¤” JUSTIFICATIVA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ A luz branca do Sol Ã© composta por vÃ¡rias cores, cada uma com um comprimento de onda diferente. Ao a\n",
      "â”œâ”€ ğŸ“Š CONFIANÃ‡A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 99.0%\n",
      "â””â”€ ğŸ•’ 2024-06-18T19:00:00Z â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "============================================================\n",
      "\n",
      "â“ PERGUNTA 3: Qual a diferenÃ§a entre Python e JavaScript?\n",
      "\n",
      "â”Œâ”€ ğŸ’¬ RESPOSTA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ Python e JavaScript sÃ£o linguagens de programaÃ§Ã£o diferentes, cada uma com suas caracterÃ­sticas e usos principais. Python Ã© uma linguagem de propÃ³sito geral, muito usada em ciÃªncia de dados, automaÃ§Ã£o, backend e inteligÃªncia artificial. JavaScript Ã© focada principalmente no desenvolvimento web, especialmente para criar interatividade em pÃ¡ginas web.\n",
      "â”œâ”€ ğŸ¤” JUSTIFICATIVA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Python tem sintaxe simples, Ã© fortemente tipada (mas dinamicamente), e Ã© conhecida pela legibilidade\n",
      "â”œâ”€ ğŸ“Š CONFIANÃ‡A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100.0%\n",
      "â””â”€ ğŸ•’ 2024-06-18T19:00:00Z â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DemonstraÃ§Ã£o completa\n",
    "print(\"\\nğŸ“ DEMONSTRAÃ‡ÃƒO COMPLETA:\")\n",
    "print(\"=\"*60)\n",
    "demo = LangChainPydanticDemo()\n",
    "demo.demo_multiple_questions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61a6bc3",
   "metadata": {},
   "source": [
    "ğŸ“ LIÃ‡Ã•ES APRENDIDAS:\n",
    "\n",
    "1. **Pydantic + LangChain = Dados Estruturados**\n",
    "   - LLM retorna sempre no formato correto\n",
    "   - ValidaÃ§Ã£o automÃ¡tica de tipos e valores\n",
    "\n",
    "2. **BenefÃ­cios para ProduÃ§Ã£o**\n",
    "   - Code completion no IDE\n",
    "   - DocumentaÃ§Ã£o automÃ¡tica\n",
    "   - IntegraÃ§Ã£o fÃ¡cil com APIs\n",
    "\n",
    "3. **Melhor UX**\n",
    "   - SaÃ­da formatada e legÃ­vel\n",
    "   - Sem scroll infinito\n",
    "   - InformaÃ§Ãµes organizadas\n",
    "\n",
    "4. **Type Safety**\n",
    "   - Erros capturados em desenvolvimento\n",
    "   - CÃ³digo mais robusto\n",
    "   - ManutenÃ§Ã£o facilitada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f84ea1",
   "metadata": {},
   "source": [
    "**Por que Pydantic Ã© Fundamental no LangChain**:\n",
    "\n",
    "1. Structured Output: ForÃ§a o LLM a retornar dados no formato exato\n",
    "2. ValidaÃ§Ã£o AutomÃ¡tica: Tipos e valores sÃ£o verificados automaticamente\n",
    "3. IntegraÃ§Ã£o Nativa: LangChain foi projetado para trabalhar com Pydantic\n",
    "4. ProduÃ§Ã£o Ready: Facilita APIs, banco de dados e frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365e92c",
   "metadata": {},
   "source": [
    "#### Other Machine-Readable Formats with Output Parsers\n",
    "___\n",
    "\n",
    "We can also use an LLM or chat model to produce output in other formats, such as CSV or XML. Using output parsers  that are classes that help us structure large language model responses.\n",
    "\n",
    "    - Providing format instructions\n",
    "    - Validating and parsing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c4a436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'cherry']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "response = parser.invoke(\"apple, banana, cherry\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
