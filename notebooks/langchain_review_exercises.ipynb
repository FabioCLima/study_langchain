{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a68ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate   \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac118180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue\n"
     ]
    }
   ],
   "source": [
    "#! TODO - Instantiate your chat model\n",
    "# * Carrega as variáveis de ambiente\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# * Verifica se a API key está configurada\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY não encontrada no arquivo .env\")\n",
    "\n",
    "# * Configura o modelo com parâmetros específicos\n",
    "model: ChatOpenAI = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,  # Controla a criatividade das respostas\n",
    ")  # type: ignore\n",
    "\n",
    "response = model.invoke(\"The Sky is ?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a17802a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá, como você está hoje?\n"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4.1\", temperature=0.0)\n",
    "translator_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um tradutor especializado. Traduza o texto para {idioma}.\"),\n",
    "    (\"human\", \"{texto}\")\n",
    "])\n",
    "parser_output = StrOutputParser()\n",
    "\n",
    "translator_chain = translator_template | model | parser_output\n",
    "\n",
    "response = translator_chain.invoke({\"texto\": \"Hello, how are you today?\", \"idioma\": \"português\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff98872",
   "metadata": {},
   "source": [
    "# Tutorial LangChain: Runnable, RunnableSequence, RunnableMap e Métodos\n",
    "\n",
    "## 1. Conceito Fundamental: Runnable\n",
    "\n",
    "O `Runnable` é a interface base do LangChain que padroniza como diferentes componentes podem ser executados e combinados. Qualquer objeto que implementa `Runnable` possui métodos como `.invoke()`, `.stream()`, `.batch()`, etc.\n",
    "\n",
    "### Componentes Runnable no seu código:\n",
    "```python\n",
    "# Todos estes são objetos Runnable:\n",
    "translator_template = ChatPromptTemplate.from_messages([...])  # Runnable\n",
    "model = ChatOpenAI(model=\"gpt-4.1\", temperature=0.0)          # Runnable  \n",
    "parser_output = StrOutputParser()                             # Runnable\n",
    "```\n",
    "\n",
    "## 2. RunnableSequence (Operador Pipe `|`)\n",
    "\n",
    "Quando você usa o operador `|`, está criando uma `RunnableSequence` - uma cadeia onde a saída de um componente vira entrada do próximo.\n",
    "\n",
    "### Seu código cria esta sequência:\n",
    "```python\n",
    "translator_chain = translator_template | model | parser_output\n",
    "# Isso é equivalente a:\n",
    "# RunnableSequence(first=translator_template, middle=[model], last=parser_output)\n",
    "```\n",
    "\n",
    "### Fluxo de dados:\n",
    "```\n",
    "Input: {\"texto\": \"Hello...\", \"idioma\": \"português\"}\n",
    "  ↓\n",
    "translator_template → Gera prompt formatado\n",
    "  ↓\n",
    "model → Processa prompt e gera resposta\n",
    "  ↓\n",
    "parser_output → Extrai string da resposta\n",
    "  ↓\n",
    "Output: \"Olá, como você está hoje?\"\n",
    "```\n",
    "\n",
    "## 3. Método .invoke()\n",
    "\n",
    "O `.invoke()` executa a cadeia de forma síncrona com um único input:\n",
    "\n",
    "```python\n",
    "# Seu código usa invoke:\n",
    "response = translator_chain.invoke({\n",
    "    \"texto\": \"Hello, how are you today?\", \n",
    "    \"idioma\": \"português\"\n",
    "})\n",
    "```\n",
    "\n",
    "### Outros métodos de execução:\n",
    "```python\n",
    "# Execução assíncrona\n",
    "response = await translator_chain.ainvoke(input_data)\n",
    "\n",
    "# Processamento em lote\n",
    "responses = translator_chain.batch([input1, input2, input3])\n",
    "\n",
    "# Streaming (útil para respostas longas)\n",
    "for chunk in translator_chain.stream(input_data):\n",
    "    print(chunk, end=\"\")\n",
    "```\n",
    "\n",
    "## 4. Método .pipe()\n",
    "\n",
    "O `.pipe()` permite adicionar mais componentes à cadeia existente:\n",
    "\n",
    "```python\n",
    "# Criando uma cadeia base\n",
    "base_chain = translator_template | model\n",
    "\n",
    "# Adicionando parser com pipe\n",
    "complete_chain = base_chain.pipe(parser_output)\n",
    "\n",
    "# Ou adicionando mais processamento\n",
    "def capitalize_output(text):\n",
    "    return text.upper()\n",
    "\n",
    "enhanced_chain = translator_chain.pipe(capitalize_output)\n",
    "```\n",
    "\n",
    "## 5. Método .bind()\n",
    "\n",
    "O `.bind()` permite \"fixar\" parâmetros em um Runnable:\n",
    "\n",
    "```python\n",
    "# Exemplo com seu modelo\n",
    "model_with_params = model.bind(\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# Criando template específico para francês\n",
    "french_translator = translator_template.bind(idioma=\"francês\")\n",
    "\n",
    "# Agora só precisa passar o texto:\n",
    "french_chain = french_translator | model | parser_output\n",
    "response = french_chain.invoke({\"texto\": \"Hello world\"})\n",
    "```\n",
    "\n",
    "## 6. RunnableMap\n",
    "\n",
    "O `RunnableMap` permite executar múltiplos Runnables em paralelo:\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableMap\n",
    "\n",
    "# Criando tradutores para diferentes idiomas\n",
    "spanish_chain = translator_template.bind(idioma=\"espanhol\") | model | parser_output\n",
    "french_chain = translator_template.bind(idioma=\"francês\") | model | parser_output\n",
    "\n",
    "# Executando em paralelo\n",
    "multi_translator = RunnableMap({\n",
    "    \"spanish\": spanish_chain,\n",
    "    \"french\": french_chain,\n",
    "    \"portuguese\": translator_chain\n",
    "})\n",
    "\n",
    "# Resultado será um dicionário com todas as traduções\n",
    "results = multi_translator.invoke({\"texto\": \"Hello world\"})\n",
    "# Output: {\n",
    "#   \"spanish\": \"Hola mundo\",\n",
    "#   \"french\": \"Bonjour le monde\", \n",
    "#   \"portuguese\": \"Olá mundo\"\n",
    "# }\n",
    "```\n",
    "\n",
    "## 7. Exemplo Completo Expandido\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "\n",
    "# Componentes base\n",
    "model = ChatOpenAI(model=\"gpt-4\", temperature=0.0)\n",
    "translator_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um tradutor especializado. Traduza o texto para {idioma}.\"),\n",
    "    (\"human\", \"{texto}\")\n",
    "])\n",
    "parser_output = StrOutputParser()\n",
    "\n",
    "# Função para contar palavras\n",
    "def count_words(text):\n",
    "    return {\"translation\": text, \"word_count\": len(text.split())}\n",
    "\n",
    "# Cadeia completa com processamento adicional\n",
    "enhanced_chain = (\n",
    "    translator_template \n",
    "    | model \n",
    "    | parser_output \n",
    "    | RunnableLambda(count_words)\n",
    ")\n",
    "\n",
    "# Múltiplas operações em paralelo\n",
    "parallel_operations = RunnableMap({\n",
    "    \"translation\": translator_chain,\n",
    "    \"original_length\": RunnableLambda(lambda x: len(x[\"texto\"].split())),\n",
    "    \"target_language\": RunnableLambda(lambda x: x[\"idioma\"])\n",
    "})\n",
    "\n",
    "# Execução\n",
    "input_data = {\"texto\": \"Hello, how are you today?\", \"idioma\": \"português\"}\n",
    "result = parallel_operations.invoke(input_data)\n",
    "```\n",
    "\n",
    "## 8. Vantagens da Arquitetura Runnable\n",
    "\n",
    "1. **Composabilidade**: Fácil combinação de componentes\n",
    "2. **Padronização**: Interface uniforme para todos os componentes\n",
    "3. **Flexibilidade**: Métodos síncronos, assíncronos, batch e streaming\n",
    "4. **Debugging**: Cada etapa pode ser testada independentemente\n",
    "5. **Reutilização**: Componentes podem ser reutilizados em diferentes cadeias\n",
    "\n",
    "## 9. Dicas Práticas\n",
    "\n",
    "- Use `|` para sequências lineares\n",
    "- Use `RunnableMap` para operações paralelas\n",
    "- Use `.bind()` para fixar parâmetros frequentemente usados\n",
    "- Use `.pipe()` para adicionar processamento a cadeias existentes\n",
    "- Teste cada componente individualmente antes de combiná-los"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2083ce0d",
   "metadata": {},
   "source": [
    "### `RunnableParallel`\n",
    "\n",
    "- Executa múltiplas operações SIMULTANEAMENTE\n",
    "- cada operação recebe a mesma entrada\n",
    "- O resultado é um dicionário com chaves nomeadas\n",
    "- Ideal para operações independentes que podem rodar em paralelo\n",
    "\n",
    "Dica: `RunnableParallel` é perfeito para qdo se precisa fazer múltiplas coisas ao mesmo tempo, como processar um texto e salvar logs, ou analisar diferentes aspectos de uma ideia simultaneamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee0e9bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá, como você está hoje?\n"
     ]
    }
   ],
   "source": [
    "logs = []\n",
    "\n",
    "def log_and_pass(text: str) -> str:\n",
    "    \"\"\"Log simples que passa o texto adiante\"\"\"\n",
    "    print(f\"[LOG] Tradução gerada: {text}\")\n",
    "    \n",
    "# Cadeia com log simples\n",
    "model = ChatOpenAI(model=\"gpt-4.1\", temperature=0.0)\n",
    "translator_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Você é um tradutor especializado. Traduza o texto para {idioma}.\"),\n",
    "        (\"human\", \"{texto}\")\n",
    "    ]\n",
    ")\n",
    "parser_output = StrOutputParser()\n",
    "\n",
    "# Adicionando log após o parse\n",
    "translator_chain_with_log = (\n",
    "    translator_template\n",
    "    | model\n",
    "    | parser_output \n",
    "    | RunnableLambda(log_and_pass)\n",
    ")\n",
    "\n",
    "response = translator_chain.invoke({\"texto\": \"Hello, how are you today?\", \"idioma\": \"português\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cb272fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTANDO CHAIN DE TRADUÇÃO COM LOG ===\n",
      "Teste 1 - Português para Inglês:\n",
      "  Tradução: Hello, how are you today?\n",
      "  Logs salvos até agora: 1\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lista para guardar os logs\n",
    "logs = []\n",
    "\n",
    "# Cadeia com log simples\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "translator_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Você é um tradutor especializado. Traduza o texto para {idioma}.\"),\n",
    "        (\"human\", \"{texto}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Parser simples\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Chain que faz parse E salva log ao mesmo tempo\n",
    "parser_and_log_output_chain = RunnableParallel(\n",
    "    output=parser,\n",
    "    log=RunnableLambda(lambda x: logs.append(x))\n",
    ")\n",
    "\n",
    "# Montando a chain completa\n",
    "translator_chain = translator_template | model | parser_and_log_output_chain\n",
    "\n",
    "# Testando a chain de tradução\n",
    "print(\"=== TESTANDO CHAIN DE TRADUÇÃO COM LOG ===\")\n",
    "\n",
    "# Teste 1: Português para Inglês\n",
    "resultado1 = translator_chain.invoke({\n",
    "    \"idioma\": \"inglês\",\n",
    "    \"texto\": \"Olá, como você está hoje?\"\n",
    "})\n",
    "\n",
    "print(\"Teste 1 - Português para Inglês:\")\n",
    "print(f\"  Tradução: {resultado1['output']}\")\n",
    "print(f\"  Logs salvos até agora: {len(logs)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f7246da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste 2 - Português para Francês:\n",
      "  Tradução: J'aime beaucoup programmer en Python.\n",
      "  Logs salvos até agora: 2\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Teste 2: Português para Francês\n",
    "resultado2 = translator_chain.invoke({\n",
    "    \"idioma\": \"francês\", \n",
    "    \"texto\": \"Eu gosto muito de programar em Python\"\n",
    "})\n",
    "\n",
    "print(\"Teste 2 - Português para Francês:\")\n",
    "print(f\"  Tradução: {resultado2['output']}\")\n",
    "print(f\"  Logs salvos até agora: {len(logs)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66dc6632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste 3 - Português para Espanhol:\n",
      "  Tradução: La inteligencia artificial está evolucionando rápidamente.\n",
      "  Logs salvos até agora: 3\n",
      "\n",
      "==================================================\n",
      "TODOS OS LOGS CAPTURADOS:\n",
      "Log 1: content='Hello, how are you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 32, 'total_tokens': 39, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-C0T4ER5RlHAO5iHkz6qHDEd64O6Ge', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--3483d7bd-c3fb-4750-a388-0e95dd992b6d-0' usage_metadata={'input_tokens': 32, 'output_tokens': 7, 'total_tokens': 39, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Log 2: content=\"J'aime beaucoup programmer en Python.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 33, 'total_tokens': 40, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-C0T4Xa2pt7nivLKLXflYeE2x5n8Mn', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--885f0cde-2f09-4cab-b25d-c54c0f50094b-0' usage_metadata={'input_tokens': 33, 'output_tokens': 7, 'total_tokens': 40, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Log 3: content='La inteligencia artificial está evolucionando rápidamente.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 32, 'total_tokens': 40, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-C0T4f16JOOoJPm8AqF1C2KuwMcTP1', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--8e0c7b0c-ff54-4371-b261-66f1e43b8224-0' usage_metadata={'input_tokens': 32, 'output_tokens': 8, 'total_tokens': 40, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Total de traduções logadas: 3\n"
     ]
    }
   ],
   "source": [
    "# Teste 3: Português para Espanhol\n",
    "resultado3 = translator_chain.invoke({\n",
    "    \"idioma\": \"espanhol\",\n",
    "    \"texto\": \"A inteligência artificial está evoluindo rapidamente\"\n",
    "})\n",
    "\n",
    "print(\"Teste 3 - Português para Espanhol:\")\n",
    "print(f\"  Tradução: {resultado3['output']}\")\n",
    "print(f\"  Logs salvos até agora: {len(logs)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TODOS OS LOGS CAPTURADOS:\")\n",
    "for i, log in enumerate(logs, 1):\n",
    "    print(f\"Log {i}: {log}\")\n",
    "\n",
    "print(f\"\\nTotal de traduções logadas: {len(logs)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
