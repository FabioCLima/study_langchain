{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "637142e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "from dotenv import load_dotenv  # type: ignore\n",
    "from langchain_openai import ChatOpenAI  # type: ignore\n",
    "from openai import OpenAI  # type: ignore\n",
    "\n",
    "\n",
    "#* Exceção customizada para ausência da chave da API\n",
    "class ApiKeyNotFoundError(Exception):\n",
    "    \"\"\"\n",
    "    Exceção lançada quando a chave OPENAI_API_KEY não é encontrada no arquivo .env.\n",
    "    Herda de Exception (herança simples em Python).\n",
    "    Boas práticas: crie exceções específicas para facilitar o tratamento de erros.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class ApiKeyLoader:\n",
    "    \"\"\"\n",
    "    Utility to load the OpenAI API key from a .env file.\n",
    "    If no path is provided, it searches for .env in the current and parent directories.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_path: Path | None = None) -> None:\n",
    "        if env_path is not None:\n",
    "            if not env_path.exists() or not env_path.is_file():\n",
    "                raise ValueError(f\"Invalid .env path: {env_path}\")\n",
    "            self.env_path = env_path\n",
    "        else:\n",
    "            found_env = self._find_env_path()\n",
    "            if found_env is None:\n",
    "                raise ValueError(\"Could not find a .env file in current or parent directories.\")\n",
    "            self.env_path = found_env\n",
    "\n",
    "    def _find_env_path(self) -> Path | None:\n",
    "        \"\"\"Searches for a .env file in current and parent directories.\"\"\"\n",
    "        current = Path(__file__).resolve().parent\n",
    "        for parent in [current, *current.parents]:\n",
    "            candidate = parent / \".env\"\n",
    "            if candidate.exists() and candidate.is_file():\n",
    "                return candidate\n",
    "        return None\n",
    "\n",
    "    def get_openai_key(self) -> str:\n",
    "        load_dotenv(self.env_path)\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ApiKeyNotFoundError(\"OPENAI_API_KEY not found in .env file.\")\n",
    "        return api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2654ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatModelFactory:\n",
    "    \"\"\"\n",
    "    Factory class para criar diferentes tipos de modelos de chat do LangChain.\n",
    "\n",
    "    Esta classe implementa o padrão Factory Method, permitindo a criação\n",
    "    de diferentes configurações de modelos ChatOpenAI de forma centralizada\n",
    "    e reutilizável.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Chave da API da OpenAI\n",
    "\n",
    "    Example:\n",
    "        >>> factory = ChatModelFactory(\"your-api-key\")\n",
    "        >>> analytical_model = factory.create_analytical_model()\n",
    "        >>> creative_model = factory.create_creative_model()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str) -> None:\n",
    "        \"\"\"\n",
    "        Inicializa a factory com a chave da API.\n",
    "        \n",
    "        Args:\n",
    "            api_key (str): Chave válida da API da OpenAI\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: Se a api_key estiver vazia ou None\n",
    "        \"\"\"\n",
    "        if not api_key:\n",
    "            msg = \"OpenAI API key is required for ChatModelFactory\"\n",
    "            raise ValueError(msg)\n",
    "        self.api_key: str = api_key\n",
    "\n",
    "    def create_analytical_model(\n",
    "        self, \n",
    "        model_name: str = \"gpt-4\", \n",
    "        temperature: float = 0.1\n",
    "    ) -> ChatOpenAI:\n",
    "        \"\"\"\n",
    "        Cria um modelo otimizado para análises e tarefas que requerem precisão.\n",
    "\n",
    "        Configuração:\n",
    "        - Temperatura baixa (0.1) para respostas mais determinísticas\n",
    "        - Modelo GPT-4 por padrão para melhor capacidade analítica\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Nome do modelo OpenAI (padrão: \"gpt-4\")\n",
    "            temperature (float): Controla a aleatoriedade (padrão: 0.1)\n",
    "\n",
    "        Returns:\n",
    "            ChatOpenAI: Instância configurada para análises\n",
    "        \"\"\"\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key,  # type: ignore\n",
    "            model=model_name,      # fixed\n",
    "            temperature=temperature,\n",
    "        )\n",
    "    \n",
    "    def create_creative_model(\n",
    "        self, \n",
    "        model_name: str = \"gpt-3.5-turbo\", \n",
    "        temperature: float = 0.8\n",
    "    ) -> ChatOpenAI:\n",
    "        \"\"\"\n",
    "        Cria um modelo otimizado para tarefas criativas.\n",
    "        \n",
    "        Configuração:\n",
    "        - Temperatura alta (0.8) para respostas mais criativas\n",
    "        - GPT-3.5-turbo por padrão (mais rápido e econômico)\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Nome do modelo OpenAI (padrão: \"gpt-3.5-turbo\")\n",
    "            temperature (float): Controla a criatividade (padrão: 0.8)\n",
    "            \n",
    "        Returns:\n",
    "            ChatOpenAI: Instância configurada para criatividade\n",
    "        \"\"\"\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key,  # type: ignore\n",
    "            model=model_name,      # fixed\n",
    "            temperature=temperature,\n",
    "        \n",
    "        )\n",
    "    \n",
    "    def create_conversational_model(\n",
    "        self, \n",
    "        model_name: str = \"gpt-3.5-turbo\", \n",
    "        temperature: float = 0.7\n",
    "    ) -> ChatOpenAI:\n",
    "        \"\"\"\n",
    "        Cria um modelo balanceado para conversas naturais.\n",
    "        \n",
    "        Configuração:\n",
    "        - Temperatura moderada (0.7) para equilíbrio entre precisão e naturalidade\n",
    "        - GPT-3.5-turbo por padrão para boa performance\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Nome do modelo OpenAI (padrão: \"gpt-3.5-turbo\")\n",
    "            temperature (float): Controla a naturalidade (padrão: 0.7)\n",
    "            \n",
    "        Returns:\n",
    "            ChatOpenAI: Instância configurada para conversas\n",
    "        \"\"\"\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key,  # type: ignore\n",
    "            model=model_name,      # fixed\n",
    "            temperature=temperature,\n",
    "        )\n",
    "    \n",
    "    def create_custom_model(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        temperature: float,\n",
    "        max_tokens: int = 2048,\n",
    "        **kwargs: object\n",
    "    ) -> ChatOpenAI:\n",
    "        \"\"\"\n",
    "        Cria um modelo com configurações personalizadas.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Nome do modelo OpenAI\n",
    "            temperature (float): Controla a aleatoriedade (0.0 a 2.0)\n",
    "            max_tokens (int): Número máximo de tokens na resposta\n",
    "            **kwargs: Argumentos adicionais para o modelo\n",
    "            \n",
    "        Returns:\n",
    "            ChatOpenAI: Instância com configurações personalizadas\n",
    "        \"\"\"\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key,  # type: ignore\n",
    "            model=model_name,      # fixed\n",
    "            temperature=temperature,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4121e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FUNÇÃO UTILITÁRIA PARA CRIAR MODELO ANALÍTICO ===\n",
    "def create_analytical_model() -> Optional[ChatOpenAI]:\n",
    "    \"\"\"\n",
    "    Função utilitária que demonstra o uso completo do módulo para criar\n",
    "    um modelo analítico.\n",
    "\n",
    "    Esta função:\n",
    "    1. Carrega a chave da API do arquivo .env\n",
    "    2. Cria uma instância da ChatModelFactory\n",
    "    3. Retorna um modelo configurado para análises\n",
    "\n",
    "    Returns:\n",
    "        Optional[ChatOpenAI]: Modelo analítico ou None em caso de erro\n",
    "\n",
    "    Example:\n",
    "        >>> model = create_analytical_model()\n",
    "        >>> if model:\n",
    "        ...     response = model.invoke(\"Analise este texto...\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        env_file_path: Path = Path(__file__).resolve().parent / \".env\"\n",
    "        loader: ApiKeyLoader = ApiKeyLoader(Path(env_file_path))\n",
    "        openai_api_key = loader.get_openai_key()\n",
    "        print(f\"Chave da API OpenAI carregada com sucesso: ***{openai_api_key[-4:]}\")\n",
    "\n",
    "        chat_factory = ChatModelFactory(openai_api_key)\n",
    "        analytical_llm = chat_factory.create_analytical_model()\n",
    "        print(\"Modelo de chat OpenAI criado com sucesso!\")\n",
    "        return analytical_llm\n",
    "\n",
    "    except (ValueError, ApiKeyNotFoundError) as e:\n",
    "        print(f\"Erro ao carregar a chave da API: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b674f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uso avançado\n",
    "loader = ApiKeyLoader(Path(\".env\"))\n",
    "api_key = loader.get_openai_key()\n",
    "factory = ChatModelFactory(api_key)\n",
    "#* analytical model\n",
    "analytical_model = factory.create_analytical_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eb73794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"gpt-4.1\", model_provider=\"openai\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69808327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Fabio! 😊 How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 11, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_b3f1157249', 'id': 'chatcmpl-BxCCaCz01FwzdDWTJw9OIeSsDDhpX', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c15e289e-7d59-4fe7-9118-7fe3f30aaa9c-0', usage_metadata={'input_tokens': 11, 'output_tokens': 11, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Fabio\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43279d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Fabio! How can I help you today, Fabio? 😊\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "response = model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Fabio\"),\n",
    "        AIMessage(content=\"Hello Fabio! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What' my name?\"),\n",
    "    ]\n",
    ")\n",
    "print(response.content)  # Exibe a resposta do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd7fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebca292",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "___\n",
    "\n",
    "Um Prompt Template é um modelo de texto usado para estruturar prompts (entradas de texto) enviados a um modelo de linguagem, como o GPT, de forma reutilizável e flexível. Ele permite que você insira variáveis dinamicamente no prompt, tornando-o mais organizado e fácil de manter.\n",
    "\n",
    "No contexto do LangChain, o PromptTemplate ajuda a criar prompts com partes fixas e partes variáveis, de modo que o mesmo modelo de linguagem possa ser usado em diferentes situações apenas mudando os valores das variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "171529e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"Me fale 3 fatos sobre o uso de {tablets} para estudar e trabalhar.\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac981079",
   "metadata": {},
   "source": [
    "### Construtor Padrão\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9b8750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"Me fale 3 fatos sobre o uso de {tablets} para estudar e trabalhar.\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"tablets\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33f1523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me fale 3 fatos sobre o uso de samsung galaxy tab Fe10+ para estudar e trabalhar.\n"
     ]
    }
   ],
   "source": [
    "# Criar a mensagem formatada\n",
    "formatted_prompt = prompt.format(tablets=\"samsung galaxy tab Fe10+\")\n",
    "print(formatted_prompt)  # Exibe o prompt formatado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72074714",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = analytical_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e24620a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Multitarefa: O Samsung Galaxy Tab S6 FE10+ é equipado com um processador poderoso que permite multitarefa sem esforço. Isso significa que você pode ter várias aplicações abertas ao mesmo tempo, como documentos de trabalho, e-mails, e até mesmo vídeos de estudo ou apresentações. Isso torna o tablet uma ferramenta eficiente para estudar e trabalhar, pois você pode alternar facilmente entre diferentes tarefas.\n",
      "\n",
      "2. S Pen: Este tablet vem com a S Pen, que é uma caneta inteligente que permite que você faça anotações, desenhe e até mesmo controle o tablet à distância. Isso pode ser particularmente útil para estudar, pois você pode destacar informações importantes, fazer anotações à mão e até mesmo desenhar diagramas ou gráficos. Para o trabalho, a S Pen pode ser usada para assinar documentos digitalmente ou fazer apresentações.\n",
      "\n",
      "3. Tela grande e de alta qualidade: O Samsung Galaxy Tab S6 FE10+ possui uma tela de 10,5 polegadas com resolução de 2560 x 1600 pixels. Isso proporciona uma experiência visual nítida e vibrante, tornando-o ideal para ler documentos de trabalho, assistir a vídeos de estudo ou fazer videochamadas. Além disso, a tela grande também permite que você divida a tela e use duas aplicações ao mesmo tempo, o que pode aumentar a produtividade ao estudar ou trabalhar.\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(formatted_prompt)\n",
    "print(response.content)  # Exibe a resposta do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b734665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"tablets\"],\n",
    "    template=\"Me fale 3 fatos sobre o uso de {tablets} para estudar e trabalhar.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8971b001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. Multitarefa: O Samsung Galaxy Tab S6 FE10+ é equipado com um processador poderoso que permite multitarefa sem problemas. Isso significa que você pode ter várias aplicações abertas ao mesmo tempo, como um documento do Word, uma planilha do Excel e um navegador da web, sem que o tablet fique lento. Isso é especialmente útil para estudar e trabalhar, pois você pode alternar facilmente entre diferentes tarefas.\\n\\n2. S Pen: Este tablet vem com a S Pen, que é uma caneta stylus que permite escrever, desenhar e fazer anotações diretamente na tela. Isso pode ser muito útil para tomar notas durante uma aula ou reunião, ou para esboçar ideias durante um brainstorming. A S Pen também tem uma função de tradução, o que pode ser útil para estudar línguas estrangeiras.\\n\\n3. Tela grande e de alta qualidade: O Samsung Galaxy Tab S6 FE10+ tem uma tela de 10,5 polegadas com uma resolução de 2560 x 1600 pixels. Isso proporciona muito espaço para trabalhar e estudar, e a alta resolução garante que o texto e as imagens sejam nítidos e fáceis de ler. Além disso, a tela tem uma taxa de atualização de 120 Hz, o que significa que ela é muito responsiva ao toque.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 316, 'prompt_tokens': 31, 'total_tokens': 347, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BxCTGVu3zMAmrFdjoVsClYfT110ai', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a3366b5d-843b-4534-b00b-1090cb9f12fe-0', usage_metadata={'input_tokens': 31, 'output_tokens': 316, 'total_tokens': 347, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f0411bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# Template para cada exemplo individual\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"operation\", \"step\", \"result\", \"explanation\"],\n",
    "    template=\"Operação: {operation}\\nPasso: {step}\\nResultado: {result}\\nExplicação: {explanation}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5e66993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplos que mostram a progressão matemática\n",
    "examples = [\n",
    "    {\n",
    "        \"operation\": \"2 + 2\",\n",
    "        \"step\": \"Soma simples\",\n",
    "        \"result\": \"4\",\n",
    "        \"explanation\": \"Adição básica: somamos 2 duas vezes\"\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"2 × 2\",\n",
    "        \"step\": \"Multiplicação como soma repetida\",\n",
    "        \"result\": \"4\",\n",
    "        \"explanation\": \"2 × 2 = 2 + 2 = 4 (multiplicação é soma repetida)\"\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"2²\",\n",
    "        \"step\": \"Exponenciação básica\",\n",
    "        \"result\": \"4\",\n",
    "        \"explanation\": \"2² = 2 × 2 = 4 (exponenciação é multiplicação repetida)\"\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"2³\",\n",
    "        \"step\": \"Expandindo a exponenciação\",\n",
    "        \"result\": \"8\",\n",
    "        \"explanation\": \"2³ = 2 × 2 × 2 = 8\"\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"2⁴\",\n",
    "        \"step\": \"Continuando o padrão\",\n",
    "        \"result\": \"16\",\n",
    "        \"explanation\": \"2⁴ = 2 × 2 × 2 × 2 = 16\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b4482de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operação: 2 + 2\n",
      "Passo: Soma simples\n",
      "Resultado: 4\n",
      "Explicação: Adição básica: somamos 2 duas vezes\n",
      "\n",
      "\n",
      "Operação: 2 × 2\n",
      "Passo: Multiplicação como soma repetida\n",
      "Resultado: 4\n",
      "Explicação: 2 × 2 = 2 + 2 = 4 (multiplicação é soma repetida)\n",
      "\n",
      "\n",
      "Operação: 2²\n",
      "Passo: Exponenciação básica\n",
      "Resultado: 4\n",
      "Explicação: 2² = 2 × 2 = 4 (exponenciação é multiplicação repetida)\n",
      "\n",
      "\n",
      "Operação: 2³\n",
      "Passo: Expandindo a exponenciação\n",
      "Resultado: 8\n",
      "Explicação: 2³ = 2 × 2 × 2 = 8\n",
      "\n",
      "\n",
      "Operação: 2⁴\n",
      "Passo: Continuando o padrão\n",
      "Resultado: 16\n",
      "Explicação: 2⁴ = 2 × 2 × 2 × 2 = 16\n",
      "\n",
      "\n",
      "\n",
      "Agora resolva: 2⁸\n",
      "Siga o mesmo padrão dos exemplos acima.\n",
      "Operação: 2⁸\n",
      "Passo:\n",
      "Resultado:\n",
      "Explicação:\n",
      "\n",
      "\n",
      "==================================================\n",
      "RESPOSTA DO MODELO:\n",
      "Passo: Continuando o padrão\n",
      "Resultado: 256\n",
      "Explicação: 2⁸ = 2 × 2 × 2 × 2 × 2 × 2 × 2 × 2 = 256\n"
     ]
    }
   ],
   "source": [
    "# Prompt principal (suffix)\n",
    "suffix = \"\"\"\n",
    "Agora resolva: {problem}\n",
    "Siga o mesmo padrão dos exemplos acima.\n",
    "Operação: {problem}\n",
    "Passo:\n",
    "Resultado:\n",
    "Explicação:\n",
    "\"\"\"\n",
    "\n",
    "# Criando o Few-shot Prompt Template\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"problem\"]\n",
    ")\n",
    "\n",
    "# Testando com 2⁸\n",
    "formatted_prompt = few_shot_prompt.format(problem=\"2⁸\")\n",
    "print(formatted_prompt)\n",
    "\n",
    "# Usando com o modelo\n",
    "response = model.invoke(formatted_prompt)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESPOSTA DO MODELO:\")\n",
    "print(response.content) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff54744",
   "metadata": {},
   "source": [
    "## Criar um chatbot\n",
    "___\n",
    "\n",
    "Neste desafio, você criará um chatbot que se lembra de interações anteriores, segue um fluxo de conversa estruturado e oferece respostas mais humanas usando o Few-Shot Prompting.\n",
    "Ao aproveitar memória, prompts estruturados e exemplos few-shot, seu chatbot se comportará de forma consistente e envolvente.\n",
    "\n",
    "**Cenário**\n",
    "Você está desenvolvendo um assistente virtual para uma empresa. Seu chatbot precisa:\n",
    "- Manter o histórico da conversa\n",
    "- Responder de forma consistente usando exemplos predefinidos de few-shot\n",
    "- Ser personalizável para diferentes estilos, como:\n",
    "- Um assistente robótico com tom de ficção científica\n",
    "- Um chatbot casual para interações divertidas\n",
    "- Um assistente de IA profissional para tarefas empresariais\n",
    "Ao final deste exercício, você terá um chatbot totalmente funcional que pode conversar de forma dinâmica, mantendo uma personalidade predefinida.\n",
    "\n",
    "**Desafio**\n",
    "Seu chatbot deve:\n",
    "- Registrar o histórico da conversa\n",
    "- Utilizar uma abordagem estruturada de Few-Shot Prompting\n",
    "- Permitir a personalização de tom e personalidade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8060ae",
   "metadata": {},
   "source": [
    "* 1. PERSONALIDADE (ChatPromptTemplate)\n",
    "* 2. FORMATO/BLUEPRINT(FewShotPrompt)\n",
    "* 3 ....\n",
    "\n",
    "Pq até agora só vimos como inicializar o chatmodel, como estruturar a mensagem (SystemMessage, HumanMessage, AIMessage), Prompt Templates (basic PromptTemplate, Few-Shot PromptTemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c35b0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a Python tutor\"),\n",
    "    HumanMessage(content=\"Explain what is a dictionary in Python\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e73dace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dictionary in Python is a built-in data type that allows you to store key-value pairs. The keys in a dictionary are unique and can be of any immutable type (like integers, strings, tuples, etc.), while the values can be of any type. \n",
      "\n",
      "Dictionaries are mutable, which means you can add, remove, and change their key-value elements after they are created. They are also unordered, meaning the items do not have a defined order, and you cannot refer to an item by its index.\n",
      "\n",
      "Here is an example of a dictionary:\n",
      "\n",
      "```python\n",
      "my_dict = {'name': 'John', 'age': 30, 'city': 'New York'}\n",
      "```\n",
      "\n",
      "In this dictionary, 'name', 'age', and 'city' are keys, and 'John', 30, and 'New York' are their respective values. You can access the values by their corresponding keys like this:\n",
      "\n",
      "```python\n",
      "print(my_dict['name'])  # Output: John\n",
      "```\n",
      "\n",
      "Dictionaries are very useful for data manipulation in Python, especially when dealing with large datasets.\n"
     ]
    }
   ],
   "source": [
    "ai_response = model.invoke(messages)\n",
    "print(ai_response.content)  # Exibe a resposta do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d317eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python classes are the fundamental concept of object-oriented programming. A class is like a blueprint or template for creating objects (a particular data structure), providing initial values for state (member variables or attributes), and implementations of behavior (member functions or methods).\n",
      "\n",
      "The class is a blueprint that defines the nature of a future object. From classes, we can construct instances. An instance is a specific object created from a particular class. For instance, if we have a class called \"Car\", we might have instances of that class like \"Toyota\", \"Ford\", etc. which are all cars but with different attributes and behaviors.\n",
      "\n",
      "Classes encapsulate data and the methods that manipulate that data within one entity.\n",
      "Response for topic 'Python classes': Python classes are the fundamental concept of object-oriented programming. A class is like a blueprint or template for creating objects (a particular data structure), providing initial values for state (member variables or attributes), and implementations of behavior (member functions or methods).\n",
      "\n",
      "The class is a blueprint that defines the nature of a future object. From classes, we can construct instances. An instance is a specific object created from a particular class. For instance, if we have a class called \"Car\", we might have instances of that class like \"Toyota\", \"Ford\", etc. which are all cars but with different attributes and behaviors.\n",
      "\n",
      "Classes encapsulate data and the methods that manipulate that data within one entity.\n"
     ]
    }
   ],
   "source": [
    "topic = \"Python classes\"\n",
    "prompt = f\"Explain the concept of {topic} in simple terms.\"\n",
    "ai_response = model.invoke(prompt)\n",
    "print(ai_response.content)  # Exibe a resposta do modelo\n",
    "print(f\"Response for topic '{topic}': {ai_response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fc39f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain the concept of {topic} in simple terms.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1abcbabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Explain the concept of {topic} in simple terms.')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42188cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Explain the concept of Python classes in simple terms.')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.invoke({\"topic\": \"Python classes\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d3ad6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A Python function is a block of reusable code that performs a specific task. Functions provide better modularity for your application and allow for high levels of code reusing. You can define functions to provide the required functionality. Here are simple rules to define a function in Python:\\n\\n- Function blocks begin with the keyword def followed by the function name and parentheses ().\\n- Any input parameters or arguments should be placed within these parentheses. You can also define parameters inside these parentheses.\\n- The first statement of a function can be an optional statement - the documentation string of the function or docstring.\\n- The code block within every function starts with a colon (:) and is indented.\\n- The statement return [expression] exits a function, optionally passing back an expression to the caller. A return statement with no arguments is the same as return None.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 167, 'prompt_tokens': 18, 'total_tokens': 185, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BxMNLqEvo99EmfieyJjtcoi8hUiHb', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a25c5249-6796-47dc-a119-a99067f68fa1-0', usage_metadata={'input_tokens': 18, 'output_tokens': 167, 'total_tokens': 185, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\n",
    "    prompt_template.invoke({\"topic\": \"Python function\"})  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a65a92",
   "metadata": {},
   "source": [
    "## Few Shot Prompt \n",
    " - ensina padrão através de exemplos\n",
    "* Uso: Treinar formato específico de resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afbaa7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Few-Shot Prompting com LangChain - Exemplo Didático\n",
    "Demonstra como usar exemplos para treinar o modelo a seguir um padrão específico\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Exemplo 1: Few-Shot básico para classificação de sentimentos\n",
    "def exemplo_classificacao_sentimentos():\n",
    "    \"\"\"\n",
    "    Ensina o modelo a classificar sentimentos usando exemplos\n",
    "    \"\"\"\n",
    "    \n",
    "    # Exemplos que \"ensinam\" o modelo\n",
    "    examples = [\n",
    "        {\n",
    "            \"texto\": \"Eu amo este produto! É incrível!\",\n",
    "            \"sentimento\": \"Positivo\"\n",
    "        },\n",
    "        {\n",
    "            \"texto\": \"Este produto é terrível, não recomendo.\",\n",
    "            \"sentimento\": \"Negativo\"\n",
    "        },\n",
    "        {\n",
    "            \"texto\": \"O produto é ok, nada especial.\",\n",
    "            \"sentimento\": \"Neutro\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Template para cada exemplo\n",
    "    example_prompt = PromptTemplate(\n",
    "        input_variables=[\"texto\", \"sentimento\"],\n",
    "        template=\"Texto: {texto}\\nSentimento: {sentimento}\"\n",
    "    )\n",
    "    \n",
    "    # Template Few-Shot completo\n",
    "    few_shot_prompt = FewShotPromptTemplate(\n",
    "        examples=examples,\n",
    "        example_prompt=example_prompt,\n",
    "        prefix=\"Classifique o sentimento dos seguintes textos:\\n\\n\",\n",
    "        suffix=\"Texto: {input}\\nSentimento:\",\n",
    "        input_variables=[\"input\"],\n",
    "        example_separator=\"\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    # Teste\n",
    "    prompt = few_shot_prompt.format(input=\"Estou muito feliz com minha compra!\")\n",
    "    print(\"=== EXEMPLO 1: Classificação de Sentimentos ===\")\n",
    "    print(prompt)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df52969e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXEMPLO 1: Classificação de Sentimentos ===\n",
      "Classifique o sentimento dos seguintes textos:\n",
      "\n",
      "\n",
      "\n",
      "Texto: Eu amo este produto! É incrível!\n",
      "Sentimento: Positivo\n",
      "\n",
      "Texto: Este produto é terrível, não recomendo.\n",
      "Sentimento: Negativo\n",
      "\n",
      "Texto: O produto é ok, nada especial.\n",
      "Sentimento: Neutro\n",
      "\n",
      "Texto: Estou muito feliz com minha compra!\n",
      "Sentimento:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exemplo_classificacao_sentimentos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
