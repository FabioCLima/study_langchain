{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "637142e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "from dotenv import load_dotenv  # type: ignore\n",
    "from langchain_openai import ChatOpenAI  # type: ignore\n",
    "from openai import OpenAI  # type: ignore\n",
    "\n",
    "\n",
    "#* Exce√ß√£o customizada para aus√™ncia da chave da API\n",
    "class ApiKeyNotFoundError(Exception):\n",
    "    \"\"\"\n",
    "    Exce√ß√£o lan√ßada quando a chave OPENAI_API_KEY n√£o √© encontrada no arquivo .env.\n",
    "    Herda de Exception (heran√ßa simples em Python).\n",
    "    Boas pr√°ticas: crie exce√ß√µes espec√≠ficas para facilitar o tratamento de erros.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class ApiKeyLoader:\n",
    "    \"\"\"\n",
    "    Utility to load the OpenAI API key from a .env file.\n",
    "    If no path is provided, it searches for .env in the current and parent directories.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_path: Path | None = None) -> None:\n",
    "        if env_path is not None:\n",
    "            if not env_path.exists() or not env_path.is_file():\n",
    "                raise ValueError(f\"Invalid .env path: {env_path}\")\n",
    "            self.env_path = env_path\n",
    "        else:\n",
    "            found_env = self._find_env_path()\n",
    "            if found_env is None:\n",
    "                raise ValueError(\"Could not find a .env file in current or parent directories.\")\n",
    "            self.env_path = found_env\n",
    "\n",
    "    def _find_env_path(self) -> Path | None:\n",
    "        \"\"\"Searches for a .env file in current and parent directories.\"\"\"\n",
    "        current = Path(__file__).resolve().parent\n",
    "        for parent in [current, *current.parents]:\n",
    "            candidate = parent / \".env\"\n",
    "            if candidate.exists() and candidate.is_file():\n",
    "                return candidate\n",
    "        return None\n",
    "\n",
    "    def get_openai_key(self) -> str:\n",
    "        load_dotenv(self.env_path)\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ApiKeyNotFoundError(\"OPENAI_API_KEY not found in .env file.\")\n",
    "        return api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2654ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatModelFactory:\n",
    "    \"\"\"\n",
    "    Factory class para criar diferentes tipos de modelos de chat do LangChain.\n",
    "\n",
    "    Esta classe implementa o padr√£o Factory Method, permitindo a cria√ß√£o\n",
    "    de diferentes configura√ß√µes de modelos ChatOpenAI de forma centralizada\n",
    "    e reutiliz√°vel.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Chave da API da OpenAI\n",
    "\n",
    "    Example:\n",
    "        >>> factory = ChatModelFactory(\"your-api-key\")\n",
    "        >>> analytical_model = factory.create_analytical_model()\n",
    "        >>> creative_model = factory.create_creative_model()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str) -> None:\n",
    "        \"\"\"\n",
    "        Inicializa a factory com a chave da API.\n",
    "        \n",
    "        Args:\n",
    "            api_key (str): Chave v√°lida da API da OpenAI\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: Se a api_key estiver vazia ou None\n",
    "        \"\"\"\n",
    "        if not api_key:\n",
    "            msg = \"OpenAI API key is required for ChatModelFactory\"\n",
    "            raise ValueError(msg)\n",
    "        self.api_key: str = api_key\n",
    "\n",
    "    def create_analytical_model(\n",
    "        self, \n",
    "        model_name: str = \"gpt-4\", \n",
    "        temperature: float = 0.1\n",
    "    ) -> ChatOpenAI:\n",
    "        \"\"\"\n",
    "        Cria um modelo otimizado para an√°lises e tarefas que requerem precis√£o.\n",
    "\n",
    "        Configura√ß√£o:\n",
    "        - Temperatura baixa (0.1) para respostas mais determin√≠sticas\n",
    "        - Modelo GPT-4 por padr√£o para melhor capacidade anal√≠tica\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Nome do modelo OpenAI (padr√£o: \"gpt-4\")\n",
    "            temperature (float): Controla a aleatoriedade (padr√£o: 0.1)\n",
    "\n",
    "        Returns:\n",
    "            ChatOpenAI: Inst√¢ncia configurada para an√°lises\n",
    "        \"\"\"\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key,  # type: ignore\n",
    "            model=model_name,      # fixed\n",
    "            temperature=temperature,\n",
    "        )\n",
    "    \n",
    "    def create_creative_model(\n",
    "        self, \n",
    "        model_name: str = \"gpt-3.5-turbo\", \n",
    "        temperature: float = 0.8\n",
    "    ) -> ChatOpenAI:\n",
    "        \"\"\"\n",
    "        Cria um modelo otimizado para tarefas criativas.\n",
    "        \n",
    "        Configura√ß√£o:\n",
    "        - Temperatura alta (0.8) para respostas mais criativas\n",
    "        - GPT-3.5-turbo por padr√£o (mais r√°pido e econ√¥mico)\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Nome do modelo OpenAI (padr√£o: \"gpt-3.5-turbo\")\n",
    "            temperature (float): Controla a criatividade (padr√£o: 0.8)\n",
    "            \n",
    "        Returns:\n",
    "            ChatOpenAI: Inst√¢ncia configurada para criatividade\n",
    "        \"\"\"\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key,  # type: ignore\n",
    "            model=model_name,      # fixed\n",
    "            temperature=temperature,\n",
    "        \n",
    "        )\n",
    "    \n",
    "    def create_conversational_model(\n",
    "        self, \n",
    "        model_name: str = \"gpt-3.5-turbo\", \n",
    "        temperature: float = 0.7\n",
    "    ) -> ChatOpenAI:\n",
    "        \"\"\"\n",
    "        Cria um modelo balanceado para conversas naturais.\n",
    "        \n",
    "        Configura√ß√£o:\n",
    "        - Temperatura moderada (0.7) para equil√≠brio entre precis√£o e naturalidade\n",
    "        - GPT-3.5-turbo por padr√£o para boa performance\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Nome do modelo OpenAI (padr√£o: \"gpt-3.5-turbo\")\n",
    "            temperature (float): Controla a naturalidade (padr√£o: 0.7)\n",
    "            \n",
    "        Returns:\n",
    "            ChatOpenAI: Inst√¢ncia configurada para conversas\n",
    "        \"\"\"\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key,  # type: ignore\n",
    "            model=model_name,      # fixed\n",
    "            temperature=temperature,\n",
    "        )\n",
    "    \n",
    "    def create_custom_model(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        temperature: float,\n",
    "        max_tokens: int = 2048,\n",
    "        **kwargs: object\n",
    "    ) -> ChatOpenAI:\n",
    "        \"\"\"\n",
    "        Cria um modelo com configura√ß√µes personalizadas.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Nome do modelo OpenAI\n",
    "            temperature (float): Controla a aleatoriedade (0.0 a 2.0)\n",
    "            max_tokens (int): N√∫mero m√°ximo de tokens na resposta\n",
    "            **kwargs: Argumentos adicionais para o modelo\n",
    "            \n",
    "        Returns:\n",
    "            ChatOpenAI: Inst√¢ncia com configura√ß√µes personalizadas\n",
    "        \"\"\"\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key,  # type: ignore\n",
    "            model=model_name,      # fixed\n",
    "            temperature=temperature,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4121e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FUN√á√ÉO UTILIT√ÅRIA PARA CRIAR MODELO ANAL√çTICO ===\n",
    "def create_analytical_model() -> Optional[ChatOpenAI]:\n",
    "    \"\"\"\n",
    "    Fun√ß√£o utilit√°ria que demonstra o uso completo do m√≥dulo para criar\n",
    "    um modelo anal√≠tico.\n",
    "\n",
    "    Esta fun√ß√£o:\n",
    "    1. Carrega a chave da API do arquivo .env\n",
    "    2. Cria uma inst√¢ncia da ChatModelFactory\n",
    "    3. Retorna um modelo configurado para an√°lises\n",
    "\n",
    "    Returns:\n",
    "        Optional[ChatOpenAI]: Modelo anal√≠tico ou None em caso de erro\n",
    "\n",
    "    Example:\n",
    "        >>> model = create_analytical_model()\n",
    "        >>> if model:\n",
    "        ...     response = model.invoke(\"Analise este texto...\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        env_file_path: Path = Path(__file__).resolve().parent / \".env\"\n",
    "        loader: ApiKeyLoader = ApiKeyLoader(Path(env_file_path))\n",
    "        openai_api_key = loader.get_openai_key()\n",
    "        print(f\"Chave da API OpenAI carregada com sucesso: ***{openai_api_key[-4:]}\")\n",
    "\n",
    "        chat_factory = ChatModelFactory(openai_api_key)\n",
    "        analytical_llm = chat_factory.create_analytical_model()\n",
    "        print(\"Modelo de chat OpenAI criado com sucesso!\")\n",
    "        return analytical_llm\n",
    "\n",
    "    except (ValueError, ApiKeyNotFoundError) as e:\n",
    "        print(f\"Erro ao carregar a chave da API: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b674f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uso avan√ßado\n",
    "loader = ApiKeyLoader(Path(\".env\"))\n",
    "api_key = loader.get_openai_key()\n",
    "factory = ChatModelFactory(api_key)\n",
    "#* analytical model\n",
    "analytical_model = factory.create_analytical_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eb73794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"gpt-4.1\", model_provider=\"openai\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69808327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Fabio! üòä How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 11, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_b3f1157249', 'id': 'chatcmpl-BxCCaCz01FwzdDWTJw9OIeSsDDhpX', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c15e289e-7d59-4fe7-9118-7fe3f30aaa9c-0', usage_metadata={'input_tokens': 11, 'output_tokens': 11, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Fabio\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43279d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Fabio! How can I help you today, Fabio? üòä\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "response = model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Fabio\"),\n",
    "        AIMessage(content=\"Hello Fabio! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What' my name?\"),\n",
    "    ]\n",
    ")\n",
    "print(response.content)  # Exibe a resposta do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd7fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebca292",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "___\n",
    "\n",
    "Um Prompt Template √© um modelo de texto usado para estruturar prompts (entradas de texto) enviados a um modelo de linguagem, como o GPT, de forma reutiliz√°vel e flex√≠vel. Ele permite que voc√™ insira vari√°veis dinamicamente no prompt, tornando-o mais organizado e f√°cil de manter.\n",
    "\n",
    "No contexto do LangChain, o PromptTemplate ajuda a criar prompts com partes fixas e partes vari√°veis, de modo que o mesmo modelo de linguagem possa ser usado em diferentes situa√ß√µes apenas mudando os valores das vari√°veis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "171529e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"Me fale 3 fatos sobre o uso de {tablets} para estudar e trabalhar.\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac981079",
   "metadata": {},
   "source": [
    "### Construtor Padr√£o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9b8750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"Me fale 3 fatos sobre o uso de {tablets} para estudar e trabalhar.\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"tablets\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33f1523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me fale 3 fatos sobre o uso de samsung galaxy tab Fe10+ para estudar e trabalhar.\n"
     ]
    }
   ],
   "source": [
    "# Criar a mensagem formatada\n",
    "formatted_prompt = prompt.format(tablets=\"samsung galaxy tab Fe10+\")\n",
    "print(formatted_prompt)  # Exibe o prompt formatado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72074714",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = analytical_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e24620a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Multitarefa: O Samsung Galaxy Tab S6 FE10+ √© equipado com um processador poderoso que permite multitarefa sem esfor√ßo. Isso significa que voc√™ pode ter v√°rias aplica√ß√µes abertas ao mesmo tempo, como documentos de trabalho, e-mails, e at√© mesmo v√≠deos de estudo ou apresenta√ß√µes. Isso torna o tablet uma ferramenta eficiente para estudar e trabalhar, pois voc√™ pode alternar facilmente entre diferentes tarefas.\n",
      "\n",
      "2. S Pen: Este tablet vem com a S Pen, que √© uma caneta inteligente que permite que voc√™ fa√ßa anota√ß√µes, desenhe e at√© mesmo controle o tablet √† dist√¢ncia. Isso pode ser particularmente √∫til para estudar, pois voc√™ pode destacar informa√ß√µes importantes, fazer anota√ß√µes √† m√£o e at√© mesmo desenhar diagramas ou gr√°ficos. Para o trabalho, a S Pen pode ser usada para assinar documentos digitalmente ou fazer apresenta√ß√µes.\n",
      "\n",
      "3. Tela grande e de alta qualidade: O Samsung Galaxy Tab S6 FE10+ possui uma tela de 10,5 polegadas com resolu√ß√£o de 2560 x 1600 pixels. Isso proporciona uma experi√™ncia visual n√≠tida e vibrante, tornando-o ideal para ler documentos de trabalho, assistir a v√≠deos de estudo ou fazer videochamadas. Al√©m disso, a tela grande tamb√©m permite que voc√™ divida a tela e use duas aplica√ß√µes ao mesmo tempo, o que pode aumentar a produtividade ao estudar ou trabalhar.\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(formatted_prompt)\n",
    "print(response.content)  # Exibe a resposta do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b734665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"tablets\"],\n",
    "    template=\"Me fale 3 fatos sobre o uso de {tablets} para estudar e trabalhar.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8971b001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. Multitarefa: O Samsung Galaxy Tab S6 FE10+ √© equipado com um processador poderoso que permite multitarefa sem problemas. Isso significa que voc√™ pode ter v√°rias aplica√ß√µes abertas ao mesmo tempo, como um documento do Word, uma planilha do Excel e um navegador da web, sem que o tablet fique lento. Isso √© especialmente √∫til para estudar e trabalhar, pois voc√™ pode alternar facilmente entre diferentes tarefas.\\n\\n2. S Pen: Este tablet vem com a S Pen, que √© uma caneta stylus que permite escrever, desenhar e fazer anota√ß√µes diretamente na tela. Isso pode ser muito √∫til para tomar notas durante uma aula ou reuni√£o, ou para esbo√ßar ideias durante um brainstorming. A S Pen tamb√©m tem uma fun√ß√£o de tradu√ß√£o, o que pode ser √∫til para estudar l√≠nguas estrangeiras.\\n\\n3. Tela grande e de alta qualidade: O Samsung Galaxy Tab S6 FE10+ tem uma tela de 10,5 polegadas com uma resolu√ß√£o de 2560 x 1600 pixels. Isso proporciona muito espa√ßo para trabalhar e estudar, e a alta resolu√ß√£o garante que o texto e as imagens sejam n√≠tidos e f√°ceis de ler. Al√©m disso, a tela tem uma taxa de atualiza√ß√£o de 120 Hz, o que significa que ela √© muito responsiva ao toque.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 316, 'prompt_tokens': 31, 'total_tokens': 347, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BxCTGVu3zMAmrFdjoVsClYfT110ai', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a3366b5d-843b-4534-b00b-1090cb9f12fe-0', usage_metadata={'input_tokens': 31, 'output_tokens': 316, 'total_tokens': 347, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f0411bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# Template para cada exemplo individual\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"operation\", \"step\", \"result\", \"explanation\"],\n",
    "    template=\"Opera√ß√£o: {operation}\\nPasso: {step}\\nResultado: {result}\\nExplica√ß√£o: {explanation}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5e66993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplos que mostram a progress√£o matem√°tica\n",
    "examples = [\n",
    "    {\n",
    "        \"operation\": \"2 + 2\",\n",
    "        \"step\": \"Soma simples\",\n",
    "        \"result\": \"4\",\n",
    "        \"explanation\": \"Adi√ß√£o b√°sica: somamos 2 duas vezes\"\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"2 √ó 2\",\n",
    "        \"step\": \"Multiplica√ß√£o como soma repetida\",\n",
    "        \"result\": \"4\",\n",
    "        \"explanation\": \"2 √ó 2 = 2 + 2 = 4 (multiplica√ß√£o √© soma repetida)\"\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"2¬≤\",\n",
    "        \"step\": \"Exponencia√ß√£o b√°sica\",\n",
    "        \"result\": \"4\",\n",
    "        \"explanation\": \"2¬≤ = 2 √ó 2 = 4 (exponencia√ß√£o √© multiplica√ß√£o repetida)\"\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"2¬≥\",\n",
    "        \"step\": \"Expandindo a exponencia√ß√£o\",\n",
    "        \"result\": \"8\",\n",
    "        \"explanation\": \"2¬≥ = 2 √ó 2 √ó 2 = 8\"\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"2‚Å¥\",\n",
    "        \"step\": \"Continuando o padr√£o\",\n",
    "        \"result\": \"16\",\n",
    "        \"explanation\": \"2‚Å¥ = 2 √ó 2 √ó 2 √ó 2 = 16\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b4482de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opera√ß√£o: 2 + 2\n",
      "Passo: Soma simples\n",
      "Resultado: 4\n",
      "Explica√ß√£o: Adi√ß√£o b√°sica: somamos 2 duas vezes\n",
      "\n",
      "\n",
      "Opera√ß√£o: 2 √ó 2\n",
      "Passo: Multiplica√ß√£o como soma repetida\n",
      "Resultado: 4\n",
      "Explica√ß√£o: 2 √ó 2 = 2 + 2 = 4 (multiplica√ß√£o √© soma repetida)\n",
      "\n",
      "\n",
      "Opera√ß√£o: 2¬≤\n",
      "Passo: Exponencia√ß√£o b√°sica\n",
      "Resultado: 4\n",
      "Explica√ß√£o: 2¬≤ = 2 √ó 2 = 4 (exponencia√ß√£o √© multiplica√ß√£o repetida)\n",
      "\n",
      "\n",
      "Opera√ß√£o: 2¬≥\n",
      "Passo: Expandindo a exponencia√ß√£o\n",
      "Resultado: 8\n",
      "Explica√ß√£o: 2¬≥ = 2 √ó 2 √ó 2 = 8\n",
      "\n",
      "\n",
      "Opera√ß√£o: 2‚Å¥\n",
      "Passo: Continuando o padr√£o\n",
      "Resultado: 16\n",
      "Explica√ß√£o: 2‚Å¥ = 2 √ó 2 √ó 2 √ó 2 = 16\n",
      "\n",
      "\n",
      "\n",
      "Agora resolva: 2‚Å∏\n",
      "Siga o mesmo padr√£o dos exemplos acima.\n",
      "Opera√ß√£o: 2‚Å∏\n",
      "Passo:\n",
      "Resultado:\n",
      "Explica√ß√£o:\n",
      "\n",
      "\n",
      "==================================================\n",
      "RESPOSTA DO MODELO:\n",
      "Passo: Continuando o padr√£o\n",
      "Resultado: 256\n",
      "Explica√ß√£o: 2‚Å∏ = 2 √ó 2 √ó 2 √ó 2 √ó 2 √ó 2 √ó 2 √ó 2 = 256\n"
     ]
    }
   ],
   "source": [
    "# Prompt principal (suffix)\n",
    "suffix = \"\"\"\n",
    "Agora resolva: {problem}\n",
    "Siga o mesmo padr√£o dos exemplos acima.\n",
    "Opera√ß√£o: {problem}\n",
    "Passo:\n",
    "Resultado:\n",
    "Explica√ß√£o:\n",
    "\"\"\"\n",
    "\n",
    "# Criando o Few-shot Prompt Template\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"problem\"]\n",
    ")\n",
    "\n",
    "# Testando com 2‚Å∏\n",
    "formatted_prompt = few_shot_prompt.format(problem=\"2‚Å∏\")\n",
    "print(formatted_prompt)\n",
    "\n",
    "# Usando com o modelo\n",
    "response = model.invoke(formatted_prompt)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESPOSTA DO MODELO:\")\n",
    "print(response.content) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff54744",
   "metadata": {},
   "source": [
    "## Criar um chatbot\n",
    "___\n",
    "\n",
    "Neste desafio, voc√™ criar√° um chatbot que se lembra de intera√ß√µes anteriores, segue um fluxo de conversa estruturado e oferece respostas mais humanas usando o Few-Shot Prompting.\n",
    "Ao aproveitar mem√≥ria, prompts estruturados e exemplos few-shot, seu chatbot se comportar√° de forma consistente e envolvente.\n",
    "\n",
    "**Cen√°rio**\n",
    "Voc√™ est√° desenvolvendo um assistente virtual para uma empresa. Seu chatbot precisa:\n",
    "- Manter o hist√≥rico da conversa\n",
    "- Responder de forma consistente usando exemplos predefinidos de few-shot\n",
    "- Ser personaliz√°vel para diferentes estilos, como:\n",
    "- Um assistente rob√≥tico com tom de fic√ß√£o cient√≠fica\n",
    "- Um chatbot casual para intera√ß√µes divertidas\n",
    "- Um assistente de IA profissional para tarefas empresariais\n",
    "Ao final deste exerc√≠cio, voc√™ ter√° um chatbot totalmente funcional que pode conversar de forma din√¢mica, mantendo uma personalidade predefinida.\n",
    "\n",
    "**Desafio**\n",
    "Seu chatbot deve:\n",
    "- Registrar o hist√≥rico da conversa\n",
    "- Utilizar uma abordagem estruturada de Few-Shot Prompting\n",
    "- Permitir a personaliza√ß√£o de tom e personalidade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8060ae",
   "metadata": {},
   "source": [
    "* 1. PERSONALIDADE (ChatPromptTemplate)\n",
    "* 2. FORMATO/BLUEPRINT(FewShotPrompt)\n",
    "* 3 ....\n",
    "\n",
    "Pq at√© agora s√≥ vimos como inicializar o chatmodel, como estruturar a mensagem (SystemMessage, HumanMessage, AIMessage), Prompt Templates (basic PromptTemplate, Few-Shot PromptTemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c35b0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a Python tutor\"),\n",
    "    HumanMessage(content=\"Explain what is a dictionary in Python\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e73dace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dictionary in Python is a built-in data type that allows you to store key-value pairs. The keys in a dictionary are unique and can be of any immutable type (like integers, strings, tuples, etc.), while the values can be of any type. \n",
      "\n",
      "Dictionaries are mutable, which means you can add, remove, and change their key-value elements after they are created. They are also unordered, meaning the items do not have a defined order, and you cannot refer to an item by its index.\n",
      "\n",
      "Here is an example of a dictionary:\n",
      "\n",
      "```python\n",
      "my_dict = {'name': 'John', 'age': 30, 'city': 'New York'}\n",
      "```\n",
      "\n",
      "In this dictionary, 'name', 'age', and 'city' are keys, and 'John', 30, and 'New York' are their respective values. You can access the values by their corresponding keys like this:\n",
      "\n",
      "```python\n",
      "print(my_dict['name'])  # Output: John\n",
      "```\n",
      "\n",
      "Dictionaries are very useful for data manipulation in Python, especially when dealing with large datasets.\n"
     ]
    }
   ],
   "source": [
    "ai_response = model.invoke(messages)\n",
    "print(ai_response.content)  # Exibe a resposta do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d317eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python classes are the fundamental concept of object-oriented programming. A class is like a blueprint or template for creating objects (a particular data structure), providing initial values for state (member variables or attributes), and implementations of behavior (member functions or methods).\n",
      "\n",
      "The class is a blueprint that defines the nature of a future object. From classes, we can construct instances. An instance is a specific object created from a particular class. For instance, if we have a class called \"Car\", we might have instances of that class like \"Toyota\", \"Ford\", etc. which are all cars but with different attributes and behaviors.\n",
      "\n",
      "Classes encapsulate data and the methods that manipulate that data within one entity.\n",
      "Response for topic 'Python classes': Python classes are the fundamental concept of object-oriented programming. A class is like a blueprint or template for creating objects (a particular data structure), providing initial values for state (member variables or attributes), and implementations of behavior (member functions or methods).\n",
      "\n",
      "The class is a blueprint that defines the nature of a future object. From classes, we can construct instances. An instance is a specific object created from a particular class. For instance, if we have a class called \"Car\", we might have instances of that class like \"Toyota\", \"Ford\", etc. which are all cars but with different attributes and behaviors.\n",
      "\n",
      "Classes encapsulate data and the methods that manipulate that data within one entity.\n"
     ]
    }
   ],
   "source": [
    "topic = \"Python classes\"\n",
    "prompt = f\"Explain the concept of {topic} in simple terms.\"\n",
    "ai_response = model.invoke(prompt)\n",
    "print(ai_response.content)  # Exibe a resposta do modelo\n",
    "print(f\"Response for topic '{topic}': {ai_response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fc39f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain the concept of {topic} in simple terms.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1abcbabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Explain the concept of {topic} in simple terms.')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42188cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Explain the concept of Python classes in simple terms.')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.invoke({\"topic\": \"Python classes\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d3ad6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A Python function is a block of reusable code that performs a specific task. Functions provide better modularity for your application and allow for high levels of code reusing. You can define functions to provide the required functionality. Here are simple rules to define a function in Python:\\n\\n- Function blocks begin with the keyword def followed by the function name and parentheses ().\\n- Any input parameters or arguments should be placed within these parentheses. You can also define parameters inside these parentheses.\\n- The first statement of a function can be an optional statement - the documentation string of the function or docstring.\\n- The code block within every function starts with a colon (:) and is indented.\\n- The statement return [expression] exits a function, optionally passing back an expression to the caller. A return statement with no arguments is the same as return None.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 167, 'prompt_tokens': 18, 'total_tokens': 185, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BxMNLqEvo99EmfieyJjtcoi8hUiHb', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a25c5249-6796-47dc-a119-a99067f68fa1-0', usage_metadata={'input_tokens': 18, 'output_tokens': 167, 'total_tokens': 185, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\n",
    "    prompt_template.invoke({\"topic\": \"Python function\"})  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a65a92",
   "metadata": {},
   "source": [
    "## Few Shot Prompt \n",
    " - ensina padr√£o atrav√©s de exemplos\n",
    "* Uso: Treinar formato espec√≠fico de resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afbaa7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Few-Shot Prompting com LangChain - Exemplo Did√°tico\n",
    "Demonstra como usar exemplos para treinar o modelo a seguir um padr√£o espec√≠fico\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Exemplo 1: Few-Shot b√°sico para classifica√ß√£o de sentimentos\n",
    "def exemplo_classificacao_sentimentos():\n",
    "    \"\"\"\n",
    "    Ensina o modelo a classificar sentimentos usando exemplos\n",
    "    \"\"\"\n",
    "    \n",
    "    # Exemplos que \"ensinam\" o modelo\n",
    "    examples = [\n",
    "        {\n",
    "            \"texto\": \"Eu amo este produto! √â incr√≠vel!\",\n",
    "            \"sentimento\": \"Positivo\"\n",
    "        },\n",
    "        {\n",
    "            \"texto\": \"Este produto √© terr√≠vel, n√£o recomendo.\",\n",
    "            \"sentimento\": \"Negativo\"\n",
    "        },\n",
    "        {\n",
    "            \"texto\": \"O produto √© ok, nada especial.\",\n",
    "            \"sentimento\": \"Neutro\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Template para cada exemplo\n",
    "    example_prompt = PromptTemplate(\n",
    "        input_variables=[\"texto\", \"sentimento\"],\n",
    "        template=\"Texto: {texto}\\nSentimento: {sentimento}\"\n",
    "    )\n",
    "    \n",
    "    # Template Few-Shot completo\n",
    "    few_shot_prompt = FewShotPromptTemplate(\n",
    "        examples=examples,\n",
    "        example_prompt=example_prompt,\n",
    "        prefix=\"Classifique o sentimento dos seguintes textos:\\n\\n\",\n",
    "        suffix=\"Texto: {input}\\nSentimento:\",\n",
    "        input_variables=[\"input\"],\n",
    "        example_separator=\"\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    # Teste\n",
    "    prompt = few_shot_prompt.format(input=\"Estou muito feliz com minha compra!\")\n",
    "    print(\"=== EXEMPLO 1: Classifica√ß√£o de Sentimentos ===\")\n",
    "    print(prompt)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df52969e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXEMPLO 1: Classifica√ß√£o de Sentimentos ===\n",
      "Classifique o sentimento dos seguintes textos:\n",
      "\n",
      "\n",
      "\n",
      "Texto: Eu amo este produto! √â incr√≠vel!\n",
      "Sentimento: Positivo\n",
      "\n",
      "Texto: Este produto √© terr√≠vel, n√£o recomendo.\n",
      "Sentimento: Negativo\n",
      "\n",
      "Texto: O produto √© ok, nada especial.\n",
      "Sentimento: Neutro\n",
      "\n",
      "Texto: Estou muito feliz com minha compra!\n",
      "Sentimento:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exemplo_classificacao_sentimentos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
